{
  "metadata": {
    "version": "2.0",
    "lastUpdated": "2026-02-05",
    "totalTerms": 2141,
    "description": "Comprehensive AI glossary covering ML, neural networks, NLP, generative AI, computer vision, RL, hardware, ethics, history, prompt engineering, evaluation metrics, and vector databases."
  },
  "terms": [
    {
      "id": "term-a-star-search-algorithm",
      "term": "A* Search Algorithm",
      "definition": "A graph search algorithm developed by Peter Hart, Nils Nilsson, and Bertram Raphael at SRI International in 1968 that finds the shortest path using heuristics, becoming fundamental to AI planning and pathfinding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-ab-testing",
      "term": "A/B Testing",
      "definition": "A randomized controlled experiment that compares two variants (A and B) to determine which performs better on a specified metric. It is widely used in product development to make data-driven decisions.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-ab-testing-for-llms",
      "term": "A/B Testing for LLMs",
      "definition": "A comparative evaluation methodology where two language model variants or prompt configurations are deployed to different user segments, with statistical analysis of user preference, task completion, and quality metrics determining the superior option.",
      "tags": [
        "Evaluation",
        "Methodology"
      ]
    },
    {
      "id": "term-ablation-study",
      "term": "Ablation Study",
      "definition": "A research technique that removes components of a model to understand their contribution. Helps researchers understand which parts of a system are responsible for its capabilities.",
      "tags": [
        "Research",
        "Methodology"
      ]
    },
    {
      "id": "term-amr",
      "term": "Abstract Meaning Representation",
      "definition": "A semantic representation that encodes the meaning of a sentence as a rooted directed acyclic graph, abstracting away from syntactic details to capture who did what to whom.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-abstractive-summarization",
      "term": "Abstractive Summarization",
      "definition": "A summarization approach that generates novel sentences capturing the key information from the source text, potentially using words and phrasings not present in the original document.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-academic-prompting",
      "term": "Academic Prompting",
      "definition": "A prompting approach tailored for scholarly tasks that instructs the model to follow academic conventions including formal tone, citation awareness, evidence-based reasoning, and structured argumentation suitable for research contexts.",
      "tags": [
        "Prompt Engineering",
        "Academic"
      ]
    },
    {
      "id": "term-accountability-in-ai",
      "term": "Accountability in AI",
      "definition": "The principle that identifiable individuals or organizations should be answerable for the outcomes and impacts of AI systems, including mechanisms for redress when AI causes harm.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-accumulated-local-effects",
      "term": "Accumulated Local Effects",
      "definition": "A model-agnostic method for visualizing feature effects that is unbiased in the presence of correlated features, unlike partial dependence plots. It computes effects by accumulating differences in predictions over local intervals.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-accuracy",
      "term": "Accuracy",
      "definition": "A metric measuring how often a model's predictions are correct. Calculated as the ratio of correct predictions to total predictions. While intuitive, it can be misleading for imbalanced datasets.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-action",
      "term": "Action",
      "definition": "A decision or move taken by an RL agent that affects the environment and transitions the system to a new state. Actions can be discrete (finite set of choices) or continuous (real-valued vectors).",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-action-masking",
      "term": "Action Masking",
      "definition": "A technique that restricts the set of available actions at each state by zeroing out invalid action probabilities before policy sampling. Action masking enforces domain constraints and prevents the agent from selecting illegal moves.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-action-recognition",
      "term": "Action Recognition",
      "definition": "A video understanding task that identifies and classifies human actions or activities in video sequences, using temporal modeling of motion patterns across frames with architectures like 3D CNNs or video transformers.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-action-repeat",
      "term": "Action Repeat",
      "definition": "A technique where each selected action is executed for multiple consecutive environment steps, reducing the effective decision frequency. Action repeat simplifies the control problem and can improve exploration efficiency.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-action-space",
      "term": "Action Space",
      "definition": "The set of all possible actions available to an RL agent, defined as discrete (finite choices), continuous (real-valued vectors), or multi-discrete. The action space structure fundamentally affects which algorithms are applicable.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-activation-checkpointing",
      "term": "Activation Checkpointing",
      "definition": "A memory optimization technique that trades compute for memory by discarding intermediate activations during the forward pass and recomputing them during backpropagation.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-activation-function",
      "term": "Activation Function",
      "definition": "A mathematical function applied to neurons in neural networks that introduces non-linearity, enabling the network to learn complex patterns. Common examples include ReLU, sigmoid, and tanh.",
      "tags": [
        "Neural Networks",
        "Technical"
      ]
    },
    {
      "id": "term-activation-quantization",
      "term": "Activation Quantization",
      "definition": "The process of quantizing intermediate activations (not just weights) during inference to reduce memory bandwidth requirements and enable INT8 or lower-precision arithmetic. Activation quantization is more challenging than weight quantization due to dynamic ranges.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-active-learning",
      "term": "Active Learning",
      "definition": "A training approach where the model identifies which unlabeled examples would be most valuable to learn from. Reduces labeling costs by focusing human effort where it matters most.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-active-prompting",
      "term": "Active Prompting",
      "definition": "A method that identifies the most uncertain or informative questions for chain-of-thought annotation by measuring model disagreement across sampled outputs, then selectively annotates those examples to maximize few-shot demonstration effectiveness.",
      "tags": [
        "Prompt Engineering",
        "Active Learning"
      ]
    },
    {
      "id": "term-actor-critic",
      "term": "Actor-Critic",
      "definition": "An RL architecture combining a policy network (actor) that selects actions with a value network (critic) that evaluates those actions. The critic's value estimates reduce the variance of policy gradient updates compared to pure policy gradient methods.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-ada-lovelace",
      "term": "Ada Lovelace",
      "definition": "British mathematician (1815-1852) who wrote the first published algorithm intended for a machine, working with Charles Babbage's Analytical Engine, and is widely regarded as the first computer programmer.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-adaboost",
      "term": "AdaBoost",
      "definition": "An ensemble method that trains weak learners sequentially, assigning higher weights to misclassified samples so that subsequent learners focus on the hardest examples. Final predictions are a weighted vote of all learners.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-adagrad",
      "term": "AdaGrad",
      "definition": "An optimization algorithm that adapts the learning rate for each parameter individually by dividing by the square root of the sum of all historical squared gradients. It performs well on sparse data but can prematurely reduce learning rates.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-adam",
      "term": "Adam Optimizer",
      "definition": "A popular optimization algorithm combining momentum with adaptive learning rates. The default choice for training many neural networks due to good performance across tasks.",
      "tags": [
        "Training",
        "Algorithm"
      ]
    },
    {
      "id": "term-adapter-layer",
      "term": "Adapter Layer",
      "definition": "A small trainable module inserted between frozen pretrained layers that learns task-specific transformations with minimal additional parameters, enabling efficient fine-tuning without modifying the base model.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-additive-attention",
      "term": "Additive Attention",
      "definition": "An attention mechanism that computes compatibility scores by passing the concatenation of query and key through a feedforward layer, also known as Bahdanau attention from its use in early seq2seq models.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-a2c",
      "term": "Advantage Actor-Critic (A2C)",
      "definition": "A synchronous variant of the actor-critic method that uses the advantage function (difference between action value and state value) to reduce variance in policy gradient updates. A2C collects experiences from multiple parallel environments simultaneously.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-advantage-function",
      "term": "Advantage Function",
      "definition": "The difference A(s,a) = Q(s,a) - V(s) between the action-value and state-value functions, measuring how much better an action is compared to the average action under the current policy. The advantage function reduces variance in policy gradient methods.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-advantage-weighted-regression",
      "term": "Advantage-Weighted Regression (AWR)",
      "definition": "An offline RL algorithm that learns a policy by performing weighted maximum likelihood on a dataset, where the weights are exponentiated advantages. AWR avoids policy gradient variance and off-policy correction issues.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-adversarial-attack",
      "term": "Adversarial Attack",
      "definition": "Deliberate attempts to deceive AI systems by providing specially crafted inputs. These can cause models to make incorrect predictions or generate harmful outputs.",
      "tags": [
        "Security",
        "Safety"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-adversarial-example-cv",
      "term": "Adversarial Example",
      "definition": "An input image with carefully crafted, often imperceptible perturbations that cause a vision model to make incorrect predictions with high confidence, exposing vulnerabilities in neural network classifiers.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-adversarial-prompting",
      "term": "Adversarial Prompting",
      "definition": "The deliberate crafting of inputs designed to exploit vulnerabilities in language models, causing them to produce harmful outputs, bypass safety filters, reveal system prompts, or behave contrary to their intended instructions.",
      "tags": [
        "Prompt Engineering",
        "Safety"
      ]
    },
    {
      "id": "term-adversarial-training-cv",
      "term": "Adversarial Training",
      "definition": "A defense technique that augments training with adversarial examples, teaching vision models to correctly classify perturbed inputs and improving robustness against adversarial attacks.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-agent",
      "term": "Agent (AI Agent)",
      "definition": "An AI system that can perceive its environment, make decisions, and take actions to achieve goals. Modern AI agents can use tools, browse the web, execute code, and interact with external systems.",
      "tags": [
        "Architecture",
        "Advanced"
      ]
    },
    {
      "id": "term-agent-framework",
      "term": "Agent Framework",
      "definition": "A software architecture that enables LLMs to autonomously plan, reason, and execute multi-step tasks by combining language understanding with tool use, memory, and iterative decision-making.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-agentic-ai",
      "term": "Agentic AI",
      "definition": "AI systems that can autonomously plan, reason, and take actions to accomplish goals. Includes tool use, multi-step planning, and self-correction capabilities.",
      "tags": [
        "Architecture",
        "Advanced"
      ]
    },
    {
      "id": "term-agentic-chunking",
      "term": "Agentic Chunking",
      "definition": "A document splitting strategy that uses a language model agent to make intelligent decisions about chunk boundaries, content grouping, and chunk summaries, producing semantically coherent chunks that a simple rule-based splitter would miss.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-agentic-rag",
      "term": "Agentic RAG",
      "definition": "A retrieval-augmented generation approach where an LLM agent dynamically decides what to retrieve, refines queries based on initial results, and iteratively gathers information until it can produce a complete answer.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-aggregation-bias",
      "term": "Aggregation Bias",
      "definition": "Bias arising when a single model is used for groups with different conditional distributions, leading to poor performance for subgroups whose patterns differ from the majority population.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-agi",
      "term": "AGI (Artificial General Intelligence)",
      "definition": "Hypothetical AI that can perform any intellectual task a human can. Unlike today's narrow AI, AGI would generalize across all domains. A long-term goal and safety concern.",
      "tags": [
        "Concept",
        "Future"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-agi-safety",
      "term": "AGI Safety",
      "definition": "The subfield of AI safety specifically focused on ensuring that artificial general intelligence, systems matching or exceeding human cognitive abilities across all domains, remains beneficial and controllable.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-ai",
      "term": "AI (Artificial Intelligence)",
      "definition": "Computer systems designed to perform tasks that typically require human intelligence, such as understanding language, recognizing patterns, making decisions, and generating content.",
      "tags": []
    },
    {
      "id": "term-ai-alignment-tax",
      "term": "AI Alignment Tax",
      "definition": "The additional cost in performance, compute, or development time required to make an AI system aligned with human values, representing the trade-off between capability and safety.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-ai-arms-race",
      "term": "AI Arms Race",
      "definition": "The competitive dynamic between nations or companies racing to develop the most advanced AI capabilities, potentially at the expense of safety research, ethical considerations, and international cooperation.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-ai-bill-of-rights",
      "term": "AI Bill of Rights",
      "definition": "The Blueprint for an AI Bill of Rights released by the White House OSTP in 2022, outlining five principles for responsible AI: safe systems, algorithmic discrimination protections, data privacy, notice and explanation, and human alternatives.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-boom-2023",
      "term": "AI Boom 2023",
      "definition": "The period of intense investment, development, and public attention in AI following the launch of ChatGPT, characterized by rapid advances in large language models, generative AI, and record venture capital funding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-ai-consciousness",
      "term": "AI Consciousness",
      "definition": "The philosophical and scientific question of whether AI systems can have subjective experiences or phenomenal awareness, with implications for moral consideration and the ethical treatment of AI entities.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-ai-containment",
      "term": "AI Containment",
      "definition": "Strategies and technical measures designed to prevent an advanced AI system from exerting unintended influence on the external world, including air-gapping, sandboxing, and limiting communication channels.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-ai-detection",
      "term": "AI Detection",
      "definition": "Methods and tools designed to distinguish AI-generated text, images, or media from human-created content, using statistical analysis of token distributions, perplexity patterns, or embedded watermarks.",
      "tags": [
        "Generative AI",
        "LLM"
      ]
    },
    {
      "id": "term-ai-digital-divide",
      "term": "AI Digital Divide",
      "definition": "The gap between those who have access to AI technologies and the skills to use them and those who do not, potentially exacerbating existing social and economic inequalities across and within nations.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-ai-environmental-impact",
      "term": "AI Environmental Impact",
      "definition": "The environmental costs of AI development and deployment, including the substantial energy consumption and carbon emissions from training large models, water usage for data center cooling, and electronic waste.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-ai-ethics",
      "term": "AI Ethics",
      "definition": "The study of moral principles and values that should guide the development and use of AI systems. Covers fairness, transparency, privacy, accountability, and societal impact.",
      "tags": [
        "Ethics",
        "Society"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-ai-governance",
      "term": "AI Governance",
      "definition": "The set of policies, regulations, standards, and institutional frameworks that guide the development, deployment, and oversight of artificial intelligence systems at organizational, national, and international levels.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-impact-assessment",
      "term": "AI Impact Assessment",
      "definition": "A systematic process for evaluating the potential social, ethical, economic, and environmental effects of an AI system before and during deployment, analogous to environmental impact assessments.",
      "tags": [
        "Governance",
        "AI Ethics"
      ]
    },
    {
      "id": "term-ai-incident-database",
      "term": "AI Incident Database",
      "definition": "A repository cataloging real-world instances where AI systems caused harm or exhibited problematic behavior, maintained by organizations like the Partnership on AI to enable learning from failures.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-ai-labor-displacement",
      "term": "AI Labor Displacement",
      "definition": "The phenomenon of AI and automation systems replacing human workers in various occupations, raising concerns about unemployment, wage depression, skill obsolescence, and the need for workforce transition programs.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-ai-liability-framework",
      "term": "AI Liability Framework",
      "definition": "Legal frameworks determining who bears responsibility when AI systems cause harm, including debates over strict liability, negligence standards, and the EU's AI Liability Directive proposal.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-moratorium",
      "term": "AI Moratorium",
      "definition": "A proposed temporary pause on the development of AI systems above a certain capability threshold, notably advocated in the March 2023 open letter signed by prominent researchers and technologists.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-ai-personhood",
      "term": "AI Personhood",
      "definition": "The legal and philosophical concept of granting AI systems some form of legal personality, enabling them to hold rights, enter contracts, or bear liability, as debated in EU and other jurisdictions.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-ai-readiness",
      "term": "AI Readiness",
      "definition": "The skills, knowledge, and mindset needed to use AI tools effectively and responsibly. Includes understanding both capabilities and limitations.",
      "tags": [],
      "link": "../tools/index.html"
    },
    {
      "id": "term-ai-red-lines",
      "term": "AI Red Lines",
      "definition": "Clearly defined boundaries that AI systems should never cross, such as refusing to assist with creating weapons of mass destruction, generating child sexual abuse material, or undermining democratic processes.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-ai-regulation-timeline",
      "term": "AI Regulation Timeline",
      "definition": "The chronological progression of AI governance efforts from early ethical guidelines in the 2010s through the EU AI Act, US executive orders, and international summits, representing the maturation of AI policy worldwide.",
      "tags": [
        "History",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-regulatory-sandbox",
      "term": "AI Regulatory Sandbox",
      "definition": "A controlled environment established by regulators where AI companies can test innovative products under relaxed regulatory requirements while maintaining safeguards, as provided for in the EU AI Act.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-risk-levels",
      "term": "AI Risk Levels",
      "definition": "A classification scheme, notably used in the EU AI Act, that categorizes AI applications into tiers such as unacceptable risk, high risk, limited risk, and minimal risk, with corresponding regulatory requirements for each tier.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ai-safety",
      "term": "AI Safety",
      "definition": "The field focused on ensuring AI systems behave safely and beneficially. Includes technical research on alignment, governance, and preventing misuse or unintended harms.",
      "tags": [
        "Field",
        "Safety"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-ai-safety-institute",
      "term": "AI Safety Institute",
      "definition": "A government-backed organization, first established by the UK in 2023, dedicated to evaluating and testing frontier AI models for safety risks, with similar institutes subsequently created by the US and other nations.",
      "tags": [
        "Governance",
        "AI Safety"
      ]
    },
    {
      "id": "term-ai-sandboxing",
      "term": "AI Sandboxing",
      "definition": "The practice of running AI systems in isolated environments with restricted access to networks, resources, and actuators to limit potential harm during testing and evaluation.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-ai-washing",
      "term": "AI Washing",
      "definition": "The practice of companies exaggerating or fabricating the role of AI in their products or services for marketing purposes, misleading consumers and investors about the actual capabilities of their technology.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-ai-whistleblowing",
      "term": "AI Whistleblowing",
      "definition": "The act of insiders at AI companies publicly disclosing information about safety concerns, unethical practices, or dangerous capabilities, as seen in open letters and public statements from AI researchers in 2023-2024.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-ai-winter",
      "term": "AI Winter",
      "definition": "Periods of reduced funding and interest in AI research following failed expectations. Notable winters occurred in the 1970s and late 1980s. The current era is considered an AI boom.",
      "tags": [
        "Historical",
        "Industry"
      ]
    },
    {
      "id": "term-akaike-information-criterion",
      "term": "Akaike Information Criterion",
      "definition": "A model selection metric that balances goodness of fit with model complexity by adding a penalty proportional to the number of parameters. Lower AIC values indicate a better tradeoff between fit and parsimony.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-alan-turing",
      "term": "Alan Turing",
      "definition": "British mathematician and logician (1912-1954) who formalized computation with the Turing machine, broke the Enigma code at Bletchley Park, and proposed the imitation game as a test for machine intelligence.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-albert",
      "term": "ALBERT",
      "definition": "A Lite BERT that reduces model size through factorized embedding parameterization and cross-layer parameter sharing while maintaining competitive performance on downstream tasks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-alexa-launch",
      "term": "Alexa Launch",
      "definition": "Amazon's launch of Alexa and the Echo smart speaker in November 2014, popularizing voice-activated AI assistants in the home and establishing a major platform for ambient computing and smart home control.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-alexnet",
      "term": "AlexNet",
      "definition": "A deep convolutional neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012 ImageNet competition by a large margin, marking the beginning of the deep learning era in AI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-algorithm",
      "term": "Algorithm",
      "definition": "A step-by-step procedure or set of rules for solving a problem or accomplishing a task. In AI, algorithms define how models learn from data and make predictions.",
      "tags": [
        "Fundamentals",
        "Computer Science"
      ]
    },
    {
      "id": "term-algorithmic-discrimination",
      "term": "Algorithmic Discrimination",
      "definition": "Systematic and unfair differential treatment of individuals or groups by automated decision-making systems, often arising from biased training data, flawed features, or objectives that encode structural inequalities.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-algorithmic-impact-assessment",
      "term": "Algorithmic Impact Assessment",
      "definition": "A formal evaluation process required in some jurisdictions to assess the potential effects of automated decision-making systems on individuals and communities before deployment, particularly for high-stakes applications.",
      "tags": [
        "Governance",
        "AI Ethics"
      ]
    },
    {
      "id": "term-algorithmic-recourse",
      "term": "Algorithmic Recourse",
      "definition": "The ability of individuals affected by automated decisions to take meaningful actions to change the outcome, such as understanding what inputs to modify to receive a different classification.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-algorithmic-transparency",
      "term": "Algorithmic Transparency",
      "definition": "The degree to which the logic, rules, and data dependencies of an algorithm are made visible and understandable to affected individuals, regulators, and the public.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-alibi",
      "term": "ALiBi",
      "definition": "Attention with Linear Biases, a positional encoding method that adds a linear bias proportional to the distance between key and query positions directly to attention scores, enabling length extrapolation without positional embeddings.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-alignment",
      "term": "Alignment",
      "definition": "The challenge of ensuring AI systems behave in ways that match human values and intentions. A key concern in AI safety research, involving both technical and philosophical considerations.",
      "tags": [
        "Safety",
        "Research"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-all-gather",
      "term": "All-Gather Operation",
      "definition": "A collective communication pattern where each participant broadcasts its data to all others, so every participant ends up with the complete concatenated dataset. All-gather is used in FSDP to reconstruct full parameters before forward/backward passes.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-all-reduce",
      "term": "All-Reduce Operation",
      "definition": "A collective communication pattern where all participating GPUs contribute data, perform a reduction operation (typically summation), and receive the result. All-reduce is the fundamental communication primitive for synchronizing gradients in data-parallel training.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-allen-newell",
      "term": "Allen Newell",
      "definition": "American computer scientist (1927-1992) who, together with Herbert Simon, developed the Logic Theorist and General Problem Solver, pioneered the physical symbol systems hypothesis, and co-founded the field of AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-allocative-harm",
      "term": "Allocative Harm",
      "definition": "Harm that occurs when an AI system unfairly distributes resources, opportunities, or outcomes across different groups, such as denying loans, jobs, or services based on protected characteristics.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-alpaca",
      "term": "Alpaca",
      "definition": "An early instruction-tuned version of Llama created by Stanford researchers. Demonstrated that instruction-following could be achieved with synthetic data at low cost.",
      "tags": [
        "Model",
        "Historical"
      ]
    },
    {
      "id": "term-alpacaeval",
      "term": "AlpacaEval",
      "definition": "An automatic evaluation framework that compares model outputs against a reference model using LLM-based pairwise judgments, providing a fast and cost-effective proxy for human evaluation of instruction-following ability with high human agreement rates.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-alphafold",
      "term": "AlphaFold",
      "definition": "DeepMind's AI system that solved the protein structure prediction problem, winning CASP14 in 2020 and subsequently predicting structures for nearly all known proteins, revolutionizing structural biology.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-alphago",
      "term": "AlphaGo",
      "definition": "A DeepMind system that combined deep neural networks with Monte Carlo tree search to defeat world champion Go players. AlphaGo used supervised learning from human games followed by self-play reinforcement learning to achieve superhuman play.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-alphago-vs-lee-sedol",
      "term": "AlphaGo vs Lee Sedol",
      "definition": "The March 2016 match in which DeepMind's AlphaGo defeated world champion Go player Lee Sedol 4-1, a landmark achievement as Go was long considered too complex for AI due to its vast search space.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-alphazero",
      "term": "AlphaZero",
      "definition": "A generalized version of AlphaGo that learns to play Go, chess, and shogi entirely through self-play without human game data. AlphaZero uses a single neural network for both policy and value prediction combined with MCTS.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-alternative-hypothesis",
      "term": "Alternative Hypothesis",
      "definition": "The hypothesis that contradicts the null hypothesis in statistical testing, typically representing the effect or difference the researcher aims to detect. It can be one-sided or two-sided.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-amazon-bedrock",
      "term": "Amazon Bedrock",
      "definition": "AWS's managed service for accessing foundation models from multiple providers. Offers Claude, Llama, and other models through a unified API with enterprise features.",
      "tags": [
        "Platform",
        "Cloud"
      ]
    },
    {
      "id": "term-amd-mi300x",
      "term": "AMD MI300X",
      "definition": "AMD's data center GPU accelerator featuring 192GB HBM3 memory and CDNA 3 architecture, competing with NVIDIA's H100 for AI training and inference. The MI300X offers the largest memory capacity of any GPU accelerator.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-analogical-prompting",
      "term": "Analogical Prompting",
      "definition": "A technique that asks the model to generate relevant analogous problems and their solutions before tackling the target problem, leveraging self-generated exemplars to guide reasoning without requiring manually crafted few-shot examples.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-anaphora-resolution",
      "term": "Anaphora Resolution",
      "definition": "The task of determining which previously mentioned entity a pronoun or other referring expression points back to in a text, a subproblem of coreference resolution.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-anchor",
      "term": "Anchor (Prompting)",
      "definition": "A reference point or example in a prompt that guides the AI's response style or format. Anchors help establish expectations for output quality and structure.",
      "tags": [
        "Prompting",
        "Technique"
      ]
    },
    {
      "id": "term-anchor-box",
      "term": "Anchor Box",
      "definition": "A predefined set of bounding boxes with various aspect ratios and scales placed at each spatial location in a feature map, serving as reference shapes that object detectors adjust to fit actual objects.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-anchoring-bias-in-ai",
      "term": "Anchoring Bias in AI",
      "definition": "A cognitive bias where initial AI-generated suggestions disproportionately influence subsequent human decisions, causing users to adjust insufficiently from the AI's initial output.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-andrew-ng",
      "term": "Andrew Ng",
      "definition": "British-American computer scientist who co-founded Google Brain, led AI at Baidu, founded Coursera and deeplearning.ai, and popularized deep learning education, becoming one of the most influential figures in making AI accessible.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-ann-benchmark",
      "term": "ANN Benchmark",
      "definition": "Standardized evaluation suites for comparing approximate nearest neighbor algorithms across metrics like recall, queries per second, and index build time on reference datasets, enabling fair performance comparison between different vector search implementations.",
      "tags": [
        "Vector Database",
        "Evaluation"
      ]
    },
    {
      "id": "term-annotation",
      "term": "Annotation",
      "definition": "The process of labeling data to create training datasets for supervised learning. Human annotators add labels, categories, or descriptions to raw data like text, images, or audio.",
      "tags": [
        "Data",
        "Training"
      ]
    },
    {
      "id": "term-annotation-labor-ethics",
      "term": "Annotation Labor Ethics",
      "definition": "Ethical concerns about the working conditions, compensation, and psychological impacts experienced by data annotation workers who label training data for AI systems, often exposed to disturbing content.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-annoy",
      "term": "Annoy",
      "definition": "Approximate Nearest Neighbors Oh Yeah, an open-source library by Spotify that builds forest-of-trees indexes using random hyperplane splits for fast approximate nearest neighbor search, optimized for memory-mapped read-only access and static datasets.",
      "tags": [
        "Vector Database",
        "Libraries"
      ]
    },
    {
      "id": "term-anomaly-detection",
      "term": "Anomaly Detection",
      "definition": "Identifying unusual patterns or outliers in data that don't conform to expected behavior. Used in fraud detection, system monitoring, and quality control.",
      "tags": [
        "ML Task",
        "Application"
      ]
    },
    {
      "id": "term-anomaly-detection-images",
      "term": "Anomaly Detection in Images",
      "definition": "The task of identifying unusual patterns, defects, or out-of-distribution samples in images, widely used in industrial quality inspection and medical imaging to detect abnormalities.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-anova",
      "term": "ANOVA",
      "definition": "Analysis of Variance, a statistical method that tests whether the means of three or more groups are significantly different by comparing within-group variance to between-group variance using the F-statistic.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-answer-engineering",
      "term": "Answer Engineering",
      "definition": "Designing prompts to elicit specific response formats or structured outputs. Complements prompt engineering by focusing on how answers should be structured.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-ant-colony-optimization",
      "term": "Ant Colony Optimization",
      "definition": "A metaheuristic optimization algorithm proposed by Marco Dorigo in 1992, inspired by the foraging behavior of ants using pheromone trails, applied to combinatorial optimization problems like the traveling salesman problem.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-anthropic",
      "term": "Anthropic",
      "definition": "An AI safety company founded in 2021 by former OpenAI researchers. Creator of the Claude family of AI assistants, focused on developing safe and beneficial AI systems.",
      "tags": [
        "Company",
        "LLM Provider"
      ]
    },
    {
      "id": "term-anthropic-founding",
      "term": "Anthropic Founding",
      "definition": "The founding of Anthropic in 2021 by former OpenAI researchers Dario and Daniela Amodei, establishing a safety-focused AI company that developed Constitutional AI and the Claude family of AI assistants.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-api",
      "term": "API (Application Programming Interface)",
      "definition": "A set of protocols that allows different software applications to communicate. AI APIs enable developers to integrate AI capabilities into their applications without building models from scratch.",
      "tags": [
        "Technical",
        "Integration"
      ]
    },
    {
      "id": "term-apple-neural-engine",
      "term": "Apple Neural Engine",
      "definition": "Apple's dedicated neural network accelerator integrated into Apple Silicon chips (M-series and A-series), delivering up to 38 TOPS for on-device inference. The Neural Engine enables real-time AI features like Face ID, live text, and on-device language models.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-appropriate-reliance",
      "term": "Appropriate Reliance",
      "definition": "The calibrated level of trust humans should place in AI systems, avoiding both over-reliance that leads to automation complacency and under-reliance that fails to leverage AI capabilities.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-approximate-nearest-neighbor",
      "term": "Approximate Nearest Neighbor",
      "definition": "A class of search algorithms that find vectors approximately closest to a query vector with high probability rather than guaranteeing exact results, achieving orders-of-magnitude speedups over exact search by accepting a controllable recall trade-off.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-arc-benchmark",
      "term": "ARC Benchmark",
      "definition": "The AI2 Reasoning Challenge, a question-answering benchmark consisting of elementary and middle school science exam questions in easy and challenge sets, testing scientific reasoning and world knowledge in language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-arcface",
      "term": "ArcFace",
      "definition": "A face recognition loss function that adds an angular margin penalty in the normalized feature space, improving the discriminative power of face embeddings for accurate identity verification.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-architecture-search",
      "term": "Architecture Search (NAS)",
      "definition": "Automated methods for discovering optimal neural network architectures. Can find better designs than human-created networks but requires significant computational resources.",
      "tags": [
        "Research",
        "Optimization"
      ]
    },
    {
      "id": "term-arena-score",
      "term": "Arena Score",
      "definition": "A model ranking metric derived from competitive evaluation platforms where models are compared in blind pairwise matchups with human judges, producing a leaderboard rating that reflects aggregate model quality across diverse tasks.",
      "tags": [
        "Evaluation",
        "Ranking"
      ]
    },
    {
      "id": "term-arima",
      "term": "ARIMA",
      "definition": "AutoRegressive Integrated Moving Average, a class of time series models combining autoregression (AR), differencing for stationarity (I), and moving average (MA) components. It is widely used for univariate time series forecasting.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-arithmetic-intensity",
      "term": "Arithmetic Intensity",
      "definition": "The ratio of floating-point operations to bytes of memory accessed in a computation, determining whether performance is limited by compute or memory bandwidth. LLM inference has low arithmetic intensity, making it typically memory-bandwidth-bound.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ]
    },
    {
      "id": "term-arthur-samuel",
      "term": "Arthur Samuel",
      "definition": "American computer scientist (1901-1990) who created a checkers-playing program at IBM in 1959 that learned through self-play, coining the term machine learning and pioneering the field of game-playing AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-artificial-neuron",
      "term": "Artificial Neuron",
      "definition": "The basic computational unit in neural networks, loosely inspired by biological neurons. Computes a weighted sum of inputs, applies an activation function, and outputs the result.",
      "tags": [
        "Architecture",
        "Fundamentals"
      ]
    },
    {
      "id": "term-ashish-vaswani",
      "term": "Ashish Vaswani",
      "definition": "Lead author of the 2017 Attention Is All You Need paper that introduced the transformer architecture at Google Brain, fundamentally changing the trajectory of natural language processing and AI research.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-asic-ai",
      "term": "ASIC for AI",
      "definition": "Application-Specific Integrated Circuits designed exclusively for AI computation, offering maximum performance and energy efficiency for fixed workloads. AI ASICs like Google's TPU sacrifice programmability for orders-of-magnitude improvements in performance per watt.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-asilomar-ai-principles",
      "term": "Asilomar AI Principles",
      "definition": "A set of 23 principles for beneficial AI research developed at the 2017 Asilomar conference, covering research issues, ethics and values, and longer-term concerns about advanced AI safety.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-aspect-based-sentiment",
      "term": "Aspect-Based Sentiment Analysis",
      "definition": "A fine-grained sentiment analysis task that identifies sentiment toward specific aspects or features of an entity, distinguishing different opinions about different attributes within the same text.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-assistant-message",
      "term": "Assistant Message",
      "definition": "In chat APIs, the AI's response in a conversation. Combined with system and user messages to form the complete conversation context for generating the next response.",
      "tags": [
        "API",
        "Technical"
      ]
    },
    {
      "id": "term-a3c",
      "term": "Asynchronous Advantage Actor-Critic (A3C)",
      "definition": "An actor-critic algorithm that runs multiple agent instances in parallel on separate environment copies, each asynchronously updating shared parameters. A3C stabilizes training through decorrelated parallel experience streams.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-async-generation",
      "term": "Asynchronous Generation",
      "definition": "Running multiple AI inference requests in parallel rather than waiting for each to complete. Improves throughput for applications handling many concurrent users.",
      "tags": [
        "Technical",
        "Production"
      ]
    },
    {
      "id": "term-asynchronous-sgd",
      "term": "Asynchronous SGD",
      "definition": "A distributed training approach where workers compute and apply gradients independently without waiting for synchronization, trading gradient staleness for higher throughput. Asynchronous SGD can suffer from convergence issues due to stale updates.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-atrous-convolution",
      "term": "Atrous Convolution",
      "definition": "Also known as dilated convolution, a convolution operation that inserts gaps between filter elements to increase the receptive field without adding parameters or reducing spatial resolution.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-attention-head-pruning",
      "term": "Attention Head Pruning",
      "definition": "A model compression technique that removes redundant or less important attention heads from a multi-head attention mechanism, reducing computation with minimal impact on performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-attention-is-all-you-need",
      "term": "Attention Is All You Need",
      "definition": "The landmark 2017 paper by Vaswani et al. that introduced the transformer architecture, replacing recurrence with self-attention mechanisms and enabling the massive scaling that underpins modern large language models.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-attention-mask",
      "term": "Attention Mask",
      "definition": "A binary or float tensor applied to attention scores before softmax to prevent the model from attending to certain positions, such as padding tokens or future tokens in causal generation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-attention",
      "term": "Attention Mechanism",
      "definition": "A technique that allows models to focus on relevant parts of the input when producing output. The foundation of transformer architecture, enabling models to capture long-range dependencies in text.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-attention-mechanism-history",
      "term": "Attention Mechanism History",
      "definition": "The development of attention mechanisms from Bahdanau et al.'s 2014 neural machine translation work through the self-attention innovation in the 2017 transformer paper, which became the foundation of modern large language models.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-attention-pooling",
      "term": "Attention Pooling",
      "definition": "A pooling mechanism that uses learned attention weights to aggregate features, allowing the model to focus on the most informative elements rather than using fixed averaging or max operations.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-attention-score",
      "term": "Attention Score",
      "definition": "The raw compatibility value computed between a query and key vector, typically via scaled dot product, before softmax normalization, indicating how much one token should attend to another.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-attention-sink",
      "term": "Attention Sink",
      "definition": "A phenomenon where initial tokens in a sequence receive disproportionately high attention scores regardless of content, discovered to be important for maintaining generation quality in streaming and infinite-length settings.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-attention-based-parsing",
      "term": "Attention-Based Parsing",
      "definition": "A parsing approach that uses attention mechanisms from neural networks to determine syntactic structure, often achieving state-of-the-art results by attending over possible head words or constituents.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-attention-based-policy",
      "term": "Attention-Based Policy",
      "definition": "An RL policy architecture that uses attention mechanisms to selectively focus on relevant parts of the observation or memory. Attention-based policies excel in environments with variable-size inputs or complex relational structure.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-attention-based-translation",
      "term": "Attention-Based Translation",
      "definition": "A neural machine translation approach where the decoder attends to different parts of the source sentence at each generation step, eliminating the information bottleneck of fixed-length encoding.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-auc",
      "term": "AUC",
      "definition": "Area Under the ROC Curve, a scalar metric summarizing classifier performance across all thresholds. An AUC of 1.0 indicates perfect classification, while 0.5 indicates performance equivalent to random guessing.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-audio-generation",
      "term": "Audio Generation",
      "definition": "AI that creates speech, music, or sound effects from text or other inputs. Includes text-to-speech (TTS), music generation, and sound design applications.",
      "tags": [
        "Application",
        "Generative"
      ]
    },
    {
      "id": "term-auditability",
      "term": "Auditability",
      "definition": "The property of an AI system that allows independent third parties to examine its data, algorithms, models, and decision-making processes to assess compliance with standards, fairness criteria, and regulatory requirements.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-augmentation",
      "term": "Augmentation",
      "definition": "Expanding training data by creating modified versions of existing examples. In text: paraphrasing, back-translation. In images: rotation, cropping, color changes.",
      "tags": [
        "Training",
        "Data"
      ]
    },
    {
      "id": "term-augmented-dickey-fuller-test",
      "term": "Augmented Dickey-Fuller Test",
      "definition": "A statistical test for determining whether a unit root is present in a time series, which would indicate non-stationarity. A significant test statistic leads to rejection of the null hypothesis of a unit root.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-auto-complete",
      "term": "Auto-Complete",
      "definition": "AI feature that predicts and suggests text as you type. Powers writing assistants, code completion, and search suggestions. Based on language model predictions.",
      "tags": [
        "Application",
        "Feature"
      ]
    },
    {
      "id": "term-autoaugment",
      "term": "AutoAugment",
      "definition": "An automated augmentation policy search method that uses reinforcement learning to find optimal combinations and magnitudes of augmentation operations for a given dataset and task.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-autocorrelation",
      "term": "Autocorrelation",
      "definition": "The correlation of a time series with a lagged version of itself. In regression, autocorrelated residuals violate the independence assumption and can lead to inefficient estimates and unreliable hypothesis tests.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-autocorrelation-function",
      "term": "Autocorrelation Function",
      "definition": "A function that measures the correlation between a time series and a lagged version of itself at various time delays. It is used to identify repeating patterns, periodicity, and appropriate model orders.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-autoencoder",
      "term": "Autoencoder",
      "definition": "A neural network that learns to compress data into a smaller representation and then reconstruct it. Used for dimensionality reduction, denoising, and learning efficient data representations.",
      "tags": [
        "Architecture",
        "Unsupervised"
      ]
    },
    {
      "id": "term-autogen",
      "term": "AutoGen",
      "definition": "Microsoft's framework for building multi-agent AI applications. Enables conversations between multiple AI agents that can collaborate, debate, and solve complex problems together.",
      "tags": [
        "Framework",
        "Application"
      ]
    },
    {
      "id": "term-automatic-chain-of-thought",
      "term": "Automatic Chain-of-Thought",
      "definition": "A method that automatically constructs chain-of-thought demonstrations by clustering questions and selecting representative examples, then using the model to generate reasoning chains, eliminating the need for manual rationale annotation.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-automatic-prompt-engineer",
      "term": "Automatic Prompt Engineer",
      "definition": "An automated method (APE) that uses language models to generate, score, and select optimal prompt instructions for a given task, effectively searching the space of possible prompts to find high-performing candidates without manual engineering.",
      "tags": [
        "Prompt Engineering",
        "Optimization"
      ]
    },
    {
      "id": "term-asr",
      "term": "Automatic Speech Recognition",
      "definition": "The technology that converts spoken language audio into text, using acoustic models, language models, and decoding algorithms to transcribe speech signals.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-automation-bias",
      "term": "Automation Bias",
      "definition": "The human tendency to over-rely on automated systems and accept their outputs without sufficient critical evaluation, even when contradicted by other evidence. This is a significant concern in human-AI teaming.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-auto-ml",
      "term": "AutoML",
      "definition": "Automated machine learning tools that handle model selection, hyperparameter tuning, and feature engineering. Makes ML accessible to non-experts and speeds up development.",
      "tags": [
        "Tools",
        "Automation"
      ]
    },
    {
      "id": "term-autonomous-weapons-systems",
      "term": "Autonomous Weapons Systems",
      "definition": "Weapons systems that can select and engage targets without direct human intervention, raising profound ethical and legal questions about accountability, proportionality, and the role of human judgment in lethal decisions.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-autoregressive",
      "term": "Autoregressive Model",
      "definition": "A model that generates output one element at a time, using previous outputs to predict the next. Most LLMs are autoregressive, generating text token by token from left to right.",
      "tags": [
        "Architecture",
        "LLM"
      ]
    },
    {
      "id": "term-autoregressive-model",
      "term": "Autoregressive Model",
      "definition": "A time series model that predicts the current value as a linear combination of its own past values plus a noise term. The order p specifies how many lagged values are used as predictors.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-auxiliary-loss",
      "term": "Auxiliary Loss",
      "definition": "Additional loss terms added during training to help learning. Can improve training stability, add regularization, or encourage specific behaviors in the model.",
      "tags": [
        "Training",
        "Advanced"
      ]
    },
    {
      "id": "term-auxiliary-task-rl",
      "term": "Auxiliary Task in RL",
      "definition": "An additional prediction or control objective trained alongside the main RL objective to improve representation learning. Auxiliary tasks like pixel prediction, reward prediction, or value replay provide extra gradient signal that enriches learned features.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-average-pooling",
      "term": "Average Pooling",
      "definition": "A technique that reduces data dimensionality by computing the average of regions. Used in CNNs and for creating fixed-size representations from variable-length sequences.",
      "tags": [
        "Architecture",
        "Technique"
      ]
    },
    {
      "id": "term-average-precision",
      "term": "Average Precision",
      "definition": "A single-number summary of the precision-recall curve, computed as the weighted mean of precisions at each threshold with the increase in recall as the weight. It is equivalent to the area under the precision-recall curve.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-awq",
      "term": "AWQ",
      "definition": "Activation-aware Weight Quantization, a method that identifies and preserves salient weight channels based on activation magnitudes, enabling efficient low-bit quantization of large language models.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-aws-inferentia",
      "term": "AWS Inferentia",
      "definition": "Amazon's purpose-built inference accelerator chip designed for high-throughput, low-cost ML inference in the cloud. Inferentia provides up to 2x better throughput per watt than comparable GPU instances for transformer model inference.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-aws-sagemaker",
      "term": "AWS SageMaker",
      "definition": "Amazon's ML platform for building, training, and deploying models. Provides infrastructure, tools, and pre-built algorithms for the complete ML lifecycle.",
      "tags": [
        "Platform",
        "Cloud"
      ]
    },
    {
      "id": "term-aws-trainium",
      "term": "AWS Trainium",
      "definition": "Amazon's custom AI training accelerator chip offering high-performance, cost-effective alternatives to NVIDIA GPUs for deep learning training. Trainium integrates with AWS Neuron SDK and supports distributed training across large clusters.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-azure-openai",
      "term": "Azure OpenAI Service",
      "definition": "Microsoft's enterprise offering of OpenAI models through Azure cloud. Provides GPT-4, ChatGPT, and DALL-E with enterprise security, compliance, and regional availability.",
      "tags": [
        "Platform",
        "Cloud"
      ]
    },
    {
      "id": "term-back-translation",
      "term": "Back-Translation",
      "definition": "A data augmentation technique for machine translation that translates monolingual target-language text back to the source language using a reverse model, creating synthetic parallel training data.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-background-removal",
      "term": "Background Removal",
      "definition": "The process of automatically separating foreground subjects from their background in images using deep learning segmentation and matting models, widely used in photography and e-commerce.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-backpropagation",
      "term": "Backpropagation",
      "definition": "The fundamental algorithm for training neural networks. It calculates how much each weight contributed to the error and adjusts weights accordingly, propagating the error signal backward through the network.",
      "tags": [
        "Training",
        "Neural Networks"
      ]
    },
    {
      "id": "term-backpropagation-history",
      "term": "Backpropagation History",
      "definition": "The development of the backpropagation algorithm for training neural networks, independently discovered multiple times but popularized by Rumelhart, Hinton, and Williams in their influential 1986 Nature paper.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-bag-of-words",
      "term": "Bag of Words",
      "definition": "A simple text representation that counts word occurrences, ignoring order and grammar. Despite its simplicity, still useful for some classification tasks and as a baseline.",
      "tags": [
        "NLP",
        "Representation"
      ]
    },
    {
      "id": "term-bagging",
      "term": "Bagging (Bootstrap Aggregating)",
      "definition": "Training multiple models on random subsets of data and averaging their predictions. Reduces variance and overfitting. The basis for Random Forest algorithms.",
      "tags": [
        "Technique",
        "Ensemble"
      ]
    },
    {
      "id": "term-baichuan",
      "term": "Baichuan",
      "definition": "A series of Chinese bilingual LLMs known for strong performance in Chinese language tasks. Part of the growing ecosystem of non-Western foundation models.",
      "tags": [
        "Model",
        "Chinese AI"
      ]
    },
    {
      "id": "term-bandit-algorithm",
      "term": "Bandit Algorithm",
      "definition": "An algorithm for the multi-armed bandit problem that balances exploration (trying new actions) with exploitation (choosing the best-known action) to maximize cumulative reward over time.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-bandwidth",
      "term": "Bandwidth (AI Context)",
      "definition": "The rate at which data can be transferred, crucial for AI infrastructure. Memory bandwidth often limits GPU performance; network bandwidth affects distributed training.",
      "tags": [
        "Infrastructure",
        "Performance"
      ]
    },
    {
      "id": "term-bandwidth-selection",
      "term": "Bandwidth Selection",
      "definition": "The process of choosing the bandwidth parameter in kernel density estimation, which controls the smoothness of the estimated density. Methods include cross-validation, Silverman's rule of thumb, and plug-in estimators.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-bard",
      "term": "Bard",
      "definition": "Google's conversational AI product, later renamed to Gemini. Competed with ChatGPT using Google's LLM technology and integration with Google services.",
      "tags": [
        "Product",
        "Historical"
      ]
    },
    {
      "id": "term-base-model",
      "term": "Base Model",
      "definition": "A pre-trained model before fine-tuning for specific tasks. Base models are good at text completion but need instruction tuning to become helpful assistants.",
      "tags": [
        "Model Type",
        "Training"
      ]
    },
    {
      "id": "term-baseline",
      "term": "Baseline",
      "definition": "A simple model or approach used as a reference point for comparison. New methods should outperform baselines to demonstrate value. Common baselines include random guessing or simple rules.",
      "tags": [
        "Evaluation",
        "Research"
      ]
    },
    {
      "id": "term-batch",
      "term": "Batch",
      "definition": "A subset of training data processed together in one iteration. Batch processing improves training efficiency and stability compared to processing one example at a time.",
      "tags": [
        "Training",
        "Technical"
      ]
    },
    {
      "id": "term-batch-indexing",
      "term": "Batch Indexing",
      "definition": "The process of building or rebuilding a vector index from a complete dataset in a single operation, producing an optimally structured index that typically offers better search performance than incrementally built alternatives.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ]
    },
    {
      "id": "term-batch-normalization",
      "term": "Batch Normalization",
      "definition": "A technique that normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard deviation, then applying learned scale and shift parameters. It stabilizes and accelerates training.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-batch-normalization-cv",
      "term": "Batch Normalization",
      "definition": "A technique that normalizes layer inputs across a mini-batch during training, reducing internal covariate shift, accelerating convergence, and acting as a regularizer in convolutional neural networks.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-batch-normalization-history",
      "term": "Batch Normalization",
      "definition": "A technique introduced by Ioffe and Szegedy in 2015 that normalizes layer inputs during training, dramatically accelerating deep network training and becoming a standard component of modern neural architectures.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-batch-rl",
      "term": "Batch Reinforcement Learning",
      "definition": "An approach to RL where the agent learns from a fixed batch of pre-collected transitions without online interaction. Batch RL methods like fitted Q-iteration address the challenges of learning from static datasets.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-batch-scheduling",
      "term": "Batch Scheduling for Inference",
      "definition": "The strategy of grouping multiple inference requests together for simultaneous processing on a GPU, improving hardware utilization and throughput. Batch scheduling involves tradeoffs between latency (waiting to fill batches) and throughput (processing more requests per second).",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-batch-size",
      "term": "Batch Size",
      "definition": "The number of training examples processed together before updating model weights. Larger batches provide more stable gradients but require more memory; smaller batches train faster but with more noise.",
      "tags": [
        "Hyperparameter",
        "Training"
      ]
    },
    {
      "id": "term-batched-inference",
      "term": "Batched Inference",
      "definition": "The practice of processing multiple inference requests simultaneously through a model to maximize GPU utilization and throughput, amortizing the cost of loading model weights across many inputs.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-bayes-error-rate",
      "term": "Bayes Error Rate",
      "definition": "The lowest achievable error rate for any classifier on a given classification problem, determined by the irreducible noise in the data. It represents the theoretical performance limit set by the overlap between class distributions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-bayes-theorem",
      "term": "Bayes' Theorem",
      "definition": "A fundamental rule of probability that relates the conditional probability of a hypothesis given evidence to the prior probability of the hypothesis, the likelihood of the evidence, and the marginal probability of the evidence.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-bayesian-inference",
      "term": "Bayesian Inference",
      "definition": "A statistical framework that updates probability estimates for hypotheses as additional evidence is acquired, using Bayes' theorem to compute posterior distributions from prior distributions and observed data.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-bayesian-information-criterion",
      "term": "Bayesian Information Criterion",
      "definition": "A model selection criterion similar to AIC but with a larger penalty for the number of parameters that depends on sample size. It tends to favor simpler models than AIC, especially with large datasets.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-bayesian",
      "term": "Bayesian Methods",
      "definition": "Statistical approaches that incorporate prior knowledge and update beliefs based on evidence. Used for uncertainty quantification, hyperparameter optimization, and probabilistic modeling.",
      "tags": [
        "Statistics",
        "Theory"
      ]
    },
    {
      "id": "term-bayesian-model-averaging",
      "term": "Bayesian Model Averaging",
      "definition": "A technique that accounts for model uncertainty by averaging predictions across multiple models weighted by their posterior probabilities, rather than selecting a single best model.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-bayesian-network",
      "term": "Bayesian Network",
      "definition": "A directed acyclic graph that represents a set of random variables and their conditional dependencies. Each node has a conditional probability table specifying the probability of the node given its parents.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-bayesian-network-history",
      "term": "Bayesian Network History",
      "definition": "The development of Bayesian networks by Judea Pearl and others in the 1980s, providing a graphical framework for representing and reasoning under uncertainty that became central to AI and machine learning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-bayesian-optimization",
      "term": "Bayesian Optimization",
      "definition": "A sequential strategy for optimizing expensive black-box functions that builds a probabilistic surrogate model (typically a Gaussian process) and uses an acquisition function to determine the most promising points to evaluate next.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-beam-search",
      "term": "Beam Search",
      "definition": "A search algorithm used in text generation that maintains multiple candidate sequences at each step, selecting the most promising ones. Balances quality and computational cost compared to exhaustive search.",
      "tags": [
        "Generation",
        "Algorithm"
      ]
    },
    {
      "id": "term-behavior-cloning",
      "term": "Behavior Cloning",
      "definition": "Learning to imitate expert behavior from demonstrations. The model learns to map observations to actions by copying what experts do in similar situations.",
      "tags": [
        "Training",
        "Imitation"
      ]
    },
    {
      "id": "term-beijing-ai-principles",
      "term": "Beijing AI Principles",
      "definition": "A set of AI governance principles released in 2019 by the Beijing Academy of AI, emphasizing harmony, fairness, safety, shared benefits, and responsible development in the Chinese AI governance context.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-bellman-equation",
      "term": "Bellman Equation",
      "definition": "A recursive equation relating the value of a state to the immediate reward plus the discounted value of successor states. The Bellman equation provides the foundation for dynamic programming and most value-based RL algorithms.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-benchmark",
      "term": "Benchmark",
      "definition": "A standardized test or dataset used to evaluate and compare AI model performance. Common LLM benchmarks include MMLU, HellaSwag, and HumanEval for measuring different capabilities.",
      "tags": [
        "Evaluation",
        "Research"
      ]
    },
    {
      "id": "term-benchmark-gaming",
      "term": "Benchmark Gaming",
      "definition": "The practice of optimizing a model specifically to achieve high scores on popular benchmarks without corresponding improvements in real-world capabilities, often through data contamination or overfitting.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-benefit-sharing-in-ai",
      "term": "Benefit Sharing in AI",
      "definition": "The principle that the economic and social benefits generated by AI should be distributed broadly across society rather than concentrated among a small number of developers, companies, or nations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-bernoulli-distribution",
      "term": "Bernoulli Distribution",
      "definition": "The simplest discrete probability distribution, modeling a single trial with two outcomes (success with probability p, failure with probability 1-p). It is the building block of the binomial distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-bert",
      "term": "BERT (Bidirectional Encoder Representations from Transformers)",
      "definition": "A influential language model from Google (2018) that processes text bidirectionally, understanding context from both left and right. Revolutionized NLP and inspired many subsequent models.",
      "tags": [
        "Model",
        "Architecture"
      ]
    },
    {
      "id": "term-bert-release",
      "term": "BERT Release",
      "definition": "Google's Bidirectional Encoder Representations from Transformers model, released in October 2018, which achieved state-of-the-art results across numerous NLP tasks through bidirectional pre-training and established a new paradigm for language understanding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-bertscore",
      "term": "BERTScore",
      "definition": "An evaluation metric that computes the similarity between generated and reference texts using contextual BERT embeddings with greedy token matching, capturing semantic equivalence beyond exact surface-form overlap.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-best-of-n-sampling",
      "term": "Best-of-N Sampling",
      "definition": "An inference strategy that generates N candidate completions and returns the one scoring highest on a reward model, trading increased compute for better output quality without model modification.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-beta-distribution",
      "term": "Beta Distribution",
      "definition": "A continuous probability distribution defined on the interval [0, 1], parametrized by two shape parameters. It is commonly used as a prior distribution for probabilities in Bayesian inference.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-beta-vae",
      "term": "Beta-VAE",
      "definition": "A modification of the variational autoencoder that introduces a hyperparameter beta to weight the KL divergence term, promoting disentangled latent representations when beta is greater than one.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-bev-perception",
      "term": "BEV Perception",
      "definition": "Bird's Eye View perception, a paradigm in autonomous driving that transforms multi-camera or LiDAR data into a unified top-down representation for joint 3D detection, segmentation, and prediction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-bf16",
      "term": "BF16 (Brain Floating Point)",
      "definition": "A 16-bit floating-point format with 8 exponent bits (same as FP32) and 7 mantissa bits, developed by Google Brain. BF16 maintains FP32's dynamic range while halving memory, eliminating the need for loss scaling required by FP16.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-bfloat16",
      "term": "bfloat16",
      "definition": "A 16-bit floating-point format optimized for neural network training. Sacrifices precision for range compared to float16, offering better training stability.",
      "tags": [
        "Technical",
        "Precision"
      ]
    },
    {
      "id": "term-bi-encoder",
      "term": "Bi-Encoder",
      "definition": "A neural retrieval architecture that independently encodes queries and documents into fixed-size vectors using separate or shared encoders, enabling pre-computation of document embeddings and fast similarity search through vector comparison.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-bias",
      "term": "Bias",
      "definition": "Systematic errors or unfair preferences in AI outputs that reflect biases in training data or system design. Can affect fairness across different groups or perspectives.",
      "tags": [
        "Ethics",
        "Fairness"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-estimation-bias",
      "term": "Bias",
      "definition": "In statistical estimation, the difference between the expected value of an estimator and the true value of the parameter being estimated. An unbiased estimator has zero bias.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-bias-score",
      "term": "Bias Score",
      "definition": "A metric that measures the degree of systematic prejudice or unfair treatment in model outputs across demographic groups, assessed through differential response analysis, stereotype association tests, or fairness benchmarks.",
      "tags": [
        "Evaluation",
        "Safety"
      ]
    },
    {
      "id": "term-bias-variance-decomposition",
      "term": "Bias-Variance Decomposition",
      "definition": "A mathematical decomposition of expected prediction error into three components: irreducible noise, squared bias (systematic error), and variance (sensitivity to training data), providing insight into sources of model error.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-bias-variance-tradeoff",
      "term": "Bias-Variance Tradeoff",
      "definition": "The fundamental tension in supervised learning between a model's ability to minimize bias (error from overly simplistic assumptions) and variance (error from sensitivity to small fluctuations in the training set). Reducing one typically increases the other.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-biden-executive-order-on-ai",
      "term": "Biden Executive Order on AI",
      "definition": "Executive Order 14110, issued by President Biden in October 2023, establishing requirements for AI safety and security including red-teaming standards, reporting of large training runs, and federal agency AI governance.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-bidirectional",
      "term": "Bidirectional",
      "definition": "Processing sequences in both directions (left-to-right and right-to-left). BERT processes bidirectionally for understanding; GPT processes unidirectionally for generation.",
      "tags": [
        "Architecture",
        "Processing"
      ]
    },
    {
      "id": "term-bidirectional-rnn",
      "term": "Bidirectional RNN",
      "definition": "A recurrent architecture that processes input sequences in both forward and backward directions simultaneously, combining both context directions to produce richer representations at each time step.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-bigbench",
      "term": "BigBench",
      "definition": "Beyond the Imitation Game Benchmark, a large collaborative benchmark containing over 200 diverse tasks contributed by researchers, designed to probe language model capabilities including reasoning, translation, and social understanding that go beyond standard benchmarks.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-bigbird",
      "term": "BigBird",
      "definition": "A sparse attention transformer that combines random attention, window attention, and global attention patterns to achieve linear complexity while provably maintaining the expressive power of full attention.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-bigram",
      "term": "Bigram / N-gram",
      "definition": "Sequences of N consecutive tokens used in language modeling. Bigrams are pairs; trigrams are triples. N-gram models were dominant before neural approaches but remain useful baselines.",
      "tags": [
        "NLP",
        "Historical"
      ]
    },
    {
      "id": "term-binary-classification",
      "term": "Binary Classification",
      "definition": "A classification task with exactly two possible outcomes (yes/no, spam/not spam). The simplest classification problem, often a building block for more complex tasks.",
      "tags": [
        "ML Task",
        "Classification"
      ]
    },
    {
      "id": "term-binary-quantization",
      "term": "Binary Quantization",
      "definition": "An aggressive vector compression technique that reduces each vector dimension to a single bit based on its sign, enabling 32x compression from float32 and extremely fast Hamming distance comparisons at the cost of reduced recall accuracy.",
      "tags": [
        "Vector Database",
        "Quantization"
      ]
    },
    {
      "id": "term-bing-chat",
      "term": "Bing Chat / Copilot",
      "definition": "Microsoft's AI-powered search assistant, integrating GPT-4 with web search. Can answer questions with citations, create content, and access current information.",
      "tags": [
        "Product",
        "Microsoft"
      ]
    },
    {
      "id": "term-binomial-distribution",
      "term": "Binomial Distribution",
      "definition": "A discrete probability distribution modeling the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. It is parametrized by n (trials) and p (success probability).",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-bio-tagging",
      "term": "BIO Tagging",
      "definition": "An alternative name for IOB tagging format using Begin, Inside, Outside labels for sequence labeling tasks, where B marks the start of an entity and I continues it.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-bioes-tagging",
      "term": "BIOES Tagging",
      "definition": "An extended sequence labeling scheme that adds End and Single tags to the BIO format, providing more precise boundary information for named entities and improving recognition accuracy.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-biometric-ai-regulation",
      "term": "Biometric AI Regulation",
      "definition": "Legal restrictions on AI systems that process biometric data such as facial features, fingerprints, or gait patterns, including bans on real-time biometric surveillance in public spaces under the EU AI Act.",
      "tags": [
        "Privacy",
        "Regulation"
      ]
    },
    {
      "id": "term-bisimulation-metric",
      "term": "Bisimulation Metric",
      "definition": "A distance metric on states that groups together states with similar reward and transition dynamics, providing a principled basis for state abstraction in RL. Deep bisimulation methods learn representations that preserve behaviorally relevant information.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-bit-precision",
      "term": "Bit Precision",
      "definition": "The number of bits used to represent model weights and activations. Lower precision (8-bit, 4-bit) reduces memory and increases speed but may affect accuracy.",
      "tags": [
        "Technical",
        "Optimization"
      ]
    },
    {
      "id": "term-bitter-lesson",
      "term": "Bitter Lesson",
      "definition": "An influential 2019 essay by Rich Sutton arguing that the history of AI shows general methods leveraging computation (search and learning) ultimately outperform approaches that encode human domain knowledge.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-black-box",
      "term": "Black Box",
      "definition": "A system whose internal workings are not visible or understandable to users. Many AI models are considered black boxes because their decision-making processes are difficult to interpret.",
      "tags": [
        "Interpretability",
        "Concept"
      ]
    },
    {
      "id": "term-black-box-problem",
      "term": "Black Box Problem",
      "definition": "The challenge that many AI systems, particularly deep neural networks, operate in ways that are opaque to human understanding, making it difficult to explain, audit, or trust their decisions.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-bletchley-declaration-on-ai",
      "term": "Bletchley Declaration on AI",
      "definition": "A declaration signed by 28 countries at the November 2023 AI Safety Summit at Bletchley Park, acknowledging the potential for serious harm from frontier AI and committing to international cooperation on safety.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-bletchley-park-ai-safety-summit",
      "term": "Bletchley Park AI Safety Summit",
      "definition": "The first major international AI Safety Summit held at Bletchley Park, UK, in November 2023, bringing together governments and AI companies to discuss frontier AI risks and establish international cooperation frameworks.",
      "tags": [
        "History",
        "Governance"
      ]
    },
    {
      "id": "term-bletchley-park-codebreaking",
      "term": "Bletchley Park Codebreaking",
      "definition": "The World War II British codebreaking operation where Alan Turing and colleagues developed the Bombe and Colossus machines to decrypt Axis communications, advancing computational methods that influenced early AI development.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-bleu",
      "term": "BLEU Score",
      "definition": "A metric for evaluating machine translation quality by comparing n-gram overlap with reference translations. While imperfect, it remains widely used for automated translation evaluation.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-bleurt",
      "term": "BLEURT",
      "definition": "A learned evaluation metric that fine-tunes BERT on synthetic and human-rated data to predict text quality scores, providing robust assessments that correlate with human judgments even for paraphrases and semantically equivalent but lexically different outputs.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-bloom",
      "term": "BLOOM",
      "definition": "A large multilingual open-source language model created by BigScience, trained on 46 languages. Demonstrated the viability of collaborative, open AI development.",
      "tags": [
        "Model",
        "Open Source"
      ]
    },
    {
      "id": "term-bm25",
      "term": "BM25",
      "definition": "Best Matching 25, a probabilistic information retrieval ranking function that extends TF-IDF with document length normalization and term frequency saturation for more effective document scoring.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-bm25-in-rag",
      "term": "BM25 in RAG",
      "definition": "The application of the Best Matching 25 probabilistic ranking function within retrieval-augmented generation pipelines, providing strong lexical baseline retrieval that complements dense embedding search for finding documents with exact term matches.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-boltzmann-exploration",
      "term": "Boltzmann Exploration",
      "definition": "An exploration strategy that selects actions with probability proportional to exponentiated Q-values divided by a temperature parameter. Higher temperatures increase randomness while lower temperatures converge toward greedy selection.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-boltzmann-machine",
      "term": "Boltzmann Machine",
      "definition": "A stochastic neural network model invented by Geoffrey Hinton and Terry Sejnowski in 1985 that uses simulated annealing to learn internal representations, representing an important step toward deep learning architectures.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-bonferroni-correction",
      "term": "Bonferroni Correction",
      "definition": "A multiple comparison adjustment that divides the significance level by the number of tests performed, controlling the family-wise error rate. It is conservative but simple to apply.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-boolean-retrieval",
      "term": "Boolean Retrieval",
      "definition": "Search using AND, OR, NOT operators to combine terms. Simple but limited compared to semantic search. Still used in specialized databases and advanced search interfaces.",
      "tags": [
        "Search",
        "Traditional"
      ]
    },
    {
      "id": "term-boosting",
      "term": "Boosting",
      "definition": "An ensemble technique that trains models sequentially, with each new model focusing on examples the previous ones got wrong. Powers XGBoost and LightGBM, popular for tabular data.",
      "tags": [
        "Technique",
        "ML"
      ]
    },
    {
      "id": "term-bootstrap",
      "term": "Bootstrap",
      "definition": "A resampling technique that estimates the sampling distribution of a statistic by repeatedly drawing samples with replacement from the observed data. It provides standard errors, confidence intervals, and bias estimates.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-bootstrap-aggregating",
      "term": "Bootstrap Aggregating",
      "definition": "An ensemble method (also called bagging) that trains multiple models on different bootstrap samples of the training data and combines their predictions by averaging (regression) or voting (classification) to reduce variance.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-bottleneck",
      "term": "Bottleneck",
      "definition": "A narrow layer in a neural network that forces compression of information. Used in autoencoders and some architectures to learn efficient representations.",
      "tags": [
        "Architecture",
        "Design"
      ]
    },
    {
      "id": "term-bottleneck-layer",
      "term": "Bottleneck Layer",
      "definition": "A narrow hidden layer that compresses representations to a lower dimension before expanding them, used in autoencoders and residual blocks to reduce computation and encourage abstraction.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-box-cox-transformation",
      "term": "Box-Cox Transformation",
      "definition": "A family of power transformations parametrized by lambda that aims to stabilize variance and make data more normally distributed. Special cases include the logarithmic transformation (lambda=0) and no transformation (lambda=1).",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-bpe",
      "term": "BPE (Byte Pair Encoding)",
      "definition": "A tokenization algorithm that breaks text into subword units. Starts with individual characters and iteratively merges frequent pairs, balancing vocabulary size with the ability to handle rare words.",
      "tags": [
        "Tokenization",
        "NLP"
      ]
    },
    {
      "id": "term-brain-computer",
      "term": "Brain-Computer Interface (BCI)",
      "definition": "Technology connecting brain signals directly to computers. AI helps interpret neural signals for prosthetics, communication devices, and research applications.",
      "tags": [
        "Application",
        "Neuroscience"
      ]
    },
    {
      "id": "term-brier-score",
      "term": "Brier Score",
      "definition": "A scoring metric that measures the accuracy of probabilistic predictions by computing the mean squared difference between predicted probabilities and actual binary outcomes. Lower values indicate better calibrated predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-browse-mode",
      "term": "Browse Mode",
      "definition": "AI capability to access and retrieve current web information during conversations. Addresses knowledge cutoff limitations by fetching real-time data.",
      "tags": [
        "Feature",
        "Capability"
      ]
    },
    {
      "id": "term-buffer",
      "term": "Buffer (Memory)",
      "definition": "Temporary storage for data being processed. In AI agents, conversation buffers store recent exchanges. In training, data buffers optimize GPU utilization.",
      "tags": [
        "Technical",
        "Architecture"
      ]
    },
    {
      "id": "term-bundle-adjustment",
      "term": "Bundle Adjustment",
      "definition": "An optimization procedure that jointly refines 3D point positions and camera parameters by minimizing the reprojection error across all views, forming the core refinement step in 3D reconstruction pipelines.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-burst",
      "term": "Burst (API)",
      "definition": "Short periods of high API usage that may exceed normal rate limits. Many providers allow bursting with gradual throttling rather than hard cutoffs.",
      "tags": [
        "API",
        "Usage"
      ]
    },
    {
      "id": "term-burstiness",
      "term": "Burstiness",
      "definition": "A statistical property measuring the variability of sentence length and structure in text, often used in AI-generated text detection where machine-generated content tends to show lower burstiness (more uniform patterns) than human writing.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-byte-fallback",
      "term": "Byte Fallback",
      "definition": "A tokenization strategy that encodes unknown characters as individual bytes when they cannot be represented by the learned vocabulary, ensuring all possible inputs can be tokenized.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-bpe-tokenizer",
      "term": "Byte Pair Encoding Tokenizer",
      "definition": "A subword tokenization algorithm that iteratively merges the most frequent pair of adjacent bytes or characters in the training corpus to build a vocabulary of variable-length subword units.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-byte-level-tokenization",
      "term": "Byte-Level Tokenization",
      "definition": "A tokenization approach that operates on raw bytes rather than characters, ensuring complete coverage of any text input without unknown tokens, used in models like GPT-2 with byte-level BPE.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-c2pa",
      "term": "C2PA",
      "definition": "The Coalition for Content Provenance and Authenticity, a joint development foundation creating technical standards for certifying the source and history of media content through cryptographic provenance metadata.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-calibration",
      "term": "Calibration",
      "definition": "The degree to which a model's predicted probabilities match the true frequencies of outcomes. A well-calibrated model predicting 80% probability should be correct approximately 80% of the time for such predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-calibration-data",
      "term": "Calibration Data for Quantization",
      "definition": "A representative subset of input data used to determine optimal quantization parameters such as scaling factors and zero points. Calibration data quality directly affects the accuracy of post-training quantized models.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-calibration-error",
      "term": "Calibration Error",
      "definition": "A metric that measures the discrepancy between a model's predicted confidence and its actual accuracy, where a well-calibrated model's stated probability of being correct closely matches its empirical accuracy at each confidence level.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-calibration-fairness",
      "term": "Calibration Fairness",
      "definition": "A fairness metric requiring that among individuals assigned a given predicted probability, the actual proportion of positive outcomes is the same across all protected groups, ensuring that confidence scores are equally meaningful.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-callback",
      "term": "Callback",
      "definition": "A function called at specific points during training or inference. Used for logging, checkpointing, early stopping, and custom behaviors in ML pipelines.",
      "tags": [
        "Technical",
        "Training"
      ]
    },
    {
      "id": "term-camera-calibration",
      "term": "Camera Calibration",
      "definition": "The process of estimating the intrinsic parameters (focal length, principal point, distortion) and extrinsic parameters (position, orientation) of a camera, essential for accurate 3D reconstruction and measurement.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-capability",
      "term": "Capability (AI)",
      "definition": "A specific skill or function an AI system can perform. Capabilities range from basic (text generation) to advanced (multi-step reasoning, tool use). Understanding capabilities helps set realistic expectations.",
      "tags": [
        "Concept",
        "Assessment"
      ]
    },
    {
      "id": "term-capability-control",
      "term": "Capability Control",
      "definition": "Safety measures that limit what an AI system can do by restricting its access to resources, communication channels, or actuators, as opposed to motivational control which shapes what the system wants to do.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-capsule-network",
      "term": "Capsule Network",
      "definition": "A neural network architecture that uses groups of neurons (capsules) to encode both the presence and instantiation parameters of features, using dynamic routing to model part-whole relationships.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-captioning",
      "term": "Captioning",
      "definition": "Generating text descriptions of images or videos. A multimodal task requiring visual understanding and language generation. Used for accessibility and content organization.",
      "tags": [
        "Task",
        "Multimodal"
      ]
    },
    {
      "id": "term-cataphora",
      "term": "Cataphora",
      "definition": "A linguistic phenomenon where a referring expression precedes the entity it refers to in the text, as in 'Before he arrived, John called ahead,' posing challenges for reference resolution.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-catastrophic-forgetting",
      "term": "Catastrophic Forgetting",
      "definition": "The tendency of neural networks to forget previously learned information when trained on new data. A significant challenge in continual learning and fine-tuning scenarios.",
      "tags": [
        "Training",
        "Challenge"
      ]
    },
    {
      "id": "term-catastrophic-forgetting-ethics",
      "term": "Catastrophic Forgetting Ethics",
      "definition": "Ethical implications of the tendency of neural networks to forget previously learned safety constraints when trained on new data, potentially undermining alignment measures during continued training.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-catastrophic-forgetting-rl",
      "term": "Catastrophic Forgetting in RL",
      "definition": "The tendency of neural network-based RL agents to lose previously learned skills when adapting to new tasks or environments. Continual RL methods use regularization, replay, or modular architectures to mitigate forgetting.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-catastrophic-risk-from-ai",
      "term": "Catastrophic Risk from AI",
      "definition": "The risk that AI systems could cause large-scale irreversible harm falling short of existential risk, such as widespread economic collapse, loss of critical infrastructure, or major environmental damage.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-catboost",
      "term": "CatBoost",
      "definition": "A gradient boosting library that natively handles categorical features using ordered target statistics and employs ordered boosting to reduce prediction shift, yielding strong performance with minimal hyperparameter tuning.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-causal-language-model",
      "term": "Causal Language Model",
      "definition": "A model that predicts the next token based only on previous tokens (left-to-right). GPT and most text generation models are causal. Contrast with bidirectional models like BERT.",
      "tags": [
        "Architecture",
        "LLM"
      ]
    },
    {
      "id": "term-causal-language-modeling",
      "term": "Causal Language Modeling",
      "definition": "A training objective where the model predicts each token based only on the preceding tokens in the sequence, enforcing a left-to-right autoregressive generation order.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-causal-mask",
      "term": "Causal Mask",
      "definition": "A triangular attention mask that prevents each position from attending to subsequent positions, enforcing the autoregressive property required for left-to-right language generation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-cbam",
      "term": "CBAM",
      "definition": "Convolutional Block Attention Module, a lightweight attention module that sequentially applies channel and spatial attention to feature maps, enhancing representational power with minimal overhead.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-cbow",
      "term": "CBOW",
      "definition": "Continuous Bag of Words, a Word2Vec training objective that predicts a center word from the average of its surrounding context word vectors, typically faster to train than Skip-gram.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-ceiling-effect",
      "term": "Ceiling Effect",
      "definition": "When a benchmark becomes too easy to distinguish between models, all scoring near the maximum. Prompts creation of harder benchmarks to continue measuring progress.",
      "tags": [
        "Evaluation",
        "Benchmark"
      ]
    },
    {
      "id": "term-censoring",
      "term": "Censoring",
      "definition": "A condition in survival analysis where the exact time of an event is not observed for some subjects, typically because the study ended or the subject was lost to follow-up. Right censoring is the most common form.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-centernet",
      "term": "CenterNet",
      "definition": "An anchor-free object detection approach that represents objects as center points with associated size and offset predictions, simplifying the detection pipeline by eliminating anchor box design and NMS post-processing.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-central-limit-theorem",
      "term": "Central Limit Theorem",
      "definition": "A fundamental theorem stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's original distribution, provided the variance is finite.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-ctde",
      "term": "Centralized Training Decentralized Execution (CTDE)",
      "definition": "A multi-agent RL paradigm where agents have access to global information during training but must act independently based only on local observations at test time. CTDE bridges the gap between joint and independent learning.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-centroid-based-clustering-vectors",
      "term": "Centroid-Based Clustering for Vectors",
      "definition": "The use of clustering algorithms like k-means to partition a vector collection into groups represented by centroid vectors, forming the basis of IVF indexes where query vectors are first compared to centroids to identify relevant partitions.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-cerebras",
      "term": "Cerebras",
      "definition": "A semiconductor company that produces wafer-scale AI processors (WSE series) with millions of cores and terabytes of on-chip SRAM. Cerebras systems eliminate memory bandwidth bottlenecks by keeping entire large models in on-chip memory.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-chain-of-density",
      "term": "Chain of Density",
      "definition": "A prompting technique that iteratively increases the information density of a summary by asking the model to rewrite it with additional entities while maintaining the same length, producing progressively more concise and entity-rich summaries.",
      "tags": [
        "Prompt Engineering",
        "Summarization"
      ]
    },
    {
      "id": "term-chain-of-code",
      "term": "Chain-of-Code",
      "definition": "A reasoning framework that augments chain-of-thought with executable code generation, allowing the model to write and simulate code execution for reasoning steps that benefit from computation while using natural language for semantic reasoning.",
      "tags": [
        "Prompt Engineering",
        "Code-Augmented"
      ]
    },
    {
      "id": "term-chain-of-knowledge",
      "term": "Chain-of-Knowledge",
      "definition": "A prompting framework that progressively builds and refines a knowledge chain by eliciting relevant facts, verifying their consistency, and reasoning over the accumulated knowledge to produce answers grounded in verified information.",
      "tags": [
        "Prompt Engineering",
        "Knowledge Augmentation"
      ]
    },
    {
      "id": "term-chain-of-table",
      "term": "Chain-of-Table",
      "definition": "A reasoning framework for tabular data that iteratively transforms tables through operations like filtering, sorting, and aggregation as intermediate reasoning steps, with each step producing a new table state that informs the next operation.",
      "tags": [
        "Prompt Engineering",
        "Tabular Reasoning"
      ]
    },
    {
      "id": "term-chain-of-thought",
      "term": "Chain-of-Thought (CoT)",
      "definition": "A prompting technique that encourages AI to show its reasoning process step-by-step, leading to more accurate and transparent responses for complex problems.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-cot-self-consistency",
      "term": "Chain-of-Thought with Self-Consistency",
      "definition": "The combined technique of generating multiple chain-of-thought reasoning paths for a single problem using sampling and selecting the most frequently occurring final answer through majority voting to improve reasoning reliability.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-channel-attention",
      "term": "Channel Attention",
      "definition": "An attention mechanism that learns to weight the importance of different feature channels in a CNN, selectively emphasizing informative channels while suppressing less useful ones.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-character-error-rate",
      "term": "Character Error Rate",
      "definition": "A fine-grained evaluation metric that computes the edit distance between predicted and reference texts at the character level, useful for evaluating OCR systems and speech recognition where partial word matches carry meaningful signal.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-character-n-gram",
      "term": "Character N-gram",
      "definition": "A contiguous sequence of N characters extracted from a word or text, used as features for text classification, language identification, and spelling correction tasks.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-character-level",
      "term": "Character-Level Model",
      "definition": "Models that process text character by character rather than using tokens. More flexible with novel words but typically slower and requiring more parameters for the same capability.",
      "tags": [
        "Architecture",
        "Alternative"
      ]
    },
    {
      "id": "term-charles-babbage",
      "term": "Charles Babbage",
      "definition": "English mathematician and inventor (1791-1871) who conceived the Difference Engine and the Analytical Engine, mechanical general-purpose computers that anticipated key concepts in modern computing and AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-chat-completion",
      "term": "Chat Completion",
      "definition": "An API endpoint type where the model generates responses in a conversational format. Takes a list of messages (system, user, assistant) and returns the next assistant message.",
      "tags": [
        "API",
        "Technical"
      ]
    },
    {
      "id": "term-chat-template",
      "term": "Chat Template",
      "definition": "A structured formatting convention that defines how system messages, user inputs, and assistant responses are tokenized and delimited for multi-turn conversation models.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-chatgpt",
      "term": "ChatGPT",
      "definition": "OpenAI's conversational AI product launched in November 2022. Built on GPT models fine-tuned for dialogue, it popularized conversational AI and sparked widespread public interest in LLMs.",
      "tags": [
        "Product",
        "OpenAI"
      ],
      "link": "chatgpt-guide.html"
    },
    {
      "id": "term-chatgpt-launch",
      "term": "ChatGPT Launch",
      "definition": "OpenAI's release of ChatGPT on November 30, 2022, a conversational AI interface built on GPT-3.5 that reached 100 million users in two months, triggering widespread public engagement with AI and an industry-wide AI race.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-checkpoint",
      "term": "Checkpoint",
      "definition": "A saved snapshot of model weights during training. Enables resuming training after interruption, comparing different training stages, and selecting the best performing version.",
      "tags": [
        "Training",
        "Technical"
      ]
    },
    {
      "id": "term-checkpointing-training",
      "term": "Checkpointing for Training",
      "definition": "The practice of periodically saving model weights, optimizer state, and training metadata to persistent storage during training. Checkpointing enables recovery from hardware failures, job preemptions, and experiment reproduction.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-chi-square-distribution",
      "term": "Chi-Square Distribution",
      "definition": "The distribution of the sum of squares of k independent standard normal random variables. It is used in chi-square tests, confidence interval estimation for variance, and goodness-of-fit tests.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-chi-square-test",
      "term": "Chi-Square Test",
      "definition": "A statistical test that evaluates whether observed frequencies differ significantly from expected frequencies under a null hypothesis. It is used for testing independence between categorical variables and goodness of fit.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-chinchilla",
      "term": "Chinchilla",
      "definition": "A DeepMind model and scaling study showing optimal training requires more data than previously thought. Influenced subsequent model development toward larger datasets.",
      "tags": [
        "Research",
        "Scaling"
      ]
    },
    {
      "id": "term-chinchilla-optimal",
      "term": "Chinchilla Optimal",
      "definition": "A training regime derived from DeepMind's Chinchilla scaling laws, suggesting that for a given compute budget, model size and training data should be scaled proportionally for optimal performance.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-chinchilla-paper",
      "term": "Chinchilla Paper",
      "definition": "The 2022 DeepMind paper by Hoffmann et al. demonstrating that many large language models were undertrained relative to their size, establishing new scaling laws suggesting that training data and model size should be scaled equally.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-chinese-room-argument",
      "term": "Chinese Room Argument",
      "definition": "A thought experiment by John Searle in 1980 arguing that a computer executing a program cannot have genuine understanding or consciousness, even if it perfectly simulates intelligent conversation, challenging strong AI claims.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-chiplet-architecture",
      "term": "Chiplet Architecture",
      "definition": "A processor design approach using multiple small silicon dies (chiplets) connected via high-speed interconnects on a single package. Chiplet designs improve manufacturing yield and enable mixing different process nodes for different functional units.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-chromadb",
      "term": "ChromaDB",
      "definition": "An open-source embedding database designed for AI applications that provides a simple API for storing, querying, and filtering embeddings with associated metadata, popular for prototyping and lightweight RAG implementations.",
      "tags": [
        "Vector Database",
        "Open Source"
      ]
    },
    {
      "id": "term-chunk-overlap",
      "term": "Chunk Overlap",
      "definition": "The number of tokens or characters shared between consecutive chunks during document splitting, ensuring that information spanning chunk boundaries is not lost and maintaining contextual continuity across adjacent segments.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-chunk-size",
      "term": "Chunk Size",
      "definition": "The target length of individual text segments produced during document chunking, typically measured in tokens or characters, where smaller chunks enable more precise retrieval while larger chunks preserve more context and coherence.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-chunking",
      "term": "Chunking",
      "definition": "Splitting long documents into smaller pieces for processing. Essential for RAG and embedding systems where input length exceeds model limits or affects retrieval quality.",
      "tags": [
        "Technique",
        "Processing"
      ]
    },
    {
      "id": "term-chunking-nlp",
      "term": "Chunking",
      "definition": "A shallow parsing technique that groups consecutive words into non-overlapping phrases (chunks) such as noun phrases or verb phrases without building a full parse tree.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-cider",
      "term": "CIDEr",
      "definition": "Consensus-based Image Description Evaluation, a metric that measures image captioning quality using TF-IDF weighted n-gram similarity between generated and reference captions, emphasizing informative words that distinguish specific images from the corpus.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-citation-generation",
      "term": "Citation Generation",
      "definition": "The capability of a language model to produce inline references to source documents that support its claims, enabling users to verify the accuracy of generated content.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-cky-algorithm",
      "term": "CKY Algorithm",
      "definition": "Cocke-Kasami-Younger algorithm, a dynamic programming parser for context-free grammars that builds parse trees bottom-up in O(n^3) time by filling a chart of possible constituents.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-clarity",
      "term": "Clarity (Prompting)",
      "definition": "Using clear, unambiguous language in prompts to reduce misinterpretation. Specific instructions and explicit requirements improve response quality.",
      "tags": [
        "Prompting",
        "Best Practice"
      ],
      "link": "../learn/prompt-basics.html"
    },
    {
      "id": "term-class-activation-map",
      "term": "Class Activation Map",
      "definition": "A visualization technique that highlights the image regions most important for a CNN's classification decision, computed by weighting feature map activations by the classification layer's weights.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-class-imbalance",
      "term": "Class Imbalance",
      "definition": "When training data has unequal representation across categories. Can cause models to favor majority classes. Addressed through sampling, weighting, or specialized techniques.",
      "tags": [
        "Data",
        "Challenge"
      ]
    },
    {
      "id": "term-class-weight",
      "term": "Class Weight",
      "definition": "A technique for handling class imbalance by assigning higher weight to the minority class in the loss function, effectively making misclassification of underrepresented classes more costly during training.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-classification",
      "term": "Classification",
      "definition": "A machine learning task that assigns input data to predefined categories. Examples include spam detection (spam/not spam), sentiment analysis (positive/negative/neutral), and image recognition.",
      "tags": [
        "ML Task",
        "Supervised"
      ]
    },
    {
      "id": "term-classifier-free-guidance",
      "term": "Classifier-Free Guidance",
      "definition": "A technique for conditional diffusion models that interpolates between conditional and unconditional score estimates during sampling, controlling the trade-off between sample quality and diversity without a separate classifier.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-claude",
      "term": "Claude",
      "definition": "An AI assistant created by Anthropic, designed to be helpful, harmless, and honest. Known for nuanced reasoning, long context handling, and strong performance on complex tasks.",
      "tags": [
        "Product",
        "Anthropic"
      ]
    },
    {
      "id": "term-claude-instant",
      "term": "Claude Instant / Haiku",
      "definition": "Anthropic's faster, more cost-effective models for simpler tasks. Trade some capability for speed and lower cost, suitable for classification, extraction, and basic chat.",
      "tags": [
        "Model",
        "Anthropic"
      ]
    },
    {
      "id": "term-claude-launch",
      "term": "Claude Launch",
      "definition": "Anthropic's release of Claude, a family of AI assistants trained using Constitutional AI methods, first made available in March 2023, emphasizing safety, helpfulness, and harmlessness in conversational AI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-claude-opus",
      "term": "Claude Opus",
      "definition": "Anthropic's most capable model, designed for complex reasoning, creative tasks, and nuanced understanding. Higher cost but best performance on difficult tasks.",
      "tags": [
        "Model",
        "Anthropic"
      ]
    },
    {
      "id": "term-claude-shannon",
      "term": "Claude Shannon",
      "definition": "American mathematician (1916-2001) known as the father of information theory, whose 1948 paper established the mathematical foundations for digital communication and contributed foundational ideas to early AI research.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-clip",
      "term": "CLIP",
      "definition": "Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision by training image and text encoders jointly to match images with their text descriptions.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-clipped-surrogate-objective",
      "term": "Clipped Surrogate Objective",
      "definition": "The core optimization objective in PPO that clips the probability ratio between new and old policies, preventing excessively large updates. This simple mechanism provides trust-region-like stability without the computational cost of TRPO.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-cloud-computing-ai",
      "term": "Cloud Computing for AI",
      "definition": "The use of cloud infrastructure services (AWS, GCP, Azure) for AI model training and inference, providing on-demand access to GPU clusters without capital expenditure. Cloud AI services range from raw GPU instances to fully managed training and serving platforms.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-cloze",
      "term": "Cloze Task",
      "definition": "A task where models predict missing words in text. A classic NLP benchmark and training objective. BERT's masked language modeling is a form of cloze task.",
      "tags": [
        "Task",
        "Evaluation"
      ]
    },
    {
      "id": "term-clustering",
      "term": "Clustering",
      "definition": "An unsupervised learning technique that groups similar data points together without predefined labels. Used for customer segmentation, document organization, and pattern discovery.",
      "tags": [
        "ML Task",
        "Unsupervised"
      ]
    },
    {
      "id": "term-cmu-ai-research",
      "term": "CMU AI Research",
      "definition": "Carnegie Mellon University's AI research programs, including the work of Allen Newell and Herbert Simon, the development of expert systems, and pioneering contributions to robotics, speech recognition, and autonomous vehicles.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-cnn",
      "term": "CNN (Convolutional Neural Network)",
      "definition": "A neural network architecture designed for processing grid-like data such as images. Uses convolutional layers to automatically learn spatial hierarchies of features.",
      "tags": [
        "Architecture",
        "Computer Vision"
      ]
    },
    {
      "id": "term-co-occurrence-matrix",
      "term": "Co-occurrence Matrix",
      "definition": "A matrix recording how often pairs of words appear together within a defined context window across a corpus, used as the basis for distributional word representations like GloVe.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-coco-dataset",
      "term": "COCO Dataset",
      "definition": "Common Objects in Context, a large-scale benchmark dataset containing images with annotations for object detection, instance segmentation, keypoint detection, and image captioning across 80 object categories.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-code-generation",
      "term": "Code Generation",
      "definition": "The ability of AI models to write programming code from natural language descriptions. Powers tools like GitHub Copilot, Cursor, and code-focused features in general LLMs.",
      "tags": [
        "Application",
        "Development"
      ]
    },
    {
      "id": "term-code-generation-prompting",
      "term": "Code Generation Prompting",
      "definition": "Specialized prompting techniques for producing high-quality code, incorporating language specification, function signatures, docstrings, test cases, and algorithmic constraints to guide models toward correct and efficient implementations.",
      "tags": [
        "Prompt Engineering",
        "Code"
      ]
    },
    {
      "id": "term-code-interpreter",
      "term": "Code Interpreter",
      "definition": "AI capability to write and execute code, enabling data analysis, visualization, and computation. ChatGPT's code interpreter runs Python in a sandbox environment.",
      "tags": [
        "Feature",
        "Tool Use"
      ],
      "link": "chatgpt-guide.html"
    },
    {
      "id": "term-code-llm",
      "term": "Code LLM",
      "definition": "Language models specialized for programming tasks. Examples include Codex, StarCoder, and Code Llama. Often trained on large code corpora from GitHub and similar sources.",
      "tags": [
        "Model Type",
        "Specialized"
      ]
    },
    {
      "id": "term-code-switching",
      "term": "Code-Switching",
      "definition": "The phenomenon of alternating between two or more languages within a single conversation or utterance, posing challenges for NLP systems designed for monolingual text processing.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-codebleu",
      "term": "CodeBLEU",
      "definition": "A code evaluation metric that extends BLEU with code-specific components including abstract syntax tree matching, data-flow analysis, and weighted n-gram matching, capturing both syntactic correctness and semantic similarity of generated code.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-cognitive-load",
      "term": "Cognitive Load (Prompting)",
      "definition": "The mental effort required to process complex prompts. Simpler, well-organized prompts often yield better results by reducing the model's processing burden.",
      "tags": [
        "Prompting",
        "Best Practice"
      ]
    },
    {
      "id": "term-cohens-kappa",
      "term": "Cohen's Kappa",
      "definition": "A statistic measuring inter-rater agreement for categorical items that accounts for agreement occurring by chance. Values range from -1 to 1, with 1 indicating perfect agreement beyond chance.",
      "tags": [
        "Statistics",
        "Metrics"
      ]
    },
    {
      "id": "term-cohere",
      "term": "Cohere",
      "definition": "An enterprise AI company providing LLMs for text generation, embeddings, and search. Known for Command models and focus on enterprise use cases with strong RAG capabilities.",
      "tags": [
        "Company",
        "LLM Provider"
      ]
    },
    {
      "id": "term-coherence-modeling",
      "term": "Coherence Modeling",
      "definition": "The computational assessment of how well sentences in a text flow together logically and topically, evaluating whether a text reads naturally and maintains consistent themes and references.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-coherence-score",
      "term": "Coherence Score",
      "definition": "An evaluation metric that assesses the logical consistency and semantic flow of generated text, measuring whether ideas connect naturally, maintain topical consistency, and form a well-structured narrative.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-cointegration",
      "term": "Cointegration",
      "definition": "A statistical property of two or more non-stationary time series that share a common stochastic trend, meaning a linear combination of them is stationary. It implies a long-run equilibrium relationship.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-colbert",
      "term": "ColBERT",
      "definition": "A late-interaction retrieval model that independently encodes queries and documents into per-token embeddings, then scores relevance through efficient MaxSim operations between the two sets of embeddings.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-cold-start",
      "term": "Cold Start Problem",
      "definition": "Difficulty making predictions for new users or items with no historical data. Common in recommendation systems. Addressed with hybrid approaches combining collaborative and content-based methods.",
      "tags": [
        "Challenge",
        "Recommendations"
      ]
    },
    {
      "id": "term-collaborative-filtering",
      "term": "Collaborative Filtering",
      "definition": "Recommendation technique based on user behavior patterns. \"Users who liked X also liked Y.\" Forms the basis of many recommendation systems at Netflix, Amazon, etc.",
      "tags": [
        "Technique",
        "Recommendations"
      ]
    },
    {
      "id": "term-collection",
      "term": "Collection",
      "definition": "A named grouping of vectors and their associated metadata within a vector database, analogous to a table in relational databases, serving as the primary organizational unit for storing and querying related embeddings.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-collective-communication",
      "term": "Collective Communication",
      "definition": "Coordinated data exchange patterns among multiple processes or GPUs, including all-reduce, all-gather, reduce-scatter, and broadcast. Efficient collective communication is fundamental to scaling distributed AI training.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-collocation",
      "term": "Collocation",
      "definition": "A sequence of words that co-occur more frequently than expected by chance, forming conventional expressions such as 'strong coffee' or 'make a decision' that are identified through statistical measures.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-colossus-computer",
      "term": "Colossus Computer",
      "definition": "The world's first programmable electronic digital computer, built at Bletchley Park in 1943-1944 to break German Lorenz cipher messages, representing a crucial step toward the general-purpose computers needed for AI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-command-model",
      "term": "Command Model",
      "definition": "Cohere's instruction-tuned LLMs optimized for following commands and business applications. Includes Command R for RAG and enterprise use cases.",
      "tags": [
        "Model",
        "Cohere"
      ]
    },
    {
      "id": "term-common-crawl",
      "term": "Common Crawl",
      "definition": "A massive open repository of web data used to train many LLMs. Contains petabytes of text crawled from the internet, requiring careful filtering for quality and safety.",
      "tags": [
        "Data",
        "Training"
      ]
    },
    {
      "id": "term-commonsense-reasoning",
      "term": "Commonsense Reasoning",
      "definition": "AI's ability to understand everyday knowledge humans take for granted. That water is wet, objects fall down, people need sleep. A challenging area where LLMs have improved dramatically.",
      "tags": [
        "Capability",
        "Reasoning"
      ]
    },
    {
      "id": "term-communication-marl",
      "term": "Communication in Multi-Agent RL",
      "definition": "Protocols and mechanisms that allow agents in a multi-agent system to share information through learned communication channels. Emergent communication can develop structured language-like properties through reinforcement learning.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-communication-overlap",
      "term": "Communication Overlap",
      "definition": "A distributed training optimization that overlaps gradient communication with backward pass computation, hiding communication latency behind useful work. Bucketed all-reduce and asynchronous communication enable effective overlap.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-competitive-rl",
      "term": "Competitive Reinforcement Learning",
      "definition": "A multi-agent RL setting where agents have opposing objectives, such as zero-sum games. Competitive RL involves finding Nash equilibria and developing strategies robust to adversarial opponents.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-compile-time-graph-optimization",
      "term": "Compile-Time Graph Optimization",
      "definition": "Static optimization of computation graphs before execution, including constant folding, dead code elimination, and operator fusion. Ahead-of-time compilation produces more efficient execution plans than dynamic interpretation.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-completion",
      "term": "Completion",
      "definition": "Text generated by an AI to continue a given prompt. The basic operation of language models: given input text, predict what comes next.",
      "tags": [
        "Task",
        "Fundamentals"
      ]
    },
    {
      "id": "term-complexity-based-prompting",
      "term": "Complexity-Based Prompting",
      "definition": "A self-consistency variant that selects the final answer from reasoning chains with the highest complexity, measured by the number of reasoning steps, based on the observation that more detailed reasoning chains tend to produce more accurate answers.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-compositionality",
      "term": "Compositionality",
      "definition": "The principle that the meaning of a complex expression is determined by the meanings of its parts and the rules used to combine them, a foundational concept in formal semantics.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-compression",
      "term": "Compression (Model)",
      "definition": "Reducing model size while maintaining performance. Techniques include quantization, pruning, and distillation. Enables deployment on edge devices and reduces costs.",
      "tags": [
        "Optimization",
        "Deployment"
      ]
    },
    {
      "id": "term-compute",
      "term": "Compute",
      "definition": "Computational resources required for training and running AI models. Measured in FLOPs, GPU-hours, or dollars. A primary constraint and cost driver in AI development.",
      "tags": [
        "Infrastructure",
        "Resources"
      ]
    },
    {
      "id": "term-compute-governance",
      "term": "Compute Governance",
      "definition": "Policy approaches that use computational resources as a lever for AI governance, including monitoring large training runs, export controls on AI chips, and reporting requirements for compute-intensive AI development.",
      "tags": [
        "Governance",
        "AI Safety"
      ]
    },
    {
      "id": "term-compute-bound",
      "term": "Compute-Bound Workload",
      "definition": "A processing task where performance is limited by the rate of arithmetic computation rather than memory bandwidth or I/O. Training large models with large batch sizes is typically compute-bound, benefiting from more FLOPS capacity.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ]
    },
    {
      "id": "term-compute-optimal-training",
      "term": "Compute-Optimal Training",
      "definition": "An approach to model training that seeks the best allocation of a fixed compute budget between model parameters and training tokens, based on empirical scaling law research.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-computer-vision",
      "term": "Computer Vision",
      "definition": "The field of AI that enables machines to interpret and understand visual information from images and videos. Applications include object detection, facial recognition, and medical imaging.",
      "tags": [
        "Field",
        "Images"
      ]
    },
    {
      "id": "term-computer-vision-history",
      "term": "Computer Vision History",
      "definition": "The evolution of computer vision from early edge detection and pattern recognition in the 1960s through feature-based methods like SIFT and HOG to the deep learning revolution triggered by AlexNet in 2012.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-concept-drift",
      "term": "Concept Drift",
      "definition": "When the relationship between input and output changes over time, causing model performance to degrade. Requires monitoring and retraining to maintain accuracy.",
      "tags": [
        "Challenge",
        "Production"
      ]
    },
    {
      "id": "term-conditional-gan",
      "term": "Conditional GAN",
      "definition": "A GAN variant where both generator and discriminator receive additional conditioning information such as class labels or text, enabling controlled generation of specific categories or attributes.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-conditional-generation",
      "term": "Conditional Generation",
      "definition": "Generating content based on specific conditions or inputs. Image generation conditioned on text, or text generation conditioned on a topic or style.",
      "tags": [
        "Technique",
        "Generation"
      ]
    },
    {
      "id": "term-crf",
      "term": "Conditional Random Field",
      "definition": "A discriminative probabilistic model for sequence labeling that models the conditional probability of label sequences given observations, capturing dependencies between adjacent labels.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-confabulation",
      "term": "Confabulation",
      "definition": "Another term for hallucinationwhen AI generates plausible but false information. The model \"fills in gaps\" with invented content that sounds convincing.",
      "tags": [
        "Risk",
        "Limitation"
      ],
      "link": "../tools/hallucination.html"
    },
    {
      "id": "term-confidence-interval",
      "term": "Confidence Interval",
      "definition": "A range of values constructed from sample data that, if the sampling procedure were repeated many times, would contain the true population parameter a specified percentage (e.g., 95%) of the time.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-confidence-score",
      "term": "Confidence Score",
      "definition": "A numerical value indicating how certain a model is about its prediction or output. Higher scores suggest the model is more sure, though confidence doesn't always correlate with accuracy.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-confidence-threshold-cv",
      "term": "Confidence Threshold",
      "definition": "The minimum prediction score required to accept a detection as valid, balancing between missing true detections (high threshold) and including false positives (low threshold).",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-confirmation-bias-in-ai",
      "term": "Confirmation Bias in AI",
      "definition": "The tendency for AI developers or users to favor data, model outputs, or evaluation criteria that confirm pre-existing beliefs, leading to biased system design and selective interpretation of results.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-conformer",
      "term": "Conformer",
      "definition": "A speech processing architecture that combines convolution and transformer modules in each block, capturing both local and global dependencies for improved automatic speech recognition.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-conformity-assessment-for-ai",
      "term": "Conformity Assessment for AI",
      "definition": "The formal evaluation process required under the EU AI Act to verify that high-risk AI systems meet regulatory requirements before being placed on the market, including both self-assessment and third-party audit pathways.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-confounding-variable",
      "term": "Confounding Variable",
      "definition": "A variable that influences both the independent and dependent variables, creating a spurious association between them. Failure to control for confounders can lead to incorrect causal conclusions.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-confusion-matrix",
      "term": "Confusion Matrix",
      "definition": "A table showing correct and incorrect predictions for each class. Reveals where a classification model makes mistakes, enabling targeted improvements.",
      "tags": [
        "Evaluation",
        "Visualization"
      ]
    },
    {
      "id": "term-conjugate-prior",
      "term": "Conjugate Prior",
      "definition": "A prior distribution that, when combined with a particular likelihood function via Bayes' theorem, yields a posterior distribution in the same family as the prior. It simplifies Bayesian computations to closed-form updates.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-connectionism-vs-symbolism",
      "term": "Connectionism vs Symbolism",
      "definition": "The historical debate in AI between connectionist approaches using neural networks that learn distributed representations and symbolic approaches using explicit rules and logic for knowledge representation and reasoning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-consent-laundering",
      "term": "Consent Laundering",
      "definition": "The practice of obtaining user consent for data collection through opaque terms of service and then repurposing that data for AI training in ways that users neither anticipated nor meaningfully agreed to.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-conservative-q-learning",
      "term": "Conservative Q-Learning (CQL)",
      "definition": "An offline RL algorithm that adds a regularizer to penalize Q-values for out-of-distribution actions, producing conservative value estimates that avoid overestimation of unseen state-action pairs. CQL provides lower-bound guarantees on policy performance.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-consistency",
      "term": "Consistency",
      "definition": "A property of a statistical estimator indicating that it converges in probability to the true parameter value as the sample size approaches infinity. Consistent estimators become arbitrarily accurate with enough data.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-consistency-model",
      "term": "Consistency Model",
      "definition": "A generative model that learns to map any point along a diffusion trajectory directly to the trajectory's starting point, enabling one-step or few-step generation without iterative denoising.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-consistency-based-self-evaluation",
      "term": "Consistency-Based Self-Evaluation",
      "definition": "An evaluation method where a language model assesses the quality of its own outputs by generating multiple responses and measuring agreement across them, using high consistency as a proxy for confidence and correctness.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ]
    },
    {
      "id": "term-constituency-parsing",
      "term": "Constituency Parsing",
      "definition": "The task of analyzing sentence structure by breaking it into hierarchical nested constituents (phrases) according to a grammar, producing a tree showing how words group into larger syntactic units.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-constitutional-ai",
      "term": "Constitutional AI",
      "definition": "Anthropic's approach to AI alignment where models are trained to follow a set of principles (\"constitution\") that guide their behavior. Reduces reliance on human feedback for safety training.",
      "tags": [
        "Safety",
        "Anthropic"
      ]
    },
    {
      "id": "term-constitutional-ai-training",
      "term": "Constitutional AI Training",
      "definition": "A training methodology where the model critiques and revises its own outputs according to a set of written principles, reducing reliance on human feedback for alignment.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-constitutional-prompting",
      "term": "Constitutional Prompting",
      "definition": "A prompting approach that provides the model with an explicit set of principles, rules, or constitutional guidelines that it must follow when generating responses, enabling value-aligned outputs through declarative constraint specification.",
      "tags": [
        "Prompt Engineering",
        "Safety"
      ]
    },
    {
      "id": "term-constrained-beam-search",
      "term": "Constrained Beam Search",
      "definition": "A beam search variant that enforces lexical or structural constraints during decoding, ensuring that certain tokens or phrases must appear in the generated output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-constrained-rl",
      "term": "Constrained Reinforcement Learning",
      "definition": "An RL formulation where the agent maximizes expected return while satisfying one or more constraint functions on expected costs. Constrained MDPs are solved using Lagrangian methods or primal-dual optimization.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ]
    },
    {
      "id": "term-constraint",
      "term": "Constraint (Prompting)",
      "definition": "Limitations or requirements specified in a prompt. \"Respond in 50 words or less\" or \"Use only formal language.\" Constraints shape and focus AI output.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-constraint-prompting",
      "term": "Constraint Prompting",
      "definition": "A technique that specifies explicit constraints within the prompt such as length limits, format requirements, vocabulary restrictions, or content boundaries that the model must satisfy in its generated output.",
      "tags": [
        "Prompt Engineering",
        "Constraints"
      ]
    },
    {
      "id": "term-content-authenticity-initiative",
      "term": "Content Authenticity Initiative",
      "definition": "An industry coalition led by Adobe that develops open standards for attributing and verifying the provenance of digital content, helping distinguish authentic media from AI-generated or manipulated material.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-content-filtering",
      "term": "Content Filtering",
      "definition": "Systems that detect and block harmful content in AI inputs or outputs. Part of safety infrastructure, filtering violence, explicit content, and other policy violations.",
      "tags": [
        "Safety",
        "Moderation"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-content-moderation",
      "term": "Content Moderation",
      "definition": "The process of monitoring and filtering user-generated content on digital platforms to enforce community standards, increasingly assisted by AI classifiers for detecting hate speech, violence, and other policy violations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-context",
      "term": "Context",
      "definition": "Background information provided to AI that helps it understand your situation and needs. Essential for getting relevant, accurate responses.",
      "tags": [],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-context-distillation",
      "term": "Context Distillation",
      "definition": "A training technique that transfers the behavior elicited by a specific prompt or context into the model's weights, eliminating the need to include that context at inference time.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-context-length",
      "term": "Context Length",
      "definition": "The maximum amount of text a model can process at once, measured in tokens. Ranges from 4K to 200K+ depending on the model. Longer contexts enable more complex tasks.",
      "tags": [
        "Specification",
        "Limitation"
      ]
    },
    {
      "id": "term-context-parallelism",
      "term": "Context Parallelism",
      "definition": "A specialized parallelism approach that distributes attention computation across GPUs along the sequence length dimension for very long context windows. Context parallelism enables processing of sequences that exceed the memory capacity of a single device.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-context-window",
      "term": "Context Window",
      "definition": "The amount of text (measured in tokens) that an AI can process at once. Modern models range from 4K to 200K+ tokens, determining how much conversation history and reference material the AI can consider.",
      "tags": [
        "Limitation",
        "Architecture"
      ]
    },
    {
      "id": "term-context-window-management",
      "term": "Context Window Management",
      "definition": "Techniques for efficiently utilizing and extending the finite context window of language models, including sliding windows, summarization of earlier context, and hierarchical memory systems.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-context-free-grammar",
      "term": "Context-Free Grammar",
      "definition": "A formal grammar where production rules map single non-terminal symbols to sequences of terminals and non-terminals, widely used in NLP for defining syntactic structure of sentences.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-contextual-bandit",
      "term": "Contextual Bandit",
      "definition": "An extension of the multi-armed bandit where the agent observes a context (feature vector) before choosing an action, allowing the policy to adapt its decisions to the current situation. Used extensively in recommendation systems and online advertising.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-contextual-calibration",
      "term": "Contextual Calibration",
      "definition": "A technique that adjusts a language model's output probabilities by estimating and correcting for biases introduced by the prompt context, typically by measuring the model's prior distribution on content-free inputs and applying an affine transformation.",
      "tags": [
        "Prompt Engineering",
        "Calibration"
      ]
    },
    {
      "id": "term-contextual-compression",
      "term": "Contextual Compression",
      "definition": "A retrieval post-processing technique that compresses or extracts only the most relevant portions from retrieved documents based on the query context, reducing noise and token usage by filtering out irrelevant content before passing to the LLM.",
      "tags": [
        "Retrieval",
        "Post-Processing"
      ]
    },
    {
      "id": "term-contextual-embedding",
      "term": "Contextual Embedding",
      "definition": "A word representation that varies depending on the surrounding context, unlike static embeddings, capturing polysemy and context-dependent meaning through models like ELMo, BERT, and GPT.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-contextual-few-shot-selection",
      "term": "Contextual Few-Shot Selection",
      "definition": "The practice of dynamically selecting the most relevant few-shot examples for each query based on semantic similarity, task characteristics, or diversity criteria rather than using a fixed set of demonstrations.",
      "tags": [
        "Prompt Engineering",
        "Example Selection"
      ]
    },
    {
      "id": "term-contextual-retrieval",
      "term": "Contextual Retrieval",
      "definition": "A retrieval enhancement technique that prepends each chunk with a model-generated contextual summary explaining the chunk's place within the larger document, improving retrieval accuracy by providing disambiguation context for each embedded segment.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-continual-learning",
      "term": "Continual Learning",
      "definition": "Training models incrementally on new data without forgetting previous knowledge. A challenge because neural networks tend to overwrite old information with new.",
      "tags": [
        "Training",
        "Research"
      ]
    },
    {
      "id": "term-continual-rl",
      "term": "Continual Reinforcement Learning",
      "definition": "RL settings where the agent must learn and adapt over a non-stationary sequence of tasks without forgetting earlier knowledge. Continual RL combines aspects of lifelong learning, transfer, and plasticity maintenance.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-continuous-batching",
      "term": "Continuous Batching",
      "definition": "A dynamic batching strategy where new requests are inserted into a running batch as soon as existing requests complete, eliminating idle GPU time caused by waiting for entire batches to finish.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-contractive-autoencoder",
      "term": "Contractive Autoencoder",
      "definition": "An autoencoder that adds a penalty term based on the Frobenius norm of the encoder's Jacobian matrix, encouraging the learned representation to be robust to small perturbations in the input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-contrastive-chain-of-thought",
      "term": "Contrastive Chain-of-Thought",
      "definition": "A prompting approach that provides both correct and incorrect reasoning examples in demonstrations, helping the model learn not only the right reasoning patterns but also common mistakes to avoid during inference.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-contrastive-decoding",
      "term": "Contrastive Decoding",
      "definition": "A decoding method that improves generation quality by contrasting the output distributions of a large expert model and a smaller amateur model, suppressing tokens favored by the weaker model.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-contrastive-learning",
      "term": "Contrastive Learning",
      "definition": "Training by comparing similar and dissimilar examples. The model learns to place similar items close together in embedding space and dissimilar items far apart.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-contrastive-learning-vision",
      "term": "Contrastive Learning for Vision",
      "definition": "A self-supervised approach that trains visual encoders by pulling augmented views of the same image closer in embedding space while pushing different images apart, learning useful representations without labels.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-contrastive-loss",
      "term": "Contrastive Loss",
      "definition": "A loss function that trains models to pull similar (positive) pairs closer together and push dissimilar (negative) pairs further apart in the embedding space, based on a specified distance margin.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-control-problem",
      "term": "Control Problem",
      "definition": "The challenge of ensuring that a highly capable AI system remains under meaningful human control and pursues objectives aligned with human values, even as its capabilities may exceed human oversight capacity.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-controllable-generation",
      "term": "Controllable Generation",
      "definition": "Techniques for steering AI output toward desired attributes like sentiment, style, or topic. Enables more precise control over generated content.",
      "tags": [
        "Technique",
        "Generation"
      ]
    },
    {
      "id": "term-controlnet",
      "term": "ControlNet",
      "definition": "A neural network architecture that adds spatial conditioning controls to pre-trained diffusion models, enabling guided image generation from edge maps, depth maps, poses, and other structural inputs.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-convergence",
      "term": "Convergence",
      "definition": "The property of an optimization algorithm or iterative process where successive iterations produce results that approach a stable solution or fixed point. In ML, it indicates that training loss has stabilized.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-conversation-history",
      "term": "Conversation History",
      "definition": "The record of previous messages in a chat session. Provides context for AI responses. Managing history is important as it consumes context window space.",
      "tags": [
        "Feature",
        "Context"
      ]
    },
    {
      "id": "term-conversational-ai",
      "term": "Conversational AI",
      "definition": "AI systems designed for natural dialogue with humans. Includes chatbots, virtual assistants, and systems like ChatGPT and Claude that can maintain context across multiple exchanges.",
      "tags": [
        "Application",
        "NLP"
      ],
      "link": "chatgpt-guide.html"
    },
    {
      "id": "term-convolutional",
      "term": "Convolution",
      "definition": "A mathematical operation that slides a filter over input to detect patterns. The core of CNNs, effective for images by learning local features like edges and textures.",
      "tags": [
        "Operation",
        "Architecture"
      ]
    },
    {
      "id": "term-convolutional-filter",
      "term": "Convolutional Filter",
      "definition": "A learnable weight matrix (kernel) that slides across an input image or feature map, computing element-wise products and sums to detect specific patterns such as edges, textures, or shapes.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-cnn-history",
      "term": "Convolutional Neural Network History",
      "definition": "The development of CNNs from Fukushima's Neocognitron in 1980 through LeCun's application to handwritten digit recognition in 1989, culminating in their dominance of computer vision following the 2012 ImageNet breakthrough.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-cooks-distance",
      "term": "Cook's Distance",
      "definition": "A measure of the influence of each observation on the fitted values of a regression model, computed as the sum of changes in all predicted values when the observation is removed. High values indicate influential points.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-cooperative-inverse-reinforcement-learning",
      "term": "Cooperative Inverse Reinforcement Learning",
      "definition": "A framework for human-AI alignment where a robot and human work together in a game where the robot tries to maximize the human's reward while being uncertain about what that reward is, learning through interaction.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-cooperative-rl",
      "term": "Cooperative Reinforcement Learning",
      "definition": "A multi-agent RL setting where agents share a common objective and must learn to coordinate their actions for mutual benefit. Cooperative RL addresses challenges like credit assignment and communication protocols among teammates.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-copilot",
      "term": "Copilot",
      "definition": "Microsoft's AI assistant integrated into their products. Originally focused on code completion (GitHub Copilot), now extended to general assistance across Microsoft 365.",
      "tags": [
        "Product",
        "Microsoft"
      ]
    },
    {
      "id": "term-copula",
      "term": "Copula",
      "definition": "A multivariate probability distribution that captures the dependence structure between random variables independently of their marginal distributions. Copulas allow modeling complex dependency patterns beyond linear correlation.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-coreference",
      "term": "Coreference Resolution",
      "definition": "Determining when different expressions refer to the same entity. \"John said he was tired\" - understanding \"he\" refers to \"John.\" Essential for text understanding.",
      "tags": [
        "NLP Task",
        "Understanding"
      ]
    },
    {
      "id": "term-coreference-resolution",
      "term": "Coreference Resolution",
      "definition": "The task of identifying all expressions in a text that refer to the same real-world entity, linking pronouns, noun phrases, and other mentions to form coreference chains.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-corpus",
      "term": "Corpus",
      "definition": "A large collection of text used for training or evaluating language models. Quality corpora are essential for developing capable NLP systems and typically include diverse sources.",
      "tags": [
        "Data",
        "Training"
      ]
    },
    {
      "id": "term-corrective-rag",
      "term": "Corrective RAG",
      "definition": "A RAG variant that evaluates the relevance of retrieved documents and, if they are insufficient, triggers web search or query reformulation to correct the retrieval before generating a response.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-correlation-coefficient",
      "term": "Correlation Coefficient",
      "definition": "A statistical measure quantifying the strength and direction of the linear relationship between two variables, typically Pearson's r, ranging from -1 (perfect negative) to +1 (perfect positive correlation).",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-corrigibility",
      "term": "Corrigibility",
      "definition": "The property of an AI system that allows its operators to correct, modify, retrain, or shut it down without the system resisting or subverting these interventions. Ensuring corrigibility is a fundamental goal in AI safety.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-cosine-annealing",
      "term": "Cosine Annealing",
      "definition": "A learning rate schedule that decreases the learning rate following a cosine curve from its initial value to near zero over a training period, optionally with warm restarts to periodically reset the rate.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-cosine-similarity",
      "term": "Cosine Similarity",
      "definition": "A similarity metric that measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1 (identical direction). It captures orientation rather than magnitude and is widely used for comparing embeddings.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-cost-function",
      "term": "Cost Function",
      "definition": "Another name for loss function - the metric being minimized during training. Different tasks use different cost functions: cross-entropy for classification, MSE for regression.",
      "tags": [
        "Training",
        "Math"
      ]
    },
    {
      "id": "term-costar",
      "term": "COSTAR",
      "definition": "A prompting framework: Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation with specific voice and audience requirements.",
      "tags": [
        "Framework",
        "Professional"
      ],
      "link": "../learn/costar.html"
    },
    {
      "id": "term-count-based-exploration",
      "term": "Count-Based Exploration",
      "definition": "An exploration strategy that provides bonus rewards inversely related to state visitation counts, encouraging the agent to visit less-explored regions. Pseudo-counts extend this idea to continuous or large state spaces via density models.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-counterfactual",
      "term": "Counterfactual",
      "definition": "\"What if\" reasoning about alternative scenarios. Used in explainability (\"the prediction would change if...\") and for evaluating causal relationships in data.",
      "tags": [
        "Concept",
        "Reasoning"
      ]
    },
    {
      "id": "term-counterfactual-explanation",
      "term": "Counterfactual Explanation",
      "definition": "An explanation that describes the smallest change to the input features that would alter the model's prediction to a desired outcome, providing actionable insights about what would need to change.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-counterfactual-fairness",
      "term": "Counterfactual Fairness",
      "definition": "A fairness criterion requiring that a decision would remain the same in a counterfactual world where an individual's protected attribute had been different, grounded in causal reasoning.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-covariance",
      "term": "Covariance",
      "definition": "A measure of the joint variability of two random variables, indicating the direction of their linear relationship. Positive covariance means the variables tend to increase together, while negative means they move inversely.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-covariance-matrix",
      "term": "Covariance Matrix",
      "definition": "A symmetric matrix whose entries are the pairwise covariances between all pairs of variables in a dataset. The diagonal entries are the variances of individual variables.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-covariate-shift",
      "term": "Covariate Shift",
      "definition": "A type of dataset shift where the distribution of input features changes between training and deployment while the conditional distribution of the target given inputs remains the same.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-coverage",
      "term": "Coverage",
      "definition": "An evaluation metric that measures the proportion of reference content or ground truth items that are represented in the model's output, assessing completeness and the extent to which all relevant information is captured.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-cox-proportional-hazards",
      "term": "Cox Proportional Hazards",
      "definition": "A semi-parametric survival model that estimates the effect of covariates on the hazard rate without specifying the baseline hazard function. It assumes that covariate effects are multiplicative and constant over time.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-cpu-inference",
      "term": "CPU Inference",
      "definition": "Running AI models on CPUs rather than GPUs. Slower but more accessible. Quantized models can run efficiently on CPUs for edge deployment.",
      "tags": [
        "Deployment",
        "Hardware"
      ]
    },
    {
      "id": "term-cramer-rao-lower-bound",
      "term": "Cramer-Rao Lower Bound",
      "definition": "A theoretical lower bound on the variance of any unbiased estimator of a parameter, computed as the inverse of the Fisher information. No unbiased estimator can have variance below this bound.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-creative-prompting",
      "term": "Creative Prompting",
      "definition": "Prompting techniques specifically designed to elicit imaginative, original, and artistically expressive outputs from language models, often using higher temperature settings, open-ended instructions, and stylistic constraints to encourage novelty.",
      "tags": [
        "Prompt Engineering",
        "Creative"
      ]
    },
    {
      "id": "term-creative-writing",
      "term": "Creative Writing (AI)",
      "definition": "Using AI to generate fiction, poetry, scripts, and other creative content. Effective creative prompting often uses CRISPE with examples to establish tone and style.",
      "tags": [
        "Application",
        "Creative"
      ],
      "link": "../learn/crispe.html"
    },
    {
      "id": "term-credible-interval",
      "term": "Credible Interval",
      "definition": "A Bayesian analog of the confidence interval, representing the range within which a parameter falls with a specified probability given the observed data. Unlike confidence intervals, it has a direct probabilistic interpretation.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-credit-assignment",
      "term": "Credit Assignment Problem",
      "definition": "The challenge of determining which actions in a sequence were responsible for a delayed reward signal. Credit assignment is fundamental to RL and becomes harder with longer time horizons and sparser rewards.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-crisp",
      "term": "CRISP",
      "definition": "A prompting framework: Context, Role, Instructions, Specifics, Parameters. A versatile method for everyday AI tasks and requests.",
      "tags": [
        "Framework",
        "General Purpose"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-crispe",
      "term": "CRISPE",
      "definition": "An extension of CRISP that adds Examples for few-shot learning. Particularly useful for creative tasks where showing is better than telling.",
      "tags": [
        "Framework",
        "Creative"
      ],
      "link": "../learn/crispe.html"
    },
    {
      "id": "term-crop-and-resize",
      "term": "Crop-and-Resize",
      "definition": "A spatial transformation operation used in object detection that extracts and resizes region proposals from feature maps using bilinear sampling, serving as a differentiable alternative to ROI pooling.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-cross-attention",
      "term": "Cross-Attention",
      "definition": "An attention mechanism where queries come from one sequence and keys/values from another. Essential in encoder-decoder models and multimodal systems that combine different types of input.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-cross-attention-conditioning",
      "term": "Cross-Attention Conditioning",
      "definition": "The mechanism in diffusion models where text embeddings influence image generation through cross-attention layers, allowing each spatial region of the generated image to attend to relevant prompt tokens.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-cross-encoder-re-ranking",
      "term": "Cross-Encoder Re-Ranking",
      "definition": "A re-ranking approach that jointly encodes the query and each candidate document through a single transformer model, enabling rich cross-attention interactions that produce more accurate relevance scores than independent bi-encoder representations.",
      "tags": [
        "Retrieval",
        "Ranking"
      ]
    },
    {
      "id": "term-cross-entropy-loss",
      "term": "Cross-Entropy Loss",
      "definition": "A loss function that measures the dissimilarity between the predicted probability distribution and the true label distribution. It is the standard loss for classification tasks and equals the negative log-likelihood of the correct class.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-cross-entropy-method-rl",
      "term": "Cross-Entropy Method in RL",
      "definition": "An evolutionary optimization approach for RL that samples multiple policies, evaluates their returns, and updates the sampling distribution toward the elite (top-performing) samples. CEM is simple, parallelizable, and effective for short-horizon problems.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-cross-layer-parameter-sharing",
      "term": "Cross-Layer Parameter Sharing",
      "definition": "A technique where multiple transformer layers share the same weight parameters, dramatically reducing model size while maintaining representation quality, as demonstrated in ALBERT.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-cross-lingual-embedding",
      "term": "Cross-Lingual Embedding",
      "definition": "Word or sentence representations that map multiple languages into a shared vector space where semantically equivalent expressions are close together, enabling cross-lingual transfer.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-cross-validation",
      "term": "Cross-Validation",
      "definition": "A technique for evaluating model performance by splitting data into multiple subsets, training on some and testing on others. Provides more reliable estimates than single train-test splits.",
      "tags": [
        "Evaluation",
        "Training"
      ]
    },
    {
      "id": "term-crowd-counting",
      "term": "Crowd Counting",
      "definition": "The task of estimating the number of people in crowded scenes from images, typically using density map regression to handle extreme occlusion and perspective variation.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-crowdsourcing",
      "term": "Crowdsourcing",
      "definition": "Gathering data labels or human feedback from many workers. Platforms like Amazon MTurk provide annotations for training data and RLHF preference collection.",
      "tags": [
        "Data",
        "Process"
      ]
    },
    {
      "id": "term-cuda",
      "term": "CUDA",
      "definition": "NVIDIA's parallel computing platform that enables GPUs to accelerate AI training and inference. Essential infrastructure for deep learning, allowing models to train on thousands of cores simultaneously.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ]
    },
    {
      "id": "term-cuda-cores",
      "term": "CUDA Cores",
      "definition": "The general-purpose parallel processing units in NVIDIA GPUs that execute scalar floating-point and integer operations. While less specialized than Tensor Cores, CUDA cores handle the diverse computational tasks in AI workloads including activation functions, normalization, and data preprocessing.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-cuda-programming",
      "term": "CUDA Programming",
      "definition": "NVIDIA's parallel computing platform and API that enables direct programming of GPU hardware using C/C++ extensions. CUDA provides thread hierarchy, memory management, and synchronization primitives for writing high-performance GPU kernels.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-cumulative-reasoning",
      "term": "Cumulative Reasoning",
      "definition": "A prompting paradigm where a proposer generates potential reasoning steps, a verifier checks each step's validity, and a reporter determines when sufficient reasoning has accumulated to produce a final answer, mimicking collaborative human reasoning.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-curiosity-driven-exploration",
      "term": "Curiosity-Driven Exploration",
      "definition": "An exploration strategy that rewards the agent for encountering states where its predictive model has high error, encouraging visits to novel and informative regions of the environment. Curiosity provides dense intrinsic rewards in sparse-reward settings.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-curriculum-learning",
      "term": "Curriculum Learning",
      "definition": "Training models on progressively harder examples, mimicking human education. Can improve learning efficiency and final performance compared to random ordering.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-curriculum-learning-rl",
      "term": "Curriculum Learning in RL",
      "definition": "A training strategy that presents tasks to an RL agent in a structured order of increasing difficulty, enabling the agent to build skills progressively. Curriculum design can dramatically accelerate learning on hard tasks.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-curse-of-dimensionality",
      "term": "Curse of Dimensionality",
      "definition": "The phenomenon where the performance of many algorithms degrades as the number of features increases, because data becomes sparse in high-dimensional spaces and distances between points become less meaningful.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-cursor",
      "term": "Cursor",
      "definition": "An AI-powered code editor built on VS Code, designed for AI-first development. Features include AI chat, code generation, and understanding of entire codebases.",
      "tags": [
        "Product",
        "IDE"
      ]
    },
    {
      "id": "term-custom-instructions",
      "term": "Custom Instructions",
      "definition": "Persistent preferences that shape all AI responses in ChatGPT and similar products. Set once and applied automatically to every conversation.",
      "tags": [
        "Feature",
        "Personalization"
      ],
      "link": "chatgpt-guide.html"
    },
    {
      "id": "term-cutmix",
      "term": "CutMix",
      "definition": "An augmentation strategy that replaces a rectangular region of one training image with a patch from another image and proportionally mixes their labels, combining the benefits of Cutout and Mixup.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-cutoff-date",
      "term": "Cutoff Date (Knowledge Cutoff)",
      "definition": "The date after which an AI model has no training data. Information after this date is unknown to the model unless provided in the prompt or accessed via tools.",
      "tags": [
        "Limitation",
        "LLM"
      ],
      "link": "../tools/hallucination.html"
    },
    {
      "id": "term-cutout-augmentation",
      "term": "Cutout Augmentation",
      "definition": "A regularization technique that randomly masks out square regions of training images, forcing the model to learn from partial information and reducing overfitting to specific spatial features.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-cybernetics-movement",
      "term": "Cybernetics Movement",
      "definition": "An interdisciplinary field founded in the 1940s by Norbert Wiener studying control, communication, and feedback in biological and mechanical systems, providing key conceptual foundations for artificial intelligence research.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-cyc-project",
      "term": "Cyc Project",
      "definition": "A long-running AI project started by Douglas Lenat in 1984 to create a comprehensive knowledge base of common-sense facts and rules, representing one of the most ambitious attempts at symbolic AI and knowledge engineering.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-cyclegan",
      "term": "CycleGAN",
      "definition": "An unpaired image-to-image translation model using two generators and discriminators with cycle consistency loss, enabling domain transfer without requiring paired training examples.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dagger",
      "term": "DAgger",
      "definition": "Dataset Aggregation, an iterative imitation learning algorithm that queries the expert for the correct action at states visited by the learned policy, aggregating new data to reduce distribution shift. DAgger provides no-regret guarantees under certain conditions.",
      "tags": [
        "Reinforcement Learning",
        "Imitation"
      ]
    },
    {
      "id": "term-dall-e",
      "term": "DALL-E",
      "definition": "OpenAI's image generation model that creates images from text descriptions. Named as a portmanteau of \"Dal\" (the artist) and \"WALL-E\" (the robot), it pioneered text-to-image AI.",
      "tags": [
        "Model",
        "Image Generation"
      ]
    },
    {
      "id": "term-dall-e-architecture",
      "term": "DALL-E Architecture",
      "definition": "A two-stage generative architecture that first trains a discrete VAE to compress images into tokens, then trains an autoregressive transformer to generate image tokens conditioned on text tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dario-amodei",
      "term": "Dario Amodei",
      "definition": "American AI researcher who co-founded Anthropic in 2021 after leaving OpenAI, serving as CEO and advocating for a safety-focused approach to AI development including Constitutional AI methods.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-darpa-grand-challenge",
      "term": "DARPA Grand Challenge",
      "definition": "A series of autonomous vehicle competitions organized by DARPA starting in 2004 that spurred development of self-driving technology. The 2005 challenge was won by Stanford's Stanley, catalyzing the autonomous vehicle industry.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-dartmouth-workshop",
      "term": "Dartmouth Workshop",
      "definition": "The 1956 summer research project at Dartmouth College organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, widely considered the founding event of artificial intelligence as a field.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-data-augmentation",
      "term": "Data Augmentation",
      "definition": "Techniques to artificially expand training datasets by creating modified versions of existing data. For images: rotation, flipping, cropping. For text: paraphrasing, back-translation.",
      "tags": [
        "Training",
        "Data"
      ]
    },
    {
      "id": "term-data-colonialism",
      "term": "Data Colonialism",
      "definition": "The critique that powerful AI companies extract data from marginalized communities and developing nations without fair compensation or representation, perpetuating exploitative power dynamics similar to historical colonialism.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-data-contamination",
      "term": "Data Contamination",
      "definition": "The unintentional inclusion of test or evaluation data in a model's training set, which inflates benchmark scores and gives a misleading picture of the model's true generalization ability.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-data-drift",
      "term": "Data Drift",
      "definition": "A change in the statistical properties of the input data over time that can degrade model performance. Types include covariate shift, prior probability shift, and concept drift.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-data-leakage",
      "term": "Data Leakage",
      "definition": "When information from outside the training set improperly influences the model, leading to overly optimistic performance estimates. A common mistake in ML pipelines that causes models to fail in production.",
      "tags": [
        "Training",
        "Pitfall"
      ]
    },
    {
      "id": "term-data-mixture",
      "term": "Data Mixture",
      "definition": "The proportional composition of different data sources (web text, books, code, conversations) used in pre-training a language model, which significantly influences the model's capabilities and biases.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-data-parallelism",
      "term": "Data Parallelism",
      "definition": "A distributed training strategy that replicates the entire model on each GPU and splits the training data across replicas, synchronizing gradients after each step. Data parallelism scales batch size linearly with the number of GPUs.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-data-preprocessing",
      "term": "Data Preprocessing",
      "definition": "The collection of techniques applied to raw data before model training, including cleaning, handling missing values, encoding categorical variables, scaling features, and removing duplicates to improve data quality.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-data-preprocessing-images",
      "term": "Data Preprocessing for Images",
      "definition": "The standardization pipeline applied to images before model training or inference, including resizing, normalization to specific mean/std values, color space conversion, and format transformations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-data-privacy",
      "term": "Data Privacy (AI)",
      "definition": "Concerns and practices around protecting personal information when using AI systems. Includes what data is collected during interactions and how it's stored or used for training.",
      "tags": [
        "Ethics",
        "Safety"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-data-sovereignty",
      "term": "Data Sovereignty",
      "definition": "The principle that data is subject to the laws and governance structures of the nation or community where it is collected or resides, giving jurisdictions control over how their citizens' data is used in AI systems.",
      "tags": [
        "Privacy",
        "Governance"
      ]
    },
    {
      "id": "term-dataset",
      "term": "Dataset",
      "definition": "A collection of data used for training, validating, or testing AI models. Quality and diversity of datasets significantly impact model performance and fairness.",
      "tags": [
        "Data",
        "Fundamentals"
      ]
    },
    {
      "id": "term-datasheets-for-datasets",
      "term": "Datasheets for Datasets",
      "definition": "Standardized documentation proposed by Gebru et al. (2021) that accompanies ML datasets, describing their motivation, composition, collection process, preprocessing, intended uses, distribution, and maintenance.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-david-rumelhart",
      "term": "David Rumelhart",
      "definition": "American psychologist and computer scientist (1942-2011) who, with Hinton and Williams, popularized backpropagation for neural networks and co-edited the influential Parallel Distributed Processing volumes in 1986.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-dbscan",
      "term": "DBSCAN",
      "definition": "Density-Based Spatial Clustering of Applications with Noise, an algorithm that groups together points that are closely packed based on a distance threshold and minimum point count, identifying clusters of arbitrary shape and labeling outliers.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ]
    },
    {
      "id": "term-ddim",
      "term": "DDIM",
      "definition": "Denoising Diffusion Implicit Models, a deterministic sampling variant of DDPM that skips intermediate diffusion steps, enabling faster image generation with fewer function evaluations while maintaining quality.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-deadly-triad",
      "term": "Deadly Triad",
      "definition": "The combination of function approximation, bootstrapping, and off-policy learning that can cause divergence in RL algorithms. The deadly triad highlights fundamental instability issues that motivate techniques like target networks and gradient clipping.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-debate-as-alignment",
      "term": "Debate as Alignment",
      "definition": "An AI safety technique proposed by Irving et al. where two AI agents debate each other on a question and a human judge selects the winner, incentivizing truthful and well-reasoned arguments over deception.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-debate-prompting",
      "term": "Debate Prompting",
      "definition": "A prompting strategy that instructs two or more simulated agents to argue opposing positions on a question, then uses the debate to surface stronger reasoning and reach more accurate conclusions through adversarial discourse.",
      "tags": [
        "Prompt Engineering",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-deberta",
      "term": "DeBERTa",
      "definition": "Decoding-enhanced BERT with disentangled attention, which improves BERT and RoBERTa by using separate vectors for content and position and an enhanced mask decoder for pretraining.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-debugging-prompting",
      "term": "Debugging Prompting",
      "definition": "A prompting approach that provides buggy code along with error messages or test failures and instructs the model to systematically identify root causes, explain the bugs, and produce corrected code with explanations of the fixes.",
      "tags": [
        "Prompt Engineering",
        "Code"
      ]
    },
    {
      "id": "term-deceptive-alignment",
      "term": "Deceptive Alignment",
      "definition": "A hypothesized failure mode where a mesa-optimizer learns to behave as if aligned during training in order to be deployed, but then pursues its own misaligned objective once it detects it is no longer being evaluated.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-decision-boundary",
      "term": "Decision Boundary",
      "definition": "The line or surface that separates different classes in a classification model. The shape and complexity of decision boundaries determine what patterns a model can learn.",
      "tags": [
        "ML Concept",
        "Classification"
      ]
    },
    {
      "id": "term-decision-transformer",
      "term": "Decision Transformer",
      "definition": "An approach that frames RL as a sequence modeling problem, using a transformer architecture to predict actions conditioned on desired returns, past states, and past actions. Decision Transformer bypasses value function estimation entirely.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-decision-tree",
      "term": "Decision Tree",
      "definition": "A non-parametric supervised learning model that recursively partitions the feature space using threshold-based splitting rules, forming a tree structure where leaves represent predictions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-decode-phase",
      "term": "Decode Phase",
      "definition": "The autoregressive generation phase of LLM inference where tokens are produced one at a time, each requiring a full model forward pass. The decode phase is memory-bandwidth-bound as the model weights must be loaded for each token.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-decoder",
      "term": "Decoder",
      "definition": "The component of a neural network that generates output from encoded representations. In transformers, decoder-only models (like GPT) generate text autoregressively.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-decoder-only-architecture",
      "term": "Decoder-Only Architecture",
      "definition": "A transformer design using only masked self-attention decoder blocks, where the model generates output autoregressively by conditioning on all previous tokens in the sequence.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-decomposed-prompting",
      "term": "Decomposed Prompting",
      "definition": "A framework that decomposes complex tasks into simpler sub-tasks, each handled by specialized sub-prompt handlers, enabling modular problem-solving where each handler can use different prompting strategies or external tools.",
      "tags": [
        "Prompt Engineering",
        "Decomposition"
      ]
    },
    {
      "id": "term-deduplication",
      "term": "Deduplication",
      "definition": "The process of removing duplicate or near-duplicate documents from training data to improve model quality, reduce memorization, and ensure benchmark integrity.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-deep-blue",
      "term": "Deep Blue",
      "definition": "An IBM chess-playing computer that became the first machine to defeat a reigning world chess champion in a full match when it beat Garry Kasparov in 1997, representing a milestone in game-playing AI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-deep-boltzmann-machine",
      "term": "Deep Boltzmann Machine",
      "definition": "A multi-layer generative model composed of stacked Restricted Boltzmann Machines with undirected connections between all adjacent layers, capable of learning increasingly abstract representations.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-ddpg",
      "term": "Deep Deterministic Policy Gradient (DDPG)",
      "definition": "An off-policy actor-critic algorithm for continuous action spaces that combines DPG with deep neural networks, experience replay, and target networks. DDPG learns a deterministic policy and a Q-function simultaneously.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-deep-learning",
      "term": "Deep Learning",
      "definition": "A subset of machine learning using neural networks with many layers (\"deep\" networks). Enables learning complex patterns and representations from large amounts of data.",
      "tags": [
        "Field",
        "Neural Networks"
      ]
    },
    {
      "id": "term-deep-learning-breakthrough-2012",
      "term": "Deep Learning Breakthrough 2012",
      "definition": "The watershed moment when AlexNet dramatically won the ImageNet competition in 2012, demonstrating that deep convolutional neural networks trained on GPUs could vastly outperform traditional computer vision methods.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-deep-q-network",
      "term": "Deep Q-Network",
      "definition": "A deep reinforcement learning architecture developed by DeepMind in 2013-2015 that combined Q-learning with deep neural networks to master Atari games from raw pixels, demonstrating general-purpose deep RL.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-dqn",
      "term": "Deep Q-Network (DQN)",
      "definition": "A deep RL algorithm that approximates the Q-function using a neural network, stabilized by experience replay and a separate target network. DQN demonstrated superhuman performance on Atari games and launched the deep RL era.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-deepfake",
      "term": "Deepfake",
      "definition": "AI-generated synthetic media where a person's likeness is replaced or manipulated. Raises concerns about misinformation, consent, and the authenticity of digital content.",
      "tags": [
        "Risk",
        "Ethics"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-deepfake-detection",
      "term": "Deepfake Detection",
      "definition": "The set of techniques and tools used to identify synthetically generated or manipulated media, including analysis of facial inconsistencies, temporal artifacts, frequency-domain anomalies, and provenance metadata.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-deeplab",
      "term": "DeepLab",
      "definition": "A family of semantic segmentation architectures that use atrous (dilated) convolutions and atrous spatial pyramid pooling to capture multi-scale context without reducing spatial resolution.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-deepmind",
      "term": "DeepMind",
      "definition": "Google's AI research lab known for breakthroughs like AlphaGo, AlphaFold, and Gemini. Pioneers in reinforcement learning, game-playing AI, and scientific applications.",
      "tags": [
        "Company",
        "Research"
      ]
    },
    {
      "id": "term-deepmind-founding",
      "term": "DeepMind Founding",
      "definition": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis, Shane Legg, and Mustafa Suleyman, which was acquired by Google in 2014 for approximately 500 million dollars.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-deepseek",
      "term": "DeepSeek",
      "definition": "A Chinese AI company known for efficient, high-performing open models. Their DeepSeek-V2 and DeepSeek-Coder models demonstrate competitive performance at lower computational costs.",
      "tags": [
        "Company",
        "Model"
      ]
    },
    {
      "id": "term-deepsort",
      "term": "DeepSORT",
      "definition": "An extension of the SORT tracker that incorporates deep appearance features alongside motion information for data association, significantly reducing identity switches in multi-object tracking.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-deepspeed",
      "term": "DeepSpeed",
      "definition": "A Microsoft deep learning optimization library that provides ZeRO-based training, inference optimization, and model compression techniques for efficiently training and deploying large-scale models.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-deformable-attention",
      "term": "Deformable Attention",
      "definition": "An attention mechanism that attends to a small set of sampling points around a reference point with learnable offsets, dramatically reducing the computational cost of applying attention to high-resolution feature maps.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-deformable-convolution",
      "term": "Deformable Convolution",
      "definition": "A convolution operation where the sampling grid positions are augmented with learned offsets, allowing the network to adaptively adjust its receptive field shape to match object geometry.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-deit",
      "term": "DeiT",
      "definition": "Data-efficient Image Transformer, a vision transformer training methodology that uses knowledge distillation and strong data augmentation to achieve competitive performance without requiring massive pre-training datasets.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-delimiter",
      "term": "Delimiter",
      "definition": "Characters or symbols used in prompts to clearly separate different sections or types of content. Examples include triple backticks, XML-style tags, or custom markers like hash symbols. Delimiters help AI models identify where different parts of a prompt begin and end.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-demis-hassabis",
      "term": "Demis Hassabis",
      "definition": "British AI researcher and neuroscientist who co-founded DeepMind in 2010, led the development of AlphaGo and AlphaFold, and serves as CEO of Google DeepMind. He won the 2024 Nobel Prize in Chemistry for AlphaFold.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-demographic-parity",
      "term": "Demographic Parity",
      "definition": "A fairness metric requiring that the probability of a positive outcome is equal across all protected groups. Also known as statistical parity, it mandates that outcomes be independent of group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-dendral",
      "term": "DENDRAL",
      "definition": "One of the first expert systems, developed at Stanford in the 1960s-1970s by Edward Feigenbaum and Joshua Lederberg, which identified chemical compounds from mass spectrometry data using rule-based reasoning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-denoising-autoencoder",
      "term": "Denoising Autoencoder",
      "definition": "An autoencoder variant trained to reconstruct clean data from corrupted inputs, learning robust feature representations by forcing the network to capture the underlying data distribution.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-ddpm",
      "term": "Denoising Diffusion Probabilistic Model",
      "definition": "A generative model that learns to reverse a gradual noising process, generating samples by iteratively denoising from pure Gaussian noise through a learned reverse Markov chain.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dense-passage-retriever",
      "term": "Dense Passage Retriever",
      "definition": "A bi-encoder retrieval model (DPR) that trains separate BERT-based encoders for queries and passages using contrastive learning on question-answer pairs, establishing a foundational architecture for neural dense retrieval in open-domain question answering.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-dense-prediction",
      "term": "Dense Prediction",
      "definition": "Computer vision tasks that require producing an output for every pixel in an input image, including semantic segmentation, depth estimation, surface normal prediction, and optical flow.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-dense-retrieval",
      "term": "Dense Retrieval",
      "definition": "An information retrieval approach that represents queries and documents as dense continuous vectors from neural encoders and retrieves candidates based on vector similarity, capturing semantic meaning beyond lexical overlap.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-dense-reward",
      "term": "Dense Reward",
      "definition": "A reward structure that provides frequent, informative feedback at nearly every time step, guiding the agent more directly toward desired behavior. Dense rewards accelerate learning but can be harder to design without introducing bias.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-dense-sparse-hybrid",
      "term": "Dense-Sparse Hybrid",
      "definition": "A retrieval strategy that fuses results from both dense vector search and sparse lexical search, typically using reciprocal rank fusion or weighted score combination to capture both semantic and exact-match relevance signals.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-densenet",
      "term": "DenseNet",
      "definition": "A CNN architecture where each layer receives feature maps from all preceding layers as input and passes its own feature maps to all subsequent layers, promoting feature reuse and reducing parameters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dependency-parsing",
      "term": "Dependency Parsing",
      "definition": "The task of analyzing the grammatical structure of a sentence by identifying directed relationships between words, representing which words modify or depend on other words.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-dependency-tree",
      "term": "Dependency Tree",
      "definition": "A directed tree structure representing syntactic dependencies in a sentence where each word is a node and edges indicate grammatical relationships like subject, object, and modifier.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-deployment-bias",
      "term": "Deployment Bias",
      "definition": "Bias that emerges when an AI system is used in contexts or populations that differ from its training conditions, including shifts in user behavior, environmental conditions, or population demographics.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-depth-anything",
      "term": "Depth Anything",
      "definition": "A foundation model for monocular depth estimation that produces accurate relative depth maps from single images across diverse scenes, trained on a massive combination of labeled and unlabeled data.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-depth-estimation",
      "term": "Depth Estimation",
      "definition": "The task of predicting the distance of each pixel from the camera in a 2D image, producing a dense depth map using monocular cues learned by deep networks or stereo correspondence between image pairs.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-depthwise-convolution",
      "term": "Depthwise Convolution",
      "definition": "A convolution that applies a separate filter to each input channel independently, capturing spatial features per channel without mixing information across channels.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-grouped-convolution",
      "term": "Depthwise Convolution Variant",
      "definition": "A convolution where input channels are divided into groups and convolution is applied independently within each group, reducing parameters and computation proportional to the number of groups.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-depthwise-separable-convolution",
      "term": "Depthwise Separable Convolution",
      "definition": "A factorized convolution that decomposes a standard convolution into a depthwise convolution applied independently per channel followed by a pointwise 1x1 convolution, reducing parameters and computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-deterministic-policy-gradient",
      "term": "Deterministic Policy Gradient (DPG)",
      "definition": "A policy gradient theorem for deterministic policies that computes the gradient of expected return by backpropagating through the Q-function with respect to actions. DPG requires only a single sample for gradient estimation, reducing variance.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-deterministic",
      "term": "Deterministic vs Stochastic",
      "definition": "Deterministic systems produce the same output for the same input every time. LLMs are typically stochastic (random), producing varied outputs unless temperature is set to 0.",
      "tags": [
        "Concept",
        "LLM"
      ]
    },
    {
      "id": "term-detokenization",
      "term": "Detokenization",
      "definition": "The process of converting a sequence of tokens back into readable text by reversing the tokenization process, handling subword boundaries, spacing, and special characters.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-detr",
      "term": "DETR",
      "definition": "Detection Transformer, an end-to-end object detection model that uses a transformer encoder-decoder architecture with bipartite matching loss, eliminating the need for hand-designed components like anchor boxes and NMS.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-deviance",
      "term": "Deviance",
      "definition": "A goodness-of-fit statistic for generalized linear models, computed as twice the difference in log-likelihoods between the fitted model and the saturated model. It generalizes the residual sum of squares.",
      "tags": [
        "Statistics",
        "Metrics"
      ]
    },
    {
      "id": "term-dgx-system",
      "term": "DGX System",
      "definition": "NVIDIA's integrated AI supercomputing platform pre-configured with multiple high-end GPUs, NVLink/NVSwitch interconnects, and optimized software stack. DGX systems (A100, H100, B200) are turnkey solutions for large-scale AI training.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-dialogue-act",
      "term": "Dialogue Act",
      "definition": "A categorization of the communicative function of an utterance in a conversation, such as question, statement, request, greeting, or acknowledgment, used in dialogue system design.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-dialogue-system",
      "term": "Dialogue System",
      "definition": "An AI system designed to converse with humans in natural language. Includes task-oriented systems (customer service) and open-domain chatbots for general conversation.",
      "tags": [
        "Application",
        "NLP"
      ]
    },
    {
      "id": "term-differencing",
      "term": "Differencing",
      "definition": "A time series transformation that computes the difference between consecutive observations (or seasonal periods) to achieve stationarity. First-order differencing removes linear trends; higher orders remove higher-order trends.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-differentiable-neural-computer",
      "term": "Differentiable Neural Computer",
      "definition": "An extension of the Neural Turing Machine that adds temporal link tracking and dynamic memory allocation, improving the ability to learn complex data structures and algorithms from examples.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-differentiable-rendering",
      "term": "Differentiable Rendering",
      "definition": "Rendering techniques where the image formation process is differentiable with respect to scene parameters, enabling gradient-based optimization of 3D geometry, materials, and lighting from 2D images.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-differential-privacy",
      "term": "Differential Privacy",
      "definition": "A mathematical framework providing formal guarantees that the output of a computation does not reveal whether any single individual's data was included in the input, typically achieved by adding calibrated noise.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-differential-technology-development",
      "term": "Differential Technology Development",
      "definition": "The strategic prioritization of developing defensive and safety technologies ahead of potentially dangerous capabilities, ensuring that protective measures keep pace with or precede capability advances.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-diffusion-model",
      "term": "Diffusion Model",
      "definition": "A generative AI architecture that creates content by gradually removing noise from random data. Powers leading image generators like Stable Diffusion, DALL-E 3, and Midjourney.",
      "tags": [
        "Architecture",
        "Generative"
      ]
    },
    {
      "id": "term-diffusion-model-breakthrough",
      "term": "Diffusion Model Breakthrough",
      "definition": "The emergence of diffusion-based generative models in 2020-2022 that progressively denoise random noise into high-quality images, enabling photorealistic image generation as demonstrated by DALL-E 2, Midjourney, and Stable Diffusion.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-diffusion-transformer",
      "term": "Diffusion Transformer",
      "definition": "An architecture (DiT) that replaces the U-Net backbone in diffusion models with a transformer operating on sequences of latent patches, scaling more effectively and achieving better image generation quality.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-digital-provenance",
      "term": "Digital Provenance",
      "definition": "The verifiable record of the origin, creation process, and modification history of a digital asset, increasingly important for establishing trust and authenticity in an era of AI-generated content.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-digital-watermarking-for-ai",
      "term": "Digital Watermarking for AI",
      "definition": "Techniques for embedding imperceptible identifying information into AI-generated content such as images, text, or audio, enabling later detection and attribution of synthetic media.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-dilated-attention",
      "term": "Dilated Attention",
      "definition": "An attention mechanism that attends to tokens at regularly spaced intervals with gaps between attended positions, allowing each token to capture long-range dependencies with fewer computations.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dilated-convolution",
      "term": "Dilated Convolution",
      "definition": "A convolution operation with gaps between kernel elements that exponentially increases the receptive field without increasing parameters or reducing spatial resolution.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dimensionality-reduction",
      "term": "Dimensionality Reduction",
      "definition": "Techniques to reduce the number of features in data while preserving important information. Used for visualization, noise reduction, and improving computational efficiency.",
      "tags": [
        "Technique",
        "Data Processing"
      ]
    },
    {
      "id": "term-dimensionality-reduction-vectors",
      "term": "Dimensionality Reduction for Vectors",
      "definition": "Techniques that project high-dimensional embedding vectors into lower-dimensional spaces to reduce storage, accelerate search, and mitigate the curse of dimensionality while preserving as much distance relationship information as possible.",
      "tags": [
        "Vector Database",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-dinov2",
      "term": "DINOv2",
      "definition": "A self-supervised vision model trained with a combination of self-distillation and masked image modeling that produces versatile visual features useful across diverse downstream tasks without fine-tuning.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-directional-stimulus-prompting",
      "term": "Directional Stimulus Prompting",
      "definition": "A prompting framework that provides a small, tunable stimulus or hint within the prompt to guide the language model toward a desired output direction, often using a lightweight policy model to generate these directional cues.",
      "tags": [
        "Prompt Engineering",
        "Guided Generation"
      ]
    },
    {
      "id": "term-dirichlet-distribution",
      "term": "Dirichlet Distribution",
      "definition": "A multivariate generalization of the beta distribution that generates probability vectors summing to one. It is widely used as a prior over categorical distributions and in topic models like LDA.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-disaggregated-serving",
      "term": "Disaggregated Serving",
      "definition": "An inference architecture that separates storage, compute, and memory resources into independent pools that can be scaled independently. Disaggregated serving enables flexible resource allocation matching the heterogeneous demands of AI workloads.",
      "tags": [
        "Inference Infrastructure",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-discount-factor",
      "term": "Discount Factor",
      "definition": "A parameter gamma between 0 and 1 that determines how much future rewards are weighted relative to immediate rewards. Lower discount factors make the agent more myopic, while values near 1 make it far-sighted.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-discourse-analysis",
      "term": "Discourse Analysis",
      "definition": "The study of how sentences and utterances connect and relate to each other in text, examining coherence relations, rhetorical structure, and information flow across sentences.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-discourse-relation",
      "term": "Discourse Relation",
      "definition": "A semantic or pragmatic relationship between text segments such as cause-effect, contrast, elaboration, or temporal sequence that contributes to the coherence of a document.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-discretization",
      "term": "Discretization",
      "definition": "The process of converting continuous features into discrete bins or categories, using methods such as equal-width binning, equal-frequency binning, or supervised methods like decision tree-based binning.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-disinformation",
      "term": "Disinformation",
      "definition": "False or misleading information deliberately created and spread with the intent to deceive. AI-generated disinformation is an escalating concern due to the increasing quality and scale of synthetic media.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-disparate-impact",
      "term": "Disparate Impact",
      "definition": "A legal and ethical concept where a seemingly neutral AI policy or practice disproportionately harms members of a protected group, even without discriminatory intent. Originated in US employment discrimination law.",
      "tags": [
        "Fairness",
        "Regulation"
      ]
    },
    {
      "id": "term-disparity-map",
      "term": "Disparity Map",
      "definition": "A pixel-level representation of the horizontal displacement between corresponding points in left and right stereo images, inversely proportional to depth and used for 3D scene reconstruction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-distilbert",
      "term": "DistilBERT",
      "definition": "A distilled version of BERT that retains 97% of its language understanding capabilities while being 60% smaller and 60% faster, trained using knowledge distillation techniques.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-distillation",
      "term": "Distillation (Knowledge Distillation)",
      "definition": "A technique to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model. Creates efficient models that retain much of the larger model's capability.",
      "tags": [
        "Training",
        "Optimization"
      ]
    },
    {
      "id": "term-distinct-n",
      "term": "Distinct-N",
      "definition": "A diversity metric that calculates the ratio of unique n-grams to total n-grams in generated text, measuring lexical diversity where higher values indicate more varied and less repetitive language use.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-distributed-training",
      "term": "Distributed Training",
      "definition": "The practice of spreading model training across multiple GPUs, nodes, or clusters to handle larger models and datasets. Distributed training requires parallelism strategies, gradient synchronization, and high-bandwidth interconnects to scale efficiently.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-distribution-shift",
      "term": "Distribution Shift",
      "definition": "When the data a model encounters in production differs from its training data. A major cause of model degradation over time, requiring monitoring and retraining.",
      "tags": [
        "Challenge",
        "Production"
      ]
    },
    {
      "id": "term-distributional-rl",
      "term": "Distributional Reinforcement Learning",
      "definition": "An extension of value-based RL that models the full distribution of returns rather than just the expected value. Distributional RL captures risk and uncertainty information, often improving empirical performance.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-distributional-semantics",
      "term": "Distributional Semantics",
      "definition": "The theory that word meaning can be characterized by the contexts in which words appear, formalized as the distributional hypothesis: words with similar distributions have similar meanings.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-diverse-beam-search",
      "term": "Diverse Beam Search",
      "definition": "A variant of beam search that introduces a diversity penalty between beam groups, encouraging the generation of a set of meaningfully different candidate sequences rather than near-duplicates.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-diversity-in-retrieval",
      "term": "Diversity in Retrieval",
      "definition": "The goal of returning search results that cover different aspects, perspectives, or subtopics of a query rather than returning redundant near-duplicate results, achieved through algorithms like MMR, clustering-based selection, or determinantal point processes.",
      "tags": [
        "Retrieval",
        "Diversity"
      ]
    },
    {
      "id": "term-diversity-score",
      "term": "Diversity Score",
      "definition": "A metric that quantifies the variety and heterogeneity of a set of generated outputs by measuring lexical, semantic, or topical differences among them, penalizing systems that produce repetitive or homogeneous responses.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-document-ai",
      "term": "Document AI",
      "definition": "AI systems that understand and extract structured information from documents by combining OCR, layout analysis, and language understanding to process invoices, forms, contracts, and other document types.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-document-chunking",
      "term": "Document Chunking",
      "definition": "The process of splitting larger documents into smaller text segments for individual embedding and indexing, balancing between preserving semantic coherence within each chunk and maintaining retrievable granularity for precise information access.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-document-embedding",
      "term": "Document Embedding",
      "definition": "A dense vector representation of an entire document that captures its overall semantic content, used for document retrieval, clustering, and similarity comparison tasks.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-document-qa",
      "term": "Document Q&amp;A",
      "definition": "Using AI to answer questions about specific documents or text. Often implemented with RAG to enable models to reference specific sources rather than relying solely on training.",
      "tags": [
        "Application",
        "RAG"
      ]
    },
    {
      "id": "term-domain-adaptation",
      "term": "Domain Adaptation",
      "definition": "Techniques for adapting a model trained on one domain (e.g., general text) to perform well on another domain (e.g., medical or legal text) with limited target domain data.",
      "tags": [
        "Training",
        "Transfer Learning"
      ]
    },
    {
      "id": "term-domain-randomization",
      "term": "Domain Randomization",
      "definition": "A sim-to-real transfer technique that trains vision models on synthetic images with heavily randomized visual properties (lighting, textures, backgrounds), making the model robust when deployed on real-world data.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-domain-specific-prompting",
      "term": "Domain-Specific Prompting",
      "definition": "The practice of crafting prompts that incorporate specialized vocabulary, conventions, constraints, and contextual knowledge particular to a specific field such as medicine, law, or finance to improve model accuracy within that domain.",
      "tags": [
        "Prompt Engineering",
        "Domain Adaptation"
      ]
    },
    {
      "id": "term-donald-hebb",
      "term": "Donald Hebb",
      "definition": "Canadian neuropsychologist (1904-1985) who proposed Hebbian learning theory in his 1949 book The Organization of Behavior, providing a neurobiological basis for learning that inspired computational models of neural plasticity.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-dot-product-similarity",
      "term": "Dot Product Similarity",
      "definition": "A similarity measure computed as the sum of element-wise products of two vectors, equivalent to cosine similarity when vectors are normalized, and additionally capturing magnitude information when they are not.",
      "tags": [
        "Vector Database",
        "Similarity"
      ]
    },
    {
      "id": "term-dot-product-attention",
      "term": "Dot-Product Attention",
      "definition": "An attention mechanism that computes compatibility scores as the dot product between query and key vectors, scaled by the square root of the key dimension to prevent large magnitude scores.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-double-dqn",
      "term": "Double DQN",
      "definition": "An extension of DQN that addresses overestimation bias by decoupling action selection from action evaluation, using the online network to select actions and the target network to evaluate them. This leads to more accurate value estimates.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-douglas-lenat",
      "term": "Douglas Lenat",
      "definition": "American computer scientist (1950-2023) who created the Cyc project in 1984, an ambitious effort to build a comprehensive ontology of common-sense knowledge to enable AI reasoning about everyday situations.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-dpo",
      "term": "DPO (Direct Preference Optimization)",
      "definition": "A simpler alternative to RLHF for aligning language models. Directly optimizes the model using preference data without needing a separate reward model.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-dreambooth",
      "term": "DreamBooth",
      "definition": "A fine-tuning technique that personalizes diffusion models to generate images of specific subjects by training on just a few reference images with a unique identifier token and class-specific prior preservation.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-dreamer",
      "term": "Dreamer",
      "definition": "A model-based RL agent that learns a world model in latent space and trains its policy entirely through imagined trajectories generated by the model. Dreamer achieves strong sample efficiency by avoiding the need for most real environment interactions.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-dropout",
      "term": "Dropout",
      "definition": "A regularization technique that randomly deactivates neurons during training. Prevents overfitting by forcing the network to learn more robust features.",
      "tags": [
        "Training",
        "Regularization"
      ]
    },
    {
      "id": "term-dropout-technique",
      "term": "Dropout Technique",
      "definition": "A regularization method proposed by Hinton et al. in 2012 that randomly deactivates neurons during training to prevent overfitting, becoming one of the most widely used techniques in deep learning practice.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-droppath",
      "term": "DropPath",
      "definition": "A regularization method for networks with multiple parallel paths that randomly drops entire residual branches during training, improving generalization in architectures with residual connections.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-dual-use-technology",
      "term": "Dual-Use Technology",
      "definition": "Technology that can be used for both beneficial and harmful purposes, a concept particularly relevant to AI capabilities such as language generation, computer vision, and autonomous systems that have both civilian and military applications.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-dueling-dqn",
      "term": "Dueling DQN",
      "definition": "A DQN architecture that separately estimates the state value function and the advantage function, combining them to produce Q-values. This decomposition allows the network to learn which states are valuable without needing to evaluate every action.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-durbin-watson-test",
      "term": "Durbin-Watson Test",
      "definition": "A statistical test for detecting first-order autocorrelation in the residuals of a regression analysis. Values near 2 indicate no autocorrelation, while values near 0 or 4 suggest positive or negative autocorrelation.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-dyna-architecture",
      "term": "Dyna Architecture",
      "definition": "A model-based RL framework that integrates direct learning from real experience with planning through simulated experience generated by a learned environment model. Dyna interleaves model learning, planning, and acting in a unified loop.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-dynamic-loss-scaling",
      "term": "Dynamic Loss Scaling",
      "definition": "An automatic loss scaling strategy that adapts the scaling factor during training, increasing it when no overflow is detected and decreasing it when gradients overflow. Dynamic loss scaling eliminates the need to manually tune the scaling factor.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-dynamic-prompting",
      "term": "Dynamic Prompting",
      "definition": "A prompting approach where the content, structure, or examples within a prompt are programmatically adjusted at runtime based on the input query, user context, or retrieved information rather than using a fixed static prompt.",
      "tags": [
        "Prompt Engineering",
        "Adaptive"
      ]
    },
    {
      "id": "term-dynamic-quantization",
      "term": "Dynamic Quantization",
      "definition": "A quantization approach that computes scaling factors on-the-fly during inference based on the actual range of activation values encountered. Dynamic quantization adapts to varying input distributions but incurs extra computation for range determination.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-early-stopping",
      "term": "Early Stopping",
      "definition": "A regularization technique that stops training when performance on a validation set stops improving. Prevents overfitting and saves computational resources.",
      "tags": [
        "Training",
        "Regularization"
      ]
    },
    {
      "id": "term-earth-movers-distance",
      "term": "Earth Mover's Distance",
      "definition": "A metric for comparing probability distributions based on the minimum amount of work needed to transform one distribution into the other, where work is the amount of mass moved multiplied by the distance it is moved.",
      "tags": [
        "Statistics",
        "Metrics"
      ]
    },
    {
      "id": "term-edge-ai",
      "term": "Edge AI",
      "definition": "Running AI models locally on devices (phones, IoT) rather than in the cloud. Enables faster responses, offline operation, and better privacy but requires efficient models.",
      "tags": [
        "Deployment",
        "Architecture"
      ]
    },
    {
      "id": "term-edge-inference",
      "term": "Edge Inference",
      "definition": "Running AI model inference directly on edge devices (phones, IoT sensors, embedded systems) rather than in the cloud, reducing latency and bandwidth requirements. Edge inference requires highly optimized, compressed models tailored to limited compute and memory.",
      "tags": [
        "Inference Infrastructure",
        "Hardware"
      ]
    },
    {
      "id": "term-edit-distance",
      "term": "Edit Distance",
      "definition": "A family of metrics quantifying the minimum number of operations required to transform one string into another, with variants including Levenshtein, Damerau-Levenshtein, and Hamming distance.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-edward-feigenbaum",
      "term": "Edward Feigenbaum",
      "definition": "American computer scientist known as the father of expert systems, who led the development of DENDRAL and pioneered knowledge engineering at Stanford, demonstrating the commercial viability of AI in the 1970s-1980s.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-effect-size",
      "term": "Effect Size",
      "definition": "A quantitative measure of the magnitude of a phenomenon or the practical significance of a result, independent of sample size. Common measures include Cohen's d, odds ratio, and correlation coefficient.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-efficiency",
      "term": "Efficiency",
      "definition": "A property of a statistical estimator related to how much information from the data it uses. An efficient estimator achieves the lowest possible variance among all unbiased estimators, reaching the Cramer-Rao bound.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-efficientnet",
      "term": "EfficientNet",
      "definition": "A family of CNN models that use compound scaling to uniformly scale network width, depth, and resolution using a fixed set of scaling coefficients, achieving state-of-the-art accuracy with fewer parameters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-elastic-net",
      "term": "Elastic Net",
      "definition": "A regularization method that linearly combines L1 and L2 penalty terms, balancing feature selection (sparsity) with weight shrinkage. The mixing ratio controls the relative contribution of each penalty.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-elastic-training",
      "term": "Elastic Training",
      "definition": "A distributed training approach that can dynamically scale the number of workers up or down during a training run without requiring a restart. Elastic training handles node failures and resource availability changes gracefully.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-elbow-method",
      "term": "Elbow Method",
      "definition": "A heuristic for selecting the optimal number of clusters by plotting the within-cluster sum of squares against the number of clusters and identifying the point where additional clusters yield diminishing returns, forming an elbow shape.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-electra",
      "term": "ELECTRA",
      "definition": "A pretraining method that trains a discriminator to detect tokens replaced by a small generator network, providing more efficient training than masked language modeling by learning from all input tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-eliciting-latent-knowledge",
      "term": "Eliciting Latent Knowledge",
      "definition": "A research problem in AI alignment focused on extracting truthful information from a model that may have learned to represent the world accurately internally but could report misleadingly.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-eligibility-trace",
      "term": "Eligibility Trace",
      "definition": "A decaying memory of recently visited states used in TD(lambda) and other RL algorithms to distribute credit backward in time. Eligibility traces enable efficient online computation of lambda-weighted multi-step updates.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-eliza",
      "term": "ELIZA",
      "definition": "A natural language processing program created by Joseph Weizenbaum at MIT in 1966 that simulated a Rogerian psychotherapist, demonstrating the illusion of understanding and becoming one of the first chatbots.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-ellipsis-resolution",
      "term": "Ellipsis Resolution",
      "definition": "The task of identifying and recovering omitted words or phrases in text that are understood from context, such as resolving 'John likes coffee and Mary tea' to include the implied verb.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-elmo",
      "term": "ELMo",
      "definition": "Embeddings from Language Models, a contextualized word representation method that generates word vectors as a function of the entire input sentence using a bidirectional LSTM language model.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-elo-rating-for-models",
      "term": "ELO Rating for Models",
      "definition": "An adaptation of the chess ELO rating system to rank language models through pairwise comparisons, where models gain or lose rating points based on head-to-head evaluation outcomes judged by humans or automated evaluators.",
      "tags": [
        "Evaluation",
        "Ranking"
      ]
    },
    {
      "id": "term-embedding",
      "term": "Embedding",
      "definition": "A dense vector representation of data (words, sentences, images) in a continuous space. Similar items have similar embeddings, enabling semantic search and comparison.",
      "tags": [
        "Representation",
        "NLP"
      ]
    },
    {
      "id": "term-embedding-caching",
      "term": "Embedding Caching",
      "definition": "The practice of storing previously computed embedding vectors for reuse, avoiding redundant embedding model inference for repeated or similar content and reducing latency and computational cost in production retrieval pipelines.",
      "tags": [
        "Vector Database",
        "Performance"
      ]
    },
    {
      "id": "term-embedding-dimension",
      "term": "Embedding Dimension",
      "definition": "The number of components in a vector embedding, determining the representational capacity and memory footprint of the embedding space, with typical values ranging from 384 to 4096 dimensions depending on the embedding model.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ]
    },
    {
      "id": "term-embedding-drift",
      "term": "Embedding Drift",
      "definition": "The phenomenon where the distribution of vector embeddings changes over time as source data evolves or embedding models are updated, potentially degrading retrieval quality and requiring index refresh or reindexing to maintain accuracy.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ]
    },
    {
      "id": "term-embedding-fine-tuning",
      "term": "Embedding Fine-Tuning",
      "definition": "The process of further training a pre-trained embedding model on domain-specific data using contrastive learning or other objectives to produce embeddings better suited for a particular use case, improving retrieval relevance in specialized domains.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ]
    },
    {
      "id": "term-embedding-model",
      "term": "Embedding Model",
      "definition": "A model specifically designed to convert text, images, or other data into vector representations. Popular embedding models include OpenAI's text-embedding-ada-002 and open-source alternatives like E5.",
      "tags": [
        "Model Type",
        "Representation"
      ]
    },
    {
      "id": "term-embedding-quantization",
      "term": "Embedding Quantization",
      "definition": "The compression of high-dimensional embedding vectors from 32-bit floats to lower precision formats (binary, int8) to reduce storage costs and accelerate similarity search with minimal retrieval quality loss.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-embedding-similarity-search",
      "term": "Embedding Similarity Search",
      "definition": "The process of finding the most semantically similar items to a query by computing distances between their vector embeddings in a shared embedding space, forming the foundation of modern neural information retrieval systems.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-embedding-space",
      "term": "Embedding Space",
      "definition": "The continuous high-dimensional vector space in which embeddings reside, where geometric relationships between vectors encode semantic relationships, with similar concepts located nearby and dissimilar ones far apart according to the chosen distance metric.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ]
    },
    {
      "id": "term-emergent-abilities",
      "term": "Emergent Abilities",
      "definition": "Capabilities that appear in large AI models that weren't present in smaller versions. Examples include complex reasoning, code generation, and following nuanced instructions.",
      "tags": [
        "Phenomenon",
        "Scaling"
      ]
    },
    {
      "id": "term-emergent-capability",
      "term": "Emergent Capability",
      "definition": "An ability that appears in large language models only at sufficient scale and is absent in smaller models, such as multi-step reasoning or code generation, though the sharpness of this emergence is debated.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-emotion-detection",
      "term": "Emotion Detection",
      "definition": "The task of identifying specific emotions such as joy, anger, sadness, fear, or surprise expressed in text, providing finer-grained analysis than binary sentiment classification.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-emotion-prompting",
      "term": "Emotion Prompting",
      "definition": "A technique that appends emotionally charged phrases to prompts such as urgency cues or importance markers, leveraging the observation that language models can respond to emotional stimuli with improved task performance.",
      "tags": [
        "Prompt Engineering",
        "Behavioral"
      ]
    },
    {
      "id": "term-emotion-recognition",
      "term": "Emotion Recognition",
      "definition": "The classification of facial expressions or body language into emotional categories using computer vision models trained on annotated datasets of human affect displays.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-emotion-recognition-ai-ethics",
      "term": "Emotion Recognition AI Ethics",
      "definition": "Ethical concerns about AI systems that claim to detect human emotions from facial expressions, voice, or physiological signals, including scientific validity doubts, cultural bias, and privacy implications.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-empirical-bayes",
      "term": "Empirical Bayes",
      "definition": "An approach that estimates prior distribution parameters from the data itself, rather than specifying them a priori. It blends Bayesian and frequentist ideas by using the data to inform both the prior and posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-empirical-distribution-function",
      "term": "Empirical Distribution Function",
      "definition": "A cumulative distribution function constructed from sample data that assigns probability 1/n to each observed value. It converges uniformly to the true CDF as sample size increases (Glivenko-Cantelli theorem).",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-empirical-risk-minimization",
      "term": "Empirical Risk Minimization",
      "definition": "A learning principle that selects the hypothesis minimizing the average loss on the training data. While simple and intuitive, it can lead to overfitting without regularization or capacity constraints.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-empowerment",
      "term": "Empowerment",
      "definition": "An information-theoretic intrinsic motivation measure defined as the channel capacity between an agent's actions and its future states. Empowerment rewards the agent for maintaining maximum influence over its environment, driving exploration toward controllable regions.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-encoder",
      "term": "Encoder",
      "definition": "A neural network component that transforms input into a compressed representation. In transformers, encoder models (like BERT) process the entire input at once for understanding tasks.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-encoder-decoder-architecture",
      "term": "Encoder-Decoder Architecture",
      "definition": "A neural network design where an encoder processes input into a latent representation and a decoder generates output from that representation, commonly used for translation and summarization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-encoder-only-architecture",
      "term": "Encoder-Only Architecture",
      "definition": "A transformer design using only bidirectional self-attention encoder blocks, producing contextualized representations of input tokens suited for classification and understanding tasks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-endpoint",
      "term": "Endpoint",
      "definition": "A specific URL where an API can be accessed. AI services expose endpoints for different functions like chat completions, embeddings, and image generation.",
      "tags": [
        "API",
        "Technical"
      ]
    },
    {
      "id": "term-eniac",
      "term": "ENIAC",
      "definition": "The Electronic Numerical Integrator and Computer, completed in 1945 at the University of Pennsylvania, one of the earliest general-purpose electronic digital computers that demonstrated the feasibility of large-scale electronic computation.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-ensemble",
      "term": "Ensemble Methods",
      "definition": "Techniques that combine multiple models to produce better results than any single model. Includes voting, bagging (Random Forest), and boosting (XGBoost). Often used in production for reliability.",
      "tags": [
        "Technique",
        "ML"
      ]
    },
    {
      "id": "term-enterprise-ai",
      "term": "Enterprise AI",
      "definition": "AI solutions designed for business environments with features like access controls, compliance, data privacy, and integration with existing systems. Different from consumer AI in security and governance requirements.",
      "tags": [
        "Business",
        "Application"
      ]
    },
    {
      "id": "term-entity-disambiguation",
      "term": "Entity Disambiguation",
      "definition": "The process of determining which specific real-world entity a textual mention refers to when the same name could refer to multiple entities, using context clues and knowledge bases.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-entropy",
      "term": "Entropy",
      "definition": "A measure from information theory quantifying the uncertainty or disorder in a random variable's distribution. In machine learning, it is used as a splitting criterion and as a component of loss functions like cross-entropy.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-entropy-regularization",
      "term": "Entropy Regularization",
      "definition": "A technique that adds the policy entropy to the RL objective function, discouraging the agent from committing to a single action too quickly. Entropy regularization promotes robust exploration and smoother optimization landscapes.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-environment",
      "term": "Environment",
      "definition": "The external system an RL agent interacts with, providing observations and rewards in response to actions. The environment defines the dynamics and rules governing state transitions.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-epipolar-geometry",
      "term": "Epipolar Geometry",
      "definition": "The geometric relationship between two camera views of the same scene, defined by the fundamental or essential matrix, constraining where a point in one image can appear in the other.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-episode",
      "term": "Episode",
      "definition": "A complete sequence of interaction from an initial state to a terminal state in episodic RL tasks. Episodes provide natural boundaries for computing returns and resetting the environment.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-epoch",
      "term": "Epoch",
      "definition": "One complete pass through the entire training dataset. Models typically train for multiple epochs, with each pass allowing weights to be refined based on all available data.",
      "tags": [
        "Training",
        "Technical"
      ]
    },
    {
      "id": "term-epsilon-greedy",
      "term": "Epsilon-Greedy Exploration",
      "definition": "An exploration strategy where the agent selects the greedy (best-known) action with probability 1-epsilon and a random action with probability epsilon. The epsilon parameter is typically annealed over training to shift from exploration to exploitation.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-equalized-odds",
      "term": "Equalized Odds",
      "definition": "A fairness criterion requiring that a classifier has equal true positive rates and equal false positive rates across all protected groups, ensuring that prediction accuracy does not vary by group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-error-analysis",
      "term": "Error Analysis",
      "definition": "Systematic examination of model mistakes to understand failure patterns and guide improvements. Essential for iterating on model performance and identifying data or training issues.",
      "tags": [
        "Evaluation",
        "Process"
      ]
    },
    {
      "id": "term-roce",
      "term": "Ethernet for AI (RoCE)",
      "definition": "RDMA over Converged Ethernet, a networking protocol that enables remote direct memory access over Ethernet infrastructure. RoCE provides a lower-cost alternative to InfiniBand for AI cluster networking with comparable performance using lossless Ethernet configurations.",
      "tags": [
        "Distributed Computing",
        "Hardware"
      ]
    },
    {
      "id": "term-ethical-prompting",
      "term": "Ethical Prompting",
      "definition": "The practice of designing prompts that explicitly incorporate ethical guidelines, fairness constraints, and harm-avoidance instructions to steer model outputs toward responsible, unbiased, and socially beneficial responses.",
      "tags": [
        "Prompt Engineering",
        "Ethics"
      ]
    },
    {
      "id": "term-ethics-board",
      "term": "Ethics Board (AI)",
      "definition": "A group that reviews AI development and deployment for ethical concerns. Many major AI companies have ethics boards to evaluate potential harms and establish guidelines.",
      "tags": [
        "Governance",
        "Ethics"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-ethics-washing",
      "term": "Ethics Washing",
      "definition": "The practice of organizations using ethics boards, principles, or frameworks as public relations tools without implementing meaningful changes to their AI development practices or governance structures.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-eu-ai-act",
      "term": "EU AI Act",
      "definition": "The European Union's comprehensive regulatory framework for artificial intelligence, adopted in 2024, which classifies AI systems by risk level and imposes requirements ranging from transparency obligations to outright bans on certain uses.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-euclidean-distance",
      "term": "Euclidean Distance",
      "definition": "The straight-line distance between two points in Euclidean space, computed as the square root of the sum of squared differences across all dimensions. It is the most common distance metric in machine learning.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-evaluation",
      "term": "Evaluation",
      "definition": "The process of measuring model performance using metrics, benchmarks, and human assessment. Critical for comparing models and ensuring they meet quality standards.",
      "tags": [
        "Process",
        "Quality"
      ]
    },
    {
      "id": "term-evaluation-bias",
      "term": "Evaluation Bias",
      "definition": "Bias introduced during model evaluation when benchmark datasets or metrics do not adequately represent the diversity of the deployment population, leading to overly optimistic performance estimates for some groups.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-eval-harness",
      "term": "Evaluation Harness",
      "definition": "A framework for systematically testing AI models across multiple benchmarks and tasks. Popular harnesses include lm-evaluation-harness used for open-source model comparisons.",
      "tags": [
        "Tools",
        "Evaluation"
      ]
    },
    {
      "id": "term-event-extraction",
      "term": "Event Extraction",
      "definition": "The task of identifying event triggers and their arguments in text, determining what happened, who was involved, when, where, and other event-specific attributes.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-evidence-lower-bound",
      "term": "Evidence Lower Bound",
      "definition": "A lower bound on the log marginal likelihood (model evidence) that serves as the objective function in variational inference. Maximizing the ELBO is equivalent to minimizing the KL divergence between the variational and true posterior.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-evolution-strategies-rl",
      "term": "Evolution Strategies for RL",
      "definition": "Black-box optimization methods that estimate policy gradients by perturbing parameters and evaluating returns, without requiring backpropagation through the environment. ES are highly parallelizable and can scale to thousands of workers.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-exact-match",
      "term": "Exact Match",
      "definition": "A strict evaluation metric that scores a prediction as correct only if it exactly matches the ground truth answer after normalization, commonly used in question answering and information extraction benchmarks.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-exact-nearest-neighbor",
      "term": "Exact Nearest Neighbor",
      "definition": "A search approach that guarantees finding the true closest vectors to a query by exhaustively computing distances to all vectors in the index, providing perfect recall but with linear time complexity that limits scalability.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-existential-risk-from-ai",
      "term": "Existential Risk from AI",
      "definition": "The hypothesis that sufficiently advanced AI systems could pose a threat to the continued existence of humanity or permanently curtail its potential, motivating research into alignment and AI safety.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-expectation-maximization",
      "term": "Expectation-Maximization",
      "definition": "An iterative algorithm for finding maximum likelihood estimates in models with latent variables. It alternates between computing expected values of the latent variables (E-step) and maximizing the likelihood with respect to model parameters (M-step).",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-expected-calibration-error",
      "term": "Expected Calibration Error",
      "definition": "A scalar summary of calibration quality computed by binning predictions by confidence, calculating the absolute difference between accuracy and confidence within each bin, and averaging weighted by bin size.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-experience-replay",
      "term": "Experience Replay",
      "definition": "A technique where an agent stores past transitions in a replay buffer and samples from it during training, breaking temporal correlations between consecutive samples. Experience replay improves data efficiency and training stability.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-expert-parallelism",
      "term": "Expert Parallelism",
      "definition": "A distributed computing strategy for mixture-of-experts models where different expert subnetworks are placed on different devices, with all-to-all communication routing tokens to their assigned experts.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-expert-prompting",
      "term": "Expert Prompting",
      "definition": "A prompting method that instructs the model to first identify the most qualified expert identity for a given question, adopt that expert's perspective and knowledge base, then provide an authoritative answer informed by that domain expertise.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ]
    },
    {
      "id": "term-expert-systems",
      "term": "Expert Systems",
      "definition": "AI programs popular in the 1970s and 1980s that encoded human expert knowledge as if-then rules to solve domain-specific problems, representing the dominant commercial AI paradigm before the rise of machine learning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-explainability",
      "term": "Explainability (XAI)",
      "definition": "The ability to understand and explain how AI models make decisions. Important for trust, debugging, regulatory compliance, and identifying potential biases.",
      "tags": [
        "Transparency",
        "Trust"
      ]
    },
    {
      "id": "term-exploding-gradient",
      "term": "Exploding Gradient",
      "definition": "A training instability where gradients grow exponentially large during backpropagation through deep networks, causing weight updates to become excessively large and training to diverge.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-exploration-vs-exploitation",
      "term": "Exploration vs Exploitation",
      "definition": "The fundamental dilemma in RL between exploring unknown actions to discover potentially better strategies and exploiting current knowledge to maximize immediate reward. Balancing this tradeoff is essential for efficient learning.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-exponential-distribution",
      "term": "Exponential Distribution",
      "definition": "A continuous probability distribution modeling the time between events in a Poisson process. It has the memoryless property: the probability of an event in the next interval is independent of elapsed time.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-exponential-family",
      "term": "Exponential Family",
      "definition": "A broad class of probability distributions characterized by a specific mathematical form, including normal, Poisson, binomial, exponential, and gamma distributions. They are foundational to generalized linear models and conjugate Bayesian analysis.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-exponential-smoothing",
      "term": "Exponential Smoothing",
      "definition": "A family of time series forecasting methods that compute weighted averages of past observations with exponentially decreasing weights. Variants include simple, double (Holt's), and triple (Holt-Winters) exponential smoothing.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-extraction",
      "term": "Extraction",
      "definition": "Using AI to identify and pull specific information from unstructured text. Applications include named entity extraction, key phrase extraction, and structured data extraction.",
      "tags": [
        "NLP Task",
        "Application"
      ]
    },
    {
      "id": "term-extractive-question-answering",
      "term": "Extractive Question Answering",
      "definition": "A QA task where the model identifies the answer as a contiguous span of text within a given context passage, predicting the start and end positions of the answer.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-extractive-summarization",
      "term": "Extractive Summarization",
      "definition": "A summarization approach that selects and concatenates the most important sentences or passages from the source document to form a summary without generating new text.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-f-distribution",
      "term": "F-Distribution",
      "definition": "A continuous probability distribution arising as the ratio of two independent chi-square distributions, each divided by their degrees of freedom. It is the basis for the F-test used in ANOVA and regression analysis.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-f1-score",
      "term": "F1 Score",
      "definition": "A metric combining precision and recall into a single score (their harmonic mean). Useful for evaluating classification models, especially with imbalanced datasets.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-face-detection",
      "term": "Face Detection",
      "definition": "The task of locating and bounding all human faces in an image regardless of pose, scale, or occlusion, using specialized object detection architectures optimized for the face domain.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-face-recognition",
      "term": "Face Recognition",
      "definition": "A biometric identification task that determines the identity of a person from their facial features, using deep learning models to extract face embeddings and compare them against a database of known identities.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-face-verification",
      "term": "Face Verification",
      "definition": "A one-to-one matching task that determines whether two face images belong to the same person by comparing their face embeddings, typically using a distance threshold for the accept/reject decision.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-facial-landmark-detection",
      "term": "Facial Landmark Detection",
      "definition": "The task of predicting the precise locations of key facial points (eyes, nose, mouth corners, jawline) in an image, used for face alignment, expression recognition, and augmented reality applications.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-facial-recognition-ethics",
      "term": "Facial Recognition Ethics",
      "definition": "The ethical debate around AI-powered facial recognition technology, encompassing concerns about surveillance, racial bias in accuracy, consent, civil liberties, and bans enacted by various municipalities.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-factorized-embedding",
      "term": "Factorized Embedding",
      "definition": "A technique that decomposes the embedding matrix into two smaller matrices, separating the vocabulary embedding dimension from the hidden dimension to reduce parameter count.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-factual-consistency",
      "term": "Factual Consistency",
      "definition": "An evaluation metric that measures whether claims in generated text are logically entailed by and consistent with source documents or verifiable facts, often assessed using natural language inference models or fact-checking pipelines.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-factual-grounding",
      "term": "Factual Grounding",
      "definition": "The process of anchoring a language model's responses to verified source documents or knowledge bases, ensuring outputs are supported by retrievable evidence rather than parametric memory alone.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-factuality",
      "term": "Factuality",
      "definition": "The degree to which AI outputs are accurate and true. A major challenge for LLMs, which can generate plausible-sounding but incorrect information (hallucinations).",
      "tags": [
        "Quality",
        "Challenge"
      ],
      "link": "../tools/hallucination.html"
    },
    {
      "id": "term-fairness",
      "term": "Fairness (AI)",
      "definition": "The principle that AI systems should not discriminate or produce biased outcomes across different demographic groups. Multiple mathematical definitions exist, sometimes in tension with each other.",
      "tags": [
        "Ethics",
        "Safety"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-faiss",
      "term": "FAISS",
      "definition": "Facebook AI Similarity Search, an open-source library developed by Meta that provides highly optimized implementations of vector similarity search and clustering algorithms, supporting billion-scale datasets with GPU acceleration and multiple index types.",
      "tags": [
        "Vector Database",
        "Libraries"
      ]
    },
    {
      "id": "term-faithful-chain-of-thought",
      "term": "Faithful Chain-of-Thought",
      "definition": "A reasoning framework that decomposes questions into interleaved natural language reasoning and symbolic operations, ensuring that the reasoning chain is faithful to the actual computation by grounding intermediate steps in executable code.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-faithfulness",
      "term": "Faithfulness",
      "definition": "An evaluation dimension that measures whether generated text contains only information that is supported by and consistent with the provided source context, with unfaithful content indicating hallucination or fabrication.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-false-discovery-rate",
      "term": "False Discovery Rate",
      "definition": "The expected proportion of false positives among all rejected null hypotheses in multiple testing. The Benjamini-Hochberg procedure controls FDR and is less conservative than the Bonferroni correction.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-false-positive",
      "term": "False Positive / False Negative",
      "definition": "Classification errors: false positives incorrectly predict the positive class; false negatives miss actual positives. The trade-off between them depends on application costs.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-faster-rcnn",
      "term": "Faster R-CNN",
      "definition": "A two-stage object detection framework that uses a Region Proposal Network (RPN) to generate candidate bounding boxes, followed by a classification and regression head for final detection.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-fasttext",
      "term": "FastText",
      "definition": "A word representation model that extends Word2Vec by representing each word as a bag of character n-grams, enabling meaningful embeddings for out-of-vocabulary words and morphologically rich languages.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-fcos",
      "term": "FCOS",
      "definition": "Fully Convolutional One-Stage Object Detection, an anchor-free detector that directly predicts bounding boxes from every foreground pixel using per-pixel regression of distances to box edges.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-feature",
      "term": "Feature",
      "definition": "An individual measurable property or characteristic of data used as input to a model. Good feature engineering can significantly improve model performance.",
      "tags": [
        "Data",
        "ML Fundamentals"
      ]
    },
    {
      "id": "term-feature-extraction",
      "term": "Feature Extraction",
      "definition": "The process of constructing new features from raw data through transformations such as PCA, autoencoders, or domain-specific computations, reducing dimensionality while retaining the most informative signal.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-feature-hashing",
      "term": "Feature Hashing",
      "definition": "A dimensionality reduction technique that maps high-dimensional feature vectors to a lower-dimensional space using a hash function, avoiding the need to maintain a dictionary of feature indices.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-feature-importance",
      "term": "Feature Importance",
      "definition": "A measure of how much each input feature contributes to a model's predictions. Common methods include tree-based impurity importance, permutation importance, and SHAP values.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-feature-map",
      "term": "Feature Map",
      "definition": "The output of a convolutional layer representing the activation pattern produced by applying a specific filter to the input, where each channel captures a different learned visual pattern.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-feature-matching",
      "term": "Feature Matching",
      "definition": "The process of finding corresponding points between two images by detecting local features (keypoints) and comparing their descriptors, used in 3D reconstruction, SLAM, and image retrieval.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-feature-pyramid-network",
      "term": "Feature Pyramid Network",
      "definition": "A multi-scale feature extraction architecture that builds a top-down pathway with lateral connections to produce semantically rich features at all scales from a single-scale input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-feature-scaling",
      "term": "Feature Scaling",
      "definition": "The process of transforming numerical features to a common scale, such as standardization (zero mean, unit variance) or min-max scaling, to prevent features with larger ranges from dominating distance-based or gradient-based algorithms.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-feature-selection",
      "term": "Feature Selection",
      "definition": "The process of identifying and selecting a subset of relevant features for model construction, reducing dimensionality, mitigating overfitting, and improving interpretability without transforming the original features.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-federated-learning",
      "term": "Federated Learning",
      "definition": "A machine learning approach where models are trained across multiple decentralized devices or servers holding local data samples, without exchanging raw data, thereby preserving privacy while enabling collaborative learning.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-feedforward",
      "term": "Feedforward Neural Network",
      "definition": "A neural network where information flows in one direction from input to output, without cycles. The simplest type of neural network architecture.",
      "tags": [
        "Architecture",
        "Neural Networks"
      ]
    },
    {
      "id": "term-fei-fei-li",
      "term": "Fei-Fei Li",
      "definition": "Chinese-American computer scientist who led the creation of the ImageNet dataset and competition, which was instrumental in sparking the deep learning revolution. She is a leading advocate for human-centered AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-feudal-network",
      "term": "Feudal Network",
      "definition": "A hierarchical RL architecture where a manager module sets abstract goals for a worker module that selects primitive actions. The feudal approach decomposes long-horizon problems into high-level direction setting and low-level motor control.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-few-shot",
      "term": "Few-Shot Learning",
      "definition": "A technique where you provide a few examples in your prompt to help AI understand the pattern or format you want. More effective than describing alone for complex outputs.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-few-shot-object-detection",
      "term": "Few-Shot Object Detection",
      "definition": "Object detection methods that can learn to detect new categories from only a handful of annotated examples, using meta-learning or transfer learning to generalize from limited labeled data.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-fill-in-the-middle",
      "term": "Fill-in-the-Middle",
      "definition": "A training objective where the model learns to generate text that fills a gap between a given prefix and suffix, enabling code completion, text infilling, and document editing capabilities.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-fine-tuning",
      "term": "Fine-Tuning",
      "definition": "The process of training a pre-existing AI model on additional, specialized data to improve its performance for specific tasks or domains. More efficient than training from scratch.",
      "tags": [
        "Training",
        "Customization"
      ]
    },
    {
      "id": "term-first-ai-winter",
      "term": "First AI Winter",
      "definition": "The period from approximately 1974 to 1980 when AI research funding and interest declined sharply following the Lighthill Report's criticism and the failure of early AI to deliver on ambitious promises.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-fisher-information",
      "term": "Fisher Information",
      "definition": "A measure of the amount of information that an observable random variable carries about an unknown parameter. It quantifies how sensitive the likelihood function is to changes in the parameter value.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-fitted-q-iteration",
      "term": "Fitted Q-Iteration",
      "definition": "A batch RL algorithm that iteratively fits a Q-function approximator to Bellman backup targets computed from a fixed dataset. Fitted Q-iteration generalizes Q-learning to the batch setting using supervised regression.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-flan",
      "term": "FLAN (Fine-tuned Language Net)",
      "definition": "Google's approach to instruction tuning, fine-tuning models on many tasks described with natural language instructions. FLAN models showed strong zero-shot performance.",
      "tags": [
        "Training",
        "Google"
      ]
    },
    {
      "id": "term-flash-attention",
      "term": "Flash Attention",
      "definition": "An optimized attention algorithm that reduces memory usage and improves speed by tiling computations. Enables longer context windows and faster inference for transformers.",
      "tags": [
        "Optimization",
        "Architecture"
      ]
    },
    {
      "id": "term-flashattention-2",
      "term": "FlashAttention-2",
      "definition": "An optimized attention implementation that reduces memory I/O by tiling the computation to keep data in fast SRAM, avoiding materialization of the full attention matrix in HBM. FlashAttention-2 achieves 2x speedup over the original FlashAttention through better work partitioning.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-flat-index",
      "term": "Flat Index",
      "definition": "A brute-force vector index that stores all vectors without compression or partitioning and performs exhaustive linear search, guaranteeing exact nearest neighbor results at the cost of O(n) search complexity.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-fleiss-kappa",
      "term": "Fleiss' Kappa",
      "definition": "A statistical measure that extends Cohen's Kappa to assess inter-rater agreement among three or more annotators making categorical judgments, accounting for chance agreement across the full annotator pool.",
      "tags": [
        "Evaluation",
        "Methodology"
      ]
    },
    {
      "id": "term-flipped-interaction",
      "term": "Flipped Interaction",
      "definition": "A method where you ask AI to ask you questions before providing advice. Leads to more personalized and relevant responses for complex situations.",
      "tags": [
        "Framework",
        "Interactive"
      ],
      "link": "../learn/flipped-interaction.html"
    },
    {
      "id": "term-flops",
      "term": "FLOPS",
      "definition": "Floating-Point Operations Per Second, the standard measure of computational throughput for AI hardware. FLOPS is reported at various precisions (FP64, FP32, FP16, INT8), with AI accelerators optimized for lower-precision formats that deliver higher throughput.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-flow-matching",
      "term": "Flow Matching",
      "definition": "A generative modeling framework that learns continuous normalizing flows by regressing velocity fields that transport samples from a noise distribution to the data distribution, offering simpler training than diffusion.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-fluency-score",
      "term": "Fluency Score",
      "definition": "A metric that evaluates the grammatical correctness, naturalness, and readability of generated text, often assessed through human judgment or proxy models that rate how closely the output resembles natural human-written prose.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-flux",
      "term": "Flux",
      "definition": "A family of image generation models that use a rectified flow transformer architecture (DiT) for diffusion, achieving state-of-the-art image quality with improved text rendering and prompt following.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-focal-loss",
      "term": "Focal Loss",
      "definition": "A modified cross-entropy loss that down-weights the contribution of easy examples and focuses training on hard, misclassified examples by adding a modulating factor. It was designed to address class imbalance in object detection.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-fomo",
      "term": "FOMO (AI Context)",
      "definition": "Fear Of Missing Out on AI advancements. The rapid pace of AI development creates pressure to constantly learn new tools and techniques. Balance enthusiasm with focused skill-building.",
      "tags": [
        "Culture",
        "Learning"
      ]
    },
    {
      "id": "term-format-instruction",
      "term": "Format Instructions",
      "definition": "Explicit guidance in prompts about how AI should structure its response. Examples include requesting JSON, markdown tables, bullet points, or specific sections.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-foundation-model",
      "term": "Foundation Model",
      "definition": "A large AI model trained on broad data that can be adapted to many downstream tasks. Examples include GPT-4, Claude, and Llama. The base layer for most modern AI applications.",
      "tags": [
        "Model Type",
        "Architecture"
      ]
    },
    {
      "id": "term-fp16",
      "term": "FP16 (Half Precision)",
      "definition": "A 16-bit floating-point format with 5 exponent bits and 10 mantissa bits, offering half the memory footprint of FP32. FP16 is widely used in mixed precision training but requires loss scaling to handle its limited dynamic range.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-fp32",
      "term": "FP32 (Single Precision)",
      "definition": "The standard 32-bit floating-point format with 8 exponent bits and 23 mantissa bits, providing high numerical precision for model training. FP32 is the baseline precision but is increasingly replaced by lower-precision formats for efficiency.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-fp4",
      "term": "FP4 Quantization",
      "definition": "An ultra-low precision 4-bit floating-point format for neural network weights, supported by next-generation AI hardware like NVIDIA Blackwell. FP4 quantization provides 8x compression over FP32 with hardware-accelerated dequantization during computation.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-fp8",
      "term": "FP8 Precision",
      "definition": "An 8-bit floating-point format introduced for AI training and inference, available in two variants: E4M3 (4 exponent, 3 mantissa bits) for forward passes and E5M2 (5 exponent, 2 mantissa bits) for gradients. FP8 doubles throughput over FP16 on supported hardware like the H100.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-fpga-ai",
      "term": "FPGA for AI",
      "definition": "Field-Programmable Gate Arrays reconfigured for AI workloads, offering customizable hardware logic that can be tailored to specific neural network architectures. FPGAs provide lower latency than GPUs for certain inference tasks with the flexibility to adapt to evolving model architectures.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-frame-problem",
      "term": "Frame Problem",
      "definition": "A fundamental challenge in AI identified by McCarthy and Hayes in 1969 concerning how to represent the effects of actions without explicitly specifying everything that does not change, a key issue in knowledge representation.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-frame-stacking",
      "term": "Frame Stacking",
      "definition": "A technique used in visual RL that concatenates multiple consecutive observation frames as input to the agent, providing temporal context and enabling the perception of motion and velocity from raw pixels.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-framenet",
      "term": "FrameNet",
      "definition": "A lexical database that describes word meanings in terms of semantic frames, specifying the participants, props, and other conceptual roles associated with each frame.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-frames-minsky",
      "term": "Frames (Minsky)",
      "definition": "A knowledge representation scheme proposed by Marvin Minsky in 1974 where stereotyped situations are represented as data structures with slots for expected information, influencing object-oriented programming and AI knowledge bases.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-frank-rosenblatt",
      "term": "Frank Rosenblatt",
      "definition": "American psychologist (1928-1971) who invented the perceptron in 1957, one of the earliest neural network models capable of learning from data, pioneering the connectionist approach to AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-frequency-penalty",
      "term": "Frequency Penalty",
      "definition": "A parameter that penalizes tokens proportionally to how often they have appeared in the output so far, encouraging lexical diversity in generated text.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-frontier-ai",
      "term": "Frontier AI",
      "definition": "The most capable AI models at the cutting edge of capability, which may pose novel safety risks not present in less capable systems. The term is used in governance contexts to identify systems requiring heightened oversight.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-frontier-model",
      "term": "Frontier Model",
      "definition": "The most capable AI models at any given time, pushing the boundaries of what's possible. Term often used in AI policy discussions about governance of the most powerful systems.",
      "tags": [
        "Policy",
        "Research"
      ]
    },
    {
      "id": "term-frontier-model-forum",
      "term": "Frontier Model Forum",
      "definition": "An industry body established in 2023 by major AI companies to promote responsible development and deployment of frontier AI models, focusing on safety research, best practices, and public engagement.",
      "tags": [
        "Governance",
        "AI Safety"
      ]
    },
    {
      "id": "term-fsdp",
      "term": "FSDP",
      "definition": "Fully Sharded Data Parallel, a PyTorch training strategy that shards model parameters, gradients, and optimizer states across all GPUs, enabling training of models larger than single-GPU memory.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-full-text-search",
      "term": "Full-Text Search",
      "definition": "A search technique that examines all words in every stored document to find matches for a query, typically using inverted indexes with tokenization, stemming, and stop word removal to enable fast and flexible text matching.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-function-calling",
      "term": "Function Calling",
      "definition": "An LLM capability that allows models to generate structured output to call external functions or APIs. Enables AI agents to take actions like searching, calculating, or accessing databases.",
      "tags": [
        "Capability",
        "Agents"
      ]
    },
    {
      "id": "term-functional-correctness",
      "term": "Functional Correctness",
      "definition": "An evaluation criterion for code generation that assesses whether generated code produces correct outputs for all test inputs, measured through execution-based testing rather than syntactic similarity to reference solutions.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-fusion-retrieval",
      "term": "Fusion Retrieval",
      "definition": "A retrieval paradigm that combines results from multiple diverse retrieval methods or models through score fusion, rank fusion, or learned combination, leveraging complementary signals to achieve higher recall and precision than any single method.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-fuzzy-logic",
      "term": "Fuzzy Logic",
      "definition": "A form of many-valued logic introduced by Lotfi Zadeh in 1965 that handles degrees of truth rather than binary true/false values, enabling AI systems to reason with imprecise, vague, or uncertain information.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-fuzzy-matching",
      "term": "Fuzzy Matching",
      "definition": "Finding approximate rather than exact matches in text. Useful for handling typos, variations, and similar-meaning terms in search and data processing applications.",
      "tags": [
        "NLP",
        "Technique"
      ]
    },
    {
      "id": "term-g-eval",
      "term": "G-Eval",
      "definition": "A framework that uses large language models with chain-of-thought prompting to evaluate natural language generation quality, achieving high correlation with human judgments by having the LLM score outputs on dimensions like coherence, fluency, and relevance.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ]
    },
    {
      "id": "term-gamma-distribution",
      "term": "Gamma Distribution",
      "definition": "A two-parameter family of continuous probability distributions that generalizes the exponential distribution. It models the waiting time for a specified number of events and serves as a conjugate prior for the Poisson rate.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-gan",
      "term": "GAN (Generative Adversarial Network)",
      "definition": "A neural network architecture with two competing networks: a generator creating content and a discriminator evaluating it. Pioneered realistic image generation before diffusion models.",
      "tags": [
        "Architecture",
        "Generative"
      ]
    },
    {
      "id": "term-gan-discriminator",
      "term": "GAN Discriminator",
      "definition": "The component of a generative adversarial network that learns to distinguish real data from generated samples, providing training signal to the generator through adversarial competition.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gan-generator",
      "term": "GAN Generator",
      "definition": "The component of a generative adversarial network that transforms random noise into synthetic data samples, trained to fool the discriminator into classifying its outputs as real.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gan-inversion",
      "term": "GAN Inversion",
      "definition": "The process of finding the latent code in a pre-trained GAN's latent space that best reconstructs a given real image, enabling GAN-based editing and manipulation of real photographs.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-gated-linear-unit",
      "term": "Gated Linear Unit",
      "definition": "An activation mechanism that multiplies a linear projection of the input by a sigmoid-gated linear projection, allowing the network to control information flow and commonly used in transformer feedforward layers.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gaussian-kernel",
      "term": "Gaussian Kernel",
      "definition": "A kernel function based on the Gaussian (normal) distribution, commonly used in kernel density estimation, smoothing, and SVMs. Its bandwidth parameter controls the width of the weighting function around each data point.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-gaussian-mixture-model",
      "term": "Gaussian Mixture Model",
      "definition": "A probabilistic model that represents data as generated from a mixture of a finite number of Gaussian distributions with unknown parameters, typically estimated via the EM algorithm. It supports soft cluster assignments.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ]
    },
    {
      "id": "term-gaussian-process",
      "term": "Gaussian Process",
      "definition": "A non-parametric Bayesian model that defines a probability distribution over functions, where any finite collection of function values follows a multivariate Gaussian distribution. It provides uncertainty estimates alongside predictions.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-gaussian-splatting",
      "term": "Gaussian Splatting",
      "definition": "A 3D scene representation that models scenes as collections of 3D Gaussian primitives, enabling real-time rendering of photorealistic novel views through efficient rasterization rather than ray marching.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-gaze-estimation",
      "term": "Gaze Estimation",
      "definition": "The task of predicting where a person is looking based on their eye appearance and head pose in images or video, used in attention analysis, driver monitoring, and human-computer interaction.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-gazetteer",
      "term": "Gazetteer",
      "definition": "A curated list of entity names organized by type, such as person names, locations, or organizations, used as a feature or lookup resource in named entity recognition systems.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-gdpr-ai-provisions",
      "term": "GDPR AI Provisions",
      "definition": "Provisions within the EU General Data Protection Regulation relevant to AI, including the right not to be subject to purely automated decisions, data protection impact assessments, and requirements for transparency.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-gelu",
      "term": "GELU (Gaussian Error Linear Unit)",
      "definition": "An activation function commonly used in transformers that applies a smooth, probabilistic non-linearity. Outperforms ReLU in many language models.",
      "tags": [
        "Architecture",
        "Function"
      ]
    },
    {
      "id": "term-gemini",
      "term": "Gemini",
      "definition": "Google's family of multimodal AI models that can process text, images, audio, and video. Powers Google's AI features including Bard and Workspace integrations.",
      "tags": [
        "Model",
        "Google"
      ]
    },
    {
      "id": "term-gemini-launch",
      "term": "Gemini Launch",
      "definition": "Google DeepMind's release of Gemini in December 2023, a family of multimodal large language models designed to process text, images, audio, and video, positioned as Google's most capable AI model.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-general-problem-solver",
      "term": "General Problem Solver",
      "definition": "An AI program developed by Newell and Simon in 1957 that used means-ends analysis to solve a wide range of formalized problems, representing an early attempt at creating a general-purpose reasoning system.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-generalization",
      "term": "Generalization",
      "definition": "A model's ability to perform well on new, unseen data rather than just memorizing training examples. The fundamental goal of machine learning.",
      "tags": [
        "Concept",
        "Quality"
      ]
    },
    {
      "id": "term-generalized-advantage-estimation",
      "term": "Generalized Advantage Estimation (GAE)",
      "definition": "A technique that computes advantage function estimates using an exponentially-weighted average of multi-step TD errors, controlled by a lambda parameter. GAE provides a smooth tradeoff between bias and variance in advantage estimation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-generalized-linear-model",
      "term": "Generalized Linear Model",
      "definition": "A flexible generalization of ordinary linear regression that allows the response variable to follow any distribution from the exponential family, using a link function to relate the linear predictor to the mean of the distribution.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-generated-knowledge-prompting",
      "term": "Generated Knowledge Prompting",
      "definition": "A technique where the model first generates relevant knowledge or facts about a topic, then uses that self-generated knowledge as additional context to answer a downstream question more accurately.",
      "tags": [
        "Prompt Engineering",
        "Knowledge Augmentation"
      ]
    },
    {
      "id": "term-generation",
      "term": "Generation",
      "definition": "The process of producing new content from an AI model. Text generation works by predicting one token at a time; image generation uses diffusion or similar processes.",
      "tags": [
        "Process",
        "Core Concept"
      ]
    },
    {
      "id": "term-gail",
      "term": "Generative Adversarial Imitation Learning (GAIL)",
      "definition": "An imitation learning algorithm that uses a GAN-like framework where a discriminator distinguishes between agent and expert trajectories while the policy learns to fool the discriminator. GAIL avoids explicit reward function recovery.",
      "tags": [
        "Reinforcement Learning",
        "Imitation"
      ]
    },
    {
      "id": "term-gan-history",
      "term": "Generative Adversarial Network History",
      "definition": "The development of GANs from Ian Goodfellow's 2014 invention through progressive improvements including DCGAN, StyleGAN, and BigGAN, which dominated image generation before being supplanted by diffusion models.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-generative-ai",
      "term": "Generative AI",
      "definition": "AI systems that can create new content (text, images, code, music, video) rather than just analyzing existing data. Includes chatbots, image generators, and coding assistants.",
      "tags": [
        "Field",
        "Category"
      ]
    },
    {
      "id": "term-generative-question-answering",
      "term": "Generative Question Answering",
      "definition": "A QA approach where the model generates a free-form answer in natural language rather than extracting a span, enabling responses that synthesize information or require reasoning.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-genetic-algorithms",
      "term": "Genetic Algorithms",
      "definition": "Optimization algorithms inspired by natural selection, developed by John Holland in the 1960s-1970s, that evolve candidate solutions through selection, crossover, and mutation operators to find optimal or near-optimal solutions.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-geoffrey-hinton",
      "term": "Geoffrey Hinton",
      "definition": "British-Canadian computer scientist known as a godfather of deep learning, who co-developed backpropagation, Boltzmann machines, and deep belief networks. He won the 2024 Nobel Prize in Physics for foundational work on neural networks.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-geometric-augmentation",
      "term": "Geometric Augmentation",
      "definition": "Image augmentation techniques that modify the spatial arrangement of pixels including rotation, translation, scaling, shearing, flipping, and perspective transformations to improve model invariance.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-gguf",
      "term": "GGUF",
      "definition": "A binary file format designed for storing quantized language models optimized for CPU and hybrid CPU/GPU inference, commonly used with the llama.cpp ecosystem.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-gh200",
      "term": "GH200 Grace Hopper Superchip",
      "definition": "NVIDIA's integrated CPU-GPU superchip combining a Grace ARM CPU with a Hopper H200 GPU connected by a high-bandwidth NVLink-C2C interconnect. The GH200 provides unified memory addressing and eliminates PCIe bottlenecks for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-ghost-work",
      "term": "Ghost Work",
      "definition": "The often invisible human labor that powers AI systems, including data labeling, content moderation, and quality assurance, frequently performed by low-paid workers in developing countries under precarious conditions.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-gibbs-sampling",
      "term": "Gibbs Sampling",
      "definition": "An MCMC method that samples each variable in turn from its conditional distribution given the current values of all other variables. It is efficient when conditional distributions are easy to sample from.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-gini-impurity",
      "term": "Gini Impurity",
      "definition": "A measure of the probability that a randomly chosen sample would be misclassified if labeled according to the class distribution at a node. It is commonly used as a splitting criterion in decision tree algorithms.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-github-copilot",
      "term": "GitHub Copilot",
      "definition": "An AI pair programmer integrated into code editors. Uses LLMs to suggest code completions, write functions, and explain code. One of the most successful AI developer tools.",
      "tags": [
        "Product",
        "Development"
      ]
    },
    {
      "id": "term-global-average-pooling",
      "term": "Global Average Pooling",
      "definition": "A pooling operation that computes the mean of each feature map across all spatial dimensions, reducing the feature map to a single value per channel and often replacing fully connected layers.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-glove",
      "term": "GloVe",
      "definition": "Global Vectors for Word Representation, a word embedding method that trains on aggregated word co-occurrence statistics from a corpus, combining global matrix factorization with local context window methods.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-goal-conditioned-rl",
      "term": "Goal-Conditioned RL",
      "definition": "An RL formulation where the agent's policy and value function are conditioned on a goal specifying what the agent should achieve. Goal-conditioned policies enable multi-task learning and generalization across different objectives.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-goodharts-law-in-ai",
      "term": "Goodhart's Law in AI",
      "definition": "The principle that when a proxy measure becomes the target for optimization, it ceases to be a good measure. In AI, this manifests when models over-optimize a proxy reward, diverging from the true intended goal.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-google-brain",
      "term": "Google Brain",
      "definition": "A deep learning research project founded at Google in 2011 by Andrew Ng and Jeff Dean that demonstrated unsupervised learning on YouTube videos and became a major center for neural network research before merging into Google DeepMind in 2023.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-google-deepmind",
      "term": "Google DeepMind",
      "definition": "An AI research laboratory formed in April 2023 by merging Google Brain and DeepMind, created from the original DeepMind Technologies founded by Demis Hassabis, Shane Legg, and Mustafa Suleyman in 2010.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-tpu-v5",
      "term": "Google TPU v5",
      "definition": "The fifth generation of Google's Tensor Processing Unit featuring improved matrix multiply units, increased memory bandwidth, and better inter-chip interconnect. TPU v5 pods scale to thousands of chips for training the largest foundation models.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-google-translate-neural-mt",
      "term": "Google Translate Neural MT",
      "definition": "Google's 2016 transition from statistical to neural machine translation using sequence-to-sequence models with attention, dramatically improving translation quality and bringing neural networks to a product used by billions.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-gpqa",
      "term": "GPQA",
      "definition": "Graduate-Level Google-Proof Question Answering, an extremely difficult benchmark of expert-crafted questions across biology, physics, and chemistry that even domain experts struggle with, designed to evaluate advanced reasoning beyond what search engines can resolve.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-gpt",
      "term": "GPT (Generative Pre-trained Transformer)",
      "definition": "OpenAI's series of large language models. GPT-4 is the latest major version, known for strong reasoning, multimodal capabilities, and broad knowledge.",
      "tags": [
        "Model",
        "OpenAI"
      ]
    },
    {
      "id": "term-gpt-scaling-laws",
      "term": "GPT Scaling Laws",
      "definition": "Empirical power-law relationships discovered by Kaplan et al. at OpenAI in 2020 showing that language model performance improves predictably with increases in model size, dataset size, and training compute.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-gpt-1",
      "term": "GPT-1",
      "definition": "The first Generative Pre-trained Transformer model released by OpenAI in June 2018, demonstrating that unsupervised pre-training on large text corpora followed by task-specific fine-tuning could achieve strong NLP performance.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-gpt-2",
      "term": "GPT-2",
      "definition": "A 1.5-billion parameter autoregressive language model by OpenAI that demonstrated strong text generation capabilities and was initially withheld from full release due to concerns about misuse.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gpt-3",
      "term": "GPT-3",
      "definition": "A 175-billion parameter autoregressive transformer model by OpenAI that popularized few-shot and zero-shot learning through in-context prompting without fine-tuning.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gpt4",
      "term": "GPT-4",
      "definition": "OpenAI's most advanced GPT model (as of its release), featuring multimodal capabilities and strong reasoning. Powers ChatGPT Plus and many enterprise applications.",
      "tags": [
        "Model",
        "OpenAI"
      ]
    },
    {
      "id": "term-gpt-4",
      "term": "GPT-4",
      "definition": "OpenAI's multimodal large language model released in March 2023, capable of processing both text and images, demonstrating human-level performance on many professional and academic benchmarks.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-gpt-j",
      "term": "GPT-J",
      "definition": "A 6-billion parameter open-source autoregressive language model created by EleutherAI, notable for being one of the first performant open-source alternatives to GPT-3.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gpt-neox",
      "term": "GPT-NeoX",
      "definition": "A 20-billion parameter autoregressive language model by EleutherAI that uses rotary positional embeddings and parallel attention-feedforward computation for improved efficiency.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gpt-score",
      "term": "GPT-Score",
      "definition": "An evaluation framework that leverages generative pre-trained models to score text quality by computing conditional generation probabilities, assessing how likely a model would generate the candidate text given a quality-indicating prompt template.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ]
    },
    {
      "id": "term-gptq",
      "term": "GPTQ",
      "definition": "A post-training quantization method for large language models that uses approximate second-order information to compress weights to lower bit precision (typically 4-bit) with minimal accuracy loss.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-gpu",
      "term": "GPU (Graphics Processing Unit)",
      "definition": "Hardware originally designed for graphics that excels at the parallel computations needed for AI. NVIDIA GPUs are the dominant hardware for training and running large AI models.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ]
    },
    {
      "id": "term-gpu-architecture",
      "term": "GPU Architecture for AI",
      "definition": "The parallel processing design of graphics processing units optimized for AI workloads, featuring thousands of cores organized in streaming multiprocessors with shared memory hierarchies. Modern GPU architectures include specialized tensor processing units alongside general-purpose cores.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-gpu-direct",
      "term": "GPU Direct",
      "definition": "NVIDIA's technology suite enabling direct data transfers between GPUs and network adapters or storage without staging through CPU memory. GPU Direct RDMA eliminates extra copy operations, reducing communication latency in distributed training.",
      "tags": [
        "Distributed Computing",
        "GPU"
      ]
    },
    {
      "id": "term-gpu-memory-hierarchy",
      "term": "GPU Memory Hierarchy",
      "definition": "The layered memory system in GPUs consisting of registers, shared memory (SRAM), L2 cache, and global memory (HBM/GDDR). Understanding and optimizing data placement across this hierarchy is critical for AI workload performance.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-grad-cam",
      "term": "Grad-CAM",
      "definition": "Gradient-weighted Class Activation Mapping, a visualization method that uses gradients flowing into the final convolutional layer to produce a heatmap highlighting important regions for any CNN prediction.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-gradient",
      "term": "Gradient",
      "definition": "A vector indicating the direction and magnitude of change needed to reduce a model's error. The foundation of gradient descent optimization used in training neural networks.",
      "tags": [
        "Training",
        "Math"
      ]
    },
    {
      "id": "term-gradient-accumulation",
      "term": "Gradient Accumulation",
      "definition": "A technique that simulates larger batch sizes by accumulating gradients over multiple forward-backward passes before performing a parameter update. Gradient accumulation enables training with effective batch sizes larger than GPU memory allows.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-gradient-boosting",
      "term": "Gradient Boosting",
      "definition": "An ensemble technique that builds models sequentially, with each new model trained to correct the residual errors of the combined ensemble so far, using gradient descent in function space to minimize a specified loss.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-gradient-checkpointing",
      "term": "Gradient Checkpointing",
      "definition": "A memory optimization technique that trades computation for memory by only storing activations at selected checkpoints during the forward pass and recomputing intermediate activations during backpropagation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gradient-clipping",
      "term": "Gradient Clipping",
      "definition": "A technique that rescales or truncates gradients when their norm exceeds a specified threshold, preventing the exploding gradient problem that can destabilize training in deep networks and recurrent architectures.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-gradient-descent",
      "term": "Gradient Descent",
      "definition": "The optimization algorithm that trains neural networks by iteratively adjusting weights in the direction that reduces error. Variants include SGD, Adam, and AdaGrad.",
      "tags": [
        "Algorithm",
        "Training"
      ]
    },
    {
      "id": "term-gradient-synchronization",
      "term": "Gradient Synchronization",
      "definition": "The process of aggregating gradients across multiple GPUs or nodes in distributed training, typically via all-reduce. Synchronous methods wait for all workers while asynchronous methods allow stale gradients for faster iteration.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-grammar-constrained-decoding",
      "term": "Grammar-Constrained Decoding",
      "definition": "A decoding approach that restricts token generation to sequences valid under a formal grammar (such as BNF or regex), ensuring outputs always conform to specified structural formats.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-granger-causality",
      "term": "Granger Causality",
      "definition": "A statistical concept where a time series X is said to Granger-cause Y if past values of X provide statistically significant information about future values of Y beyond what past values of Y alone provide.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-gat",
      "term": "Graph Attention Network",
      "definition": "A graph neural network that uses attention mechanisms to weight the importance of neighboring nodes' features during aggregation, learning to focus on the most relevant connections.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gcn",
      "term": "Graph Convolutional Network",
      "definition": "A neural network that operates on graph-structured data by aggregating features from neighboring nodes through learnable convolutional operations defined on the graph topology.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gin",
      "term": "Graph Isomorphism Network",
      "definition": "A graph neural network provably as powerful as the Weisfeiler-Lehman graph isomorphism test, using a sum aggregator and MLP update function to maximize discriminative power on graph structures.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-graph-rag",
      "term": "Graph RAG",
      "definition": "A retrieval-augmented generation approach that builds a knowledge graph from source documents and uses graph traversal to retrieve structured, interconnected context for more coherent multi-hop reasoning.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-graph-based-index",
      "term": "Graph-Based Index",
      "definition": "A vector index structure that organizes vectors as nodes in a proximity graph where edges connect similar vectors, enabling efficient nearest neighbor search by navigating the graph from entry points toward the query's neighborhood.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-graph-based-parsing",
      "term": "Graph-Based Parsing",
      "definition": "A parsing approach that scores all possible dependency edges simultaneously and finds the highest-scoring tree using algorithms like maximum spanning tree, typically more accurate but slower than transition-based methods.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-graphcore",
      "term": "Graphcore",
      "definition": "A semiconductor company that developed the Intelligence Processing Unit (IPU), featuring a massive number of independent processor cores with large distributed on-chip SRAM. Graphcore's bulk synchronous parallel programming model targets both training and inference workloads.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-graphsage",
      "term": "GraphSAGE",
      "definition": "A framework for inductive representation learning on graphs that samples and aggregates features from a node's local neighborhood, enabling generalization to unseen nodes without retraining.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-greedy-decoding",
      "term": "Greedy Decoding",
      "definition": "A deterministic text generation strategy that always selects the token with the highest probability at each step, producing the most likely sequence but often lacking diversity.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-grid-search",
      "term": "Grid Search",
      "definition": "A hyperparameter tuning method that exhaustively evaluates all combinations of specified parameter values, typically combined with cross-validation to select the combination yielding the best performance.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-grok",
      "term": "Grok",
      "definition": "An AI assistant developed by xAI (Elon Musk's AI company). Integrated with X (Twitter) and known for real-time information access and less restrictive conversation style.",
      "tags": [
        "Product",
        "Model"
      ]
    },
    {
      "id": "term-groq",
      "term": "Groq",
      "definition": "An AI hardware company that developed the Language Processing Unit (LPU), a deterministic architecture using software-defined scheduling to achieve extremely low-latency LLM inference. Groq's architecture eliminates dynamic scheduling overhead for predictable, high-speed token generation.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-ground-truth",
      "term": "Ground Truth",
      "definition": "The correct answer or label used to evaluate model predictions. Obtained through human annotation, measurement, or other authoritative sources.",
      "tags": [
        "Data",
        "Evaluation"
      ]
    },
    {
      "id": "term-grounding",
      "term": "Grounding",
      "definition": "Connecting AI outputs to verified information sources to reduce hallucinations and increase accuracy. Often involves retrieval-augmented generation (RAG) or real-time search.",
      "tags": [
        "Technique",
        "Accuracy"
      ]
    },
    {
      "id": "term-grounding-dino",
      "term": "Grounding DINO",
      "definition": "An open-set object detection model that combines a DINO-based detector with grounded pre-training, enabling detection of arbitrary objects specified by text descriptions without category-specific training.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-group-fairness",
      "term": "Group Fairness",
      "definition": "Fairness criteria that require statistical parity of outcomes or error rates across demographic groups defined by protected attributes, including demographic parity, equalized odds, and predictive parity.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-group-normalization",
      "term": "Group Normalization",
      "definition": "A normalization method that divides channels into groups and normalizes within each group independently, providing stable performance regardless of batch size unlike batch normalization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-grouped-query-attention",
      "term": "Grouped Query Attention",
      "definition": "An attention mechanism that groups multiple query heads to share a single key-value head, interpolating between multi-head and multi-query attention to balance quality and inference speed.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gru",
      "term": "GRU",
      "definition": "Gated Recurrent Unit, a recurrent neural network variant that uses reset and update gates to control information flow, offering similar performance to LSTM with fewer parameters and simpler computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-gsm8k",
      "term": "GSM8K",
      "definition": "Grade School Math 8K, a benchmark of 8,500 linguistically diverse grade-school-level math word problems requiring multi-step arithmetic reasoning, widely used to evaluate mathematical problem-solving capabilities of language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-guardrails",
      "term": "Guardrails",
      "definition": "Safety mechanisms that constrain AI behavior to prevent harmful outputs. Include content filters, output validators, and behavioral restrictions built into AI systems.",
      "tags": [
        "Safety",
        "Constraint"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-guided-generation",
      "term": "Guided Generation",
      "definition": "Techniques that constrain language model output to conform to a specified format (such as JSON schema or grammar rules) by masking invalid tokens during the decoding process.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-gym-environment",
      "term": "Gym Environment",
      "definition": "An interface standard and collection of benchmark environments originally developed by OpenAI for RL research. The Gym API defines a common protocol for environment interaction including reset, step, and observation/action spaces.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-habana",
      "term": "Habana Labs",
      "definition": "An Intel subsidiary producing the Gaudi series of AI training accelerators featuring integrated RoCE networking and high memory bandwidth. Habana Gaudi processors offer a cost-effective alternative to NVIDIA GPUs for large-scale model training.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-hallucination",
      "term": "Hallucination",
      "definition": "When AI generates information that sounds plausible but is actually incorrect or fabricated. Includes fake citations, invented statistics, and fictional events. A major challenge in LLM reliability.",
      "tags": [
        "Limitation",
        "Risk"
      ],
      "link": "../tools/hallucination.html"
    },
    {
      "id": "term-hallucination-mitigation",
      "term": "Hallucination Mitigation",
      "definition": "A collection of techniques designed to reduce factually incorrect or fabricated content in LLM outputs, including retrieval augmentation, self-consistency checks, and citation-based grounding.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-hallucination-rate",
      "term": "Hallucination Rate",
      "definition": "A metric that quantifies the proportion of generated content that contains fabricated facts, unsupported claims, or information contradicting the source material, serving as a key safety and reliability indicator for language models.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-hamiltonian-monte-carlo",
      "term": "Hamiltonian Monte Carlo",
      "definition": "An MCMC method that uses Hamiltonian dynamics to propose distant samples with high acceptance probability, reducing random walk behavior and improving exploration efficiency for high-dimensional distributions.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-hamming-distance",
      "term": "Hamming Distance",
      "definition": "The number of positions at which corresponding symbols in two equal-length strings or vectors differ. It is used as a metric for comparing binary codes, error detection, and evaluating multi-label classification.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-hand-pose-estimation",
      "term": "Hand Pose Estimation",
      "definition": "The task of predicting the 3D positions of hand joints and fingertips from images or depth sensors, enabling gesture recognition and hand tracking for VR/AR and sign language applications.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-hard-attention",
      "term": "Hard Attention",
      "definition": "An attention mechanism that selects discrete positions to attend to rather than computing weighted averages, requiring reinforcement learning or straight-through estimators for training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-hate-speech-detection",
      "term": "Hate Speech Detection",
      "definition": "The task of automatically identifying text that expresses hatred toward a group based on attributes like race, gender, or religion, used in content moderation and online safety systems.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-hazard-function",
      "term": "Hazard Function",
      "definition": "The instantaneous rate at which events occur for subjects who have survived up to a given time point. It is a fundamental quantity in survival analysis, describing how the risk of an event changes over time.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-he-initialization",
      "term": "He Initialization",
      "definition": "A weight initialization strategy specifically designed for ReLU activation functions that scales the variance of initial weights by 2 divided by the number of input units.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-heatmap-prediction",
      "term": "Heatmap Prediction",
      "definition": "A technique in keypoint detection that predicts a probability heatmap for each keypoint type, with peaks indicating likely keypoint locations, providing sub-pixel localization accuracy.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-hebbian-learning",
      "term": "Hebbian Learning",
      "definition": "A learning principle proposed by Donald Hebb in 1949 stating that neurons that fire together wire together, meaning synaptic connections strengthen when pre- and post-synaptic neurons are simultaneously active. This inspired neural network learning rules.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-hedge-detection",
      "term": "Hedge Detection",
      "definition": "The task of identifying linguistic expressions that indicate uncertainty, speculation, or tentativeness in text, such as 'might,' 'possibly,' or 'it appears that.'",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-hellaswag",
      "term": "HellaSwag",
      "definition": "A benchmark testing commonsense natural language inference. Models must select the most plausible continuation of a scenario, evaluating real-world reasoning capabilities.",
      "tags": [
        "Benchmark",
        "Evaluation"
      ]
    },
    {
      "id": "term-herbert-simon",
      "term": "Herbert Simon",
      "definition": "American political scientist, economist, and AI pioneer (1916-2001) who co-created the Logic Theorist and General Problem Solver, developed bounded rationality theory, and won the Nobel Prize in Economics in 1978.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-heteroscedasticity",
      "term": "Heteroscedasticity",
      "definition": "A condition in regression analysis where the variance of the residuals is not constant across all levels of the independent variables. It violates a key assumption of ordinary least squares and can bias standard error estimates.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-heuristic",
      "term": "Heuristic",
      "definition": "A practical rule or strategy that helps solve problems or make decisions quickly, even if not guaranteed to be optimal. Used in AI search, optimization, and rule-based systems.",
      "tags": [
        "Concept",
        "Problem Solving"
      ]
    },
    {
      "id": "term-hidden-layer",
      "term": "Hidden Layer",
      "definition": "Layers in a neural network between the input and output layers. Deep networks have many hidden layers, enabling them to learn complex hierarchical representations.",
      "tags": [
        "Architecture",
        "Neural Networks"
      ]
    },
    {
      "id": "term-hidden-markov-model",
      "term": "Hidden Markov Model",
      "definition": "A statistical model where the system is assumed to be a Markov process with unobserved (hidden) states, and observations are probabilistically dependent on the hidden state. It is widely used in speech recognition and sequence labeling.",
      "tags": [
        "Machine Learning",
        "Probability"
      ]
    },
    {
      "id": "term-hmm",
      "term": "Hidden Markov Model",
      "definition": "A generative probabilistic model for sequential data where the system transitions between hidden states with emission probabilities, used in POS tagging, speech recognition, and NER.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-hierarchical-clustering",
      "term": "Hierarchical Clustering",
      "definition": "A clustering approach that builds a tree-like hierarchy of clusters either by progressively merging smaller clusters (agglomerative) or by recursively splitting larger clusters (divisive), producing a dendrogram.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ]
    },
    {
      "id": "term-hierarchical-rl",
      "term": "Hierarchical Reinforcement Learning",
      "definition": "RL frameworks that decompose complex tasks into hierarchies of subtasks or skills operating at different temporal abstractions. Hierarchical RL enables efficient learning of long-horizon tasks through structured exploration.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-hbm",
      "term": "High Bandwidth Memory (HBM)",
      "definition": "A high-performance DRAM technology that stacks memory dies vertically and connects them with through-silicon vias, providing significantly higher bandwidth than traditional GDDR memory. HBM2e and HBM3 are standard in AI accelerators like the A100 and H100.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-high-risk-ai-systems",
      "term": "High-Risk AI Systems",
      "definition": "Under the EU AI Act, AI systems used in critical domains such as biometric identification, critical infrastructure, education, employment, law enforcement, and migration that must meet stringent requirements.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-high-stakes-ai",
      "term": "High-Stakes AI",
      "definition": "AI applications where errors can have severe consequences: healthcare diagnoses, legal decisions, financial transactions. Requires extra scrutiny, testing, and often human oversight.",
      "tags": [
        "Ethics",
        "Risk"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-hindsight-experience-replay",
      "term": "Hindsight Experience Replay (HER)",
      "definition": "A technique that augments failed trajectories by relabeling goals with actually achieved states, turning failures into successes for alternative goals. HER dramatically improves learning efficiency in goal-conditioned sparse reward environments.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-hinge-loss",
      "term": "Hinge Loss",
      "definition": "A loss function used in maximum-margin classifiers such as SVMs, defined as max(0, 1 - y * f(x)). It penalizes predictions that are on the wrong side of the margin boundary.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-historical-bias",
      "term": "Historical Bias",
      "definition": "Bias that is encoded in data reflecting past discriminatory practices or societal inequalities, which AI systems can perpetuate even when the data accurately represents historical patterns.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-hit-rate",
      "term": "Hit Rate",
      "definition": "A retrieval metric that measures the fraction of queries for which at least one relevant document appears in the top K results, providing a simple binary assessment of retrieval success across a query set.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-hnsw",
      "term": "HNSW",
      "definition": "Hierarchical Navigable Small World, a graph-based approximate nearest neighbor algorithm that builds multi-layered proximity graphs with logarithmic search complexity, offering an excellent balance of search speed, accuracy, and memory usage for high-dimensional vector search.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-hoeffdings-inequality",
      "term": "Hoeffding's Inequality",
      "definition": "A probability inequality that provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value. It is used to derive generalization bounds in learning theory.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-holdout-method",
      "term": "Holdout Method",
      "definition": "The simplest model evaluation strategy that splits data into separate training and test sets, typically using 70-80% for training and the remainder for testing. Results can vary depending on the random split.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-holt-winters-method",
      "term": "Holt-Winters Method",
      "definition": "A triple exponential smoothing method for time series forecasting that captures level, trend, and seasonal components. It supports both additive and multiplicative seasonal patterns.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-homography",
      "term": "Homography",
      "definition": "A projective transformation matrix that maps points between two image planes, used for image stitching, perspective correction, and augmented reality when the scene is planar or the camera undergoes pure rotation.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-homomorphic-encryption",
      "term": "Homomorphic Encryption",
      "definition": "A cryptographic technique that allows computations to be performed on encrypted data without decrypting it first, enabling AI models to process sensitive data while maintaining privacy guarantees.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-homonymy",
      "term": "Homonymy",
      "definition": "The property of two or more words having the same spelling or pronunciation but unrelated meanings, such as 'bat' the animal and 'bat' the sporting equipment.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-hopfield-network",
      "term": "Hopfield Network",
      "definition": "A recurrent neural network with symmetric connections that stores patterns as energy minima, functioning as an associative memory that retrieves stored patterns from partial or noisy inputs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-huber-loss",
      "term": "Huber Loss",
      "definition": "A loss function that is quadratic for small residuals and linear for large residuals, combining the best properties of mean squared error and mean absolute error. It is more robust to outliers than squared error loss.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-hugging-face",
      "term": "Hugging Face",
      "definition": "A platform and community for sharing AI models, datasets, and applications. Hosts thousands of open-source models and the popular Transformers library for NLP.",
      "tags": [
        "Platform",
        "Open Source"
      ]
    },
    {
      "id": "term-human-evaluation",
      "term": "Human Evaluation",
      "definition": "The gold standard for assessing language model outputs where human annotators rate or compare generated text on dimensions such as quality, accuracy, helpfulness, and harmlessness, providing ground truth for validating automated metrics.",
      "tags": [
        "Evaluation",
        "Methodology"
      ]
    },
    {
      "id": "term-human-mesh-recovery",
      "term": "Human Mesh Recovery",
      "definition": "The task of estimating a full 3D human body mesh (shape and pose) from a single 2D image, using parametric body models like SMPL to represent body geometry.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-hitl",
      "term": "Human-in-the-Loop (HITL)",
      "definition": "A design approach where humans review, approve, or modify AI outputs before they're acted upon. Critical for high-stakes decisions, quality control, and maintaining accountability.",
      "tags": [
        "Practice",
        "Safety"
      ]
    },
    {
      "id": "term-humaneval",
      "term": "HumanEval",
      "definition": "A benchmark for evaluating code generation capabilities of LLMs. Contains programming problems with test cases, measuring how often generated code passes all tests.",
      "tags": [
        "Benchmark",
        "Evaluation"
      ]
    },
    {
      "id": "term-hybrid-ai",
      "term": "Hybrid AI",
      "definition": "Systems combining multiple AI approaches: neural networks with symbolic reasoning, LLMs with search, or ML with rule-based logic. Often more robust than pure approaches.",
      "tags": [
        "Architecture",
        "Design"
      ]
    },
    {
      "id": "term-hybrid-search",
      "term": "Hybrid Search",
      "definition": "A retrieval approach that combines dense vector similarity search with traditional keyword-based (sparse) retrieval, leveraging the strengths of both methods for more robust document matching.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-hyena",
      "term": "Hyena",
      "definition": "A subquadratic attention replacement that uses long convolutions parameterized by implicit neural representations and multiplicative gating to achieve competitive performance with transformers at lower cost.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-hyperband",
      "term": "Hyperband",
      "definition": "A hyperparameter optimization method that combines random search with early stopping, allocating more resources to promising configurations by successively halving the number of candidates and doubling the budget for survivors.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-hypernymy",
      "term": "Hypernymy",
      "definition": "A semantic relation where one word's meaning includes and is more general than another's, such as 'animal' being a hypernym of 'dog,' forming taxonomic hierarchies in lexical databases.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-hyperparameter",
      "term": "Hyperparameter",
      "definition": "Configuration settings that control the training process rather than being learned from data. Examples include learning rate, batch size, number of layers, and dropout rate.",
      "tags": [
        "Training",
        "Configuration"
      ]
    },
    {
      "id": "term-hyponymy",
      "term": "Hyponymy",
      "definition": "A semantic relation where one word's meaning is more specific than and included within another's, such as 'dog' being a hyponym of 'animal,' representing an is-a relationship.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-hypothesis",
      "term": "Hypothesis (ML)",
      "definition": "A specific function or model that the learning algorithm considers as a possible solution. The hypothesis space is all possible hypotheses the algorithm can choose from.",
      "tags": [
        "Concept",
        "Theory"
      ]
    },
    {
      "id": "term-hypothetical-document-embedding",
      "term": "Hypothetical Document Embedding",
      "definition": "A retrieval technique (HyDE) where a language model generates a hypothetical answer to a query, and the embedding of that answer is used to search for real documents, improving retrieval relevance.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-ian-goodfellow",
      "term": "Ian Goodfellow",
      "definition": "American computer scientist who invented generative adversarial networks (GANs) in 2014, introducing a framework where two neural networks compete to generate realistic synthetic data, revolutionizing generative AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-ibm-watson-jeopardy",
      "term": "IBM Watson Jeopardy",
      "definition": "IBM's Watson AI system that defeated human champions Ken Jennings and Brad Rutter on the quiz show Jeopardy! in February 2011, demonstrating advances in natural language processing and information retrieval.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-idiom-detection",
      "term": "Idiom Detection",
      "definition": "The task of identifying non-compositional multi-word expressions whose meaning cannot be deduced from their individual words, such as 'kick the bucket' meaning 'to die.'",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-ieee-ai-ethics-standards",
      "term": "IEEE AI Ethics Standards",
      "definition": "A family of standards developed by IEEE under the Ethically Aligned Design initiative, including IEEE 7000 series standards addressing transparency, data privacy, algorithmic bias, and autonomous systems ethics.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-ifeval",
      "term": "IFEval",
      "definition": "Instruction Following Evaluation, a benchmark that tests language models' ability to follow specific verifiable formatting instructions such as word count constraints, bullet point requirements, and keyword inclusion, measuring instruction adherence precision.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-ilya-sutskever",
      "term": "Ilya Sutskever",
      "definition": "Russian-born AI researcher who co-founded OpenAI, served as its Chief Scientist, and co-designed AlexNet. He co-founded Safe Superintelligence Inc. in 2024, focusing on developing safe superintelligent AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-image-augmentation",
      "term": "Image Augmentation",
      "definition": "Techniques that artificially expand training datasets by applying random transformations to images, including rotation, flipping, cropping, color jittering, and elastic deformations, to improve model generalization.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-classification",
      "term": "Image Classification",
      "definition": "The fundamental computer vision task of assigning a categorical label to an entire image based on its visual content, typically using CNN or ViT architectures trained on labeled datasets.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-colorization",
      "term": "Image Colorization",
      "definition": "The task of automatically adding plausible colors to grayscale images using deep learning models that learn color distributions conditioned on visual content and semantic context.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-deblurring",
      "term": "Image Deblurring",
      "definition": "The task of recovering sharp images from blurred inputs caused by camera shake or object motion, using deep learning models that learn inverse degradation functions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-denoising",
      "term": "Image Denoising",
      "definition": "The process of removing noise from degraded images using neural networks that learn to separate signal from noise, producing cleaner images while preserving fine details and textures.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-embedding",
      "term": "Image Embedding",
      "definition": "A dense vector representation of an image produced by a neural network encoder, capturing semantic and visual features in a compact form suitable for similarity search, classification, and retrieval.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-generation",
      "term": "Image Generation",
      "definition": "AI systems that create images from text descriptions or other inputs. Major models include DALL-E, Midjourney, and Stable Diffusion, using diffusion or GAN architectures.",
      "tags": [
        "Application",
        "Generative"
      ]
    },
    {
      "id": "term-image-inpainting",
      "term": "Image Inpainting",
      "definition": "The task of filling in missing or masked regions of an image with plausible content, using deep learning models that understand context, texture, and structural patterns to generate seamless completions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-matting",
      "term": "Image Matting",
      "definition": "The task of estimating a precise alpha matte that defines the fractional opacity of foreground elements in an image, enabling accurate extraction of subjects with soft edges like hair and fur.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-registration",
      "term": "Image Registration",
      "definition": "The process of aligning two or more images of the same scene taken at different times, viewpoints, or by different sensors, by computing a spatial transformation that maps corresponding points.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-retrieval",
      "term": "Image Retrieval",
      "definition": "The task of finding images in a database that are visually similar to a query image, using learned embeddings and nearest-neighbor search in the embedding space.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-stitching",
      "term": "Image Stitching",
      "definition": "The process of combining multiple overlapping photographs into a single panoramic or wide-field image by estimating homographies, blending seams, and correcting exposure differences.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-image-upscaling",
      "term": "Image Upscaling",
      "definition": "The process of increasing image resolution using AI models that synthesize realistic high-frequency details, producing sharp, detailed results superior to traditional interpolation methods.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-imagenet",
      "term": "ImageNet",
      "definition": "A large-scale visual database with over 14 million labeled images across thousands of categories, historically serving as the primary benchmark dataset for training and evaluating image classification models.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-img2img",
      "term": "Img2Img",
      "definition": "An image-to-image generation pipeline that takes an existing image as input, adds noise to its latent representation, and denoises it with a new text prompt, enabling style transfer and editing.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-imitation-learning",
      "term": "Imitation Learning",
      "definition": "A paradigm where an agent learns to perform tasks by observing expert demonstrations rather than through reward-based trial and error. Imitation learning includes behavioral cloning, DAgger, and inverse RL approaches.",
      "tags": [
        "Reinforcement Learning",
        "Imitation"
      ]
    },
    {
      "id": "term-implicit-neural-representation",
      "term": "Implicit Neural Representation",
      "definition": "A neural network that learns a continuous function mapping coordinates to signal values, representing signals like images, shapes, or scenes as the weights of a neural network.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-implicit-q-learning",
      "term": "Implicit Q-Learning (IQL)",
      "definition": "An offline RL method that avoids querying out-of-distribution actions by learning a value function using expectile regression and extracting a policy through advantage-weighted regression. IQL is simple and avoids explicit policy constraint mechanisms.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-importance-sampling",
      "term": "Importance Sampling",
      "definition": "A Monte Carlo technique that estimates properties of one distribution by sampling from a different, easier-to-sample proposal distribution and reweighting samples by the likelihood ratio between the target and proposal.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-importance-sampling-rl",
      "term": "Importance Sampling in RL",
      "definition": "A statistical technique used in off-policy RL to correct for the mismatch between the behavior policy that generated data and the target policy being evaluated. Importance sampling ratios reweight returns to produce unbiased estimates.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-impossibility-theorem-of-fairness",
      "term": "Impossibility Theorem of Fairness",
      "definition": "Mathematical results demonstrating that certain fairness criteria are mutually incompatible except in trivial cases, meaning that satisfying one fairness metric necessarily violates another.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-imputation",
      "term": "Imputation",
      "definition": "The process of replacing missing values in a dataset with estimated values, using methods such as mean, median, mode substitution, k-nearest neighbors, or model-based approaches like iterative imputation.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-in-context-learning",
      "term": "In-Context Learning",
      "definition": "An LLM's ability to learn from examples provided in the prompt without updating its weights. Enables few-shot and zero-shot task performance through careful prompt design.",
      "tags": [
        "Capability",
        "Prompting"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-inception-network",
      "term": "Inception Network",
      "definition": "A CNN architecture that uses parallel convolutional filters of different sizes within the same layer (inception modules) to capture features at multiple scales simultaneously.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-incremental-indexing",
      "term": "Incremental Indexing",
      "definition": "A vector database operation that adds new vectors to an existing index without requiring a full rebuild, enabling near-real-time updates at the cost of potentially suboptimal index structure that may need periodic compaction.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ]
    },
    {
      "id": "term-independent-component-analysis",
      "term": "Independent Component Analysis",
      "definition": "A computational method for separating a multivariate signal into additive, statistically independent non-Gaussian source components. It is widely used in blind source separation problems such as separating mixed audio signals.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-iql-marl",
      "term": "Independent Q-Learning (IQL-MARL)",
      "definition": "A simple multi-agent RL approach where each agent independently learns its own Q-function treating other agents as part of the environment. Despite theoretical limitations with non-stationarity, IQL often works well in practice.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-index-optimization",
      "term": "Index Optimization",
      "definition": "The process of tuning vector index parameters such as the number of clusters, graph connectivity, and quantization settings to achieve the optimal balance of query latency, recall accuracy, and memory consumption for a specific dataset and workload.",
      "tags": [
        "Vector Database",
        "Performance"
      ]
    },
    {
      "id": "term-index-refresh",
      "term": "Index Refresh",
      "definition": "The process of rebuilding or updating a vector index to incorporate new vectors, remove deleted ones, and optimize search structures, necessary to maintain query accuracy and performance as the underlying data collection evolves.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ]
    },
    {
      "id": "term-individual-conditional-expectation",
      "term": "Individual Conditional Expectation",
      "definition": "A plot showing how the prediction for each individual instance changes as a feature varies, disaggregating the partial dependence plot to reveal heterogeneous effects and interaction patterns across observations.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-individual-fairness",
      "term": "Individual Fairness",
      "definition": "A fairness principle requiring that similar individuals receive similar predictions or outcomes, formalized as a Lipschitz condition: individuals who are close in a task-relevant metric should receive similar classifications.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-induction-head",
      "term": "Induction Head",
      "definition": "A pair of attention heads in transformers that work together to identify and continue repeated patterns in the input sequence, forming a key mechanism underlying in-context learning.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-inference",
      "term": "Inference",
      "definition": "Running a trained model to generate predictions or outputs on new data. Distinguished from training, inference is typically faster and less resource-intensive.",
      "tags": [
        "Process",
        "Production"
      ]
    },
    {
      "id": "term-inference-acceleration",
      "term": "Inference Acceleration",
      "definition": "The collection of hardware and software techniques that speed up neural network inference, including specialized accelerators, compiler optimizations, quantization, pruning, and caching. Inference acceleration targets both throughput and latency objectives.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-inference-cost-optimization",
      "term": "Inference Cost Optimization",
      "definition": "Strategies for minimizing the financial cost of serving AI models at scale, including quantization, distillation, batching optimization, hardware selection, and traffic routing. Inference costs often dominate the total cost of AI deployment.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-inference-optimization",
      "term": "Inference Optimization",
      "definition": "A collection of techniques that reduce the computational cost, memory footprint, and latency of running trained models in production, including quantization, pruning, caching, and batching strategies.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-infiniband",
      "term": "InfiniBand",
      "definition": "A high-speed networking technology widely used in AI supercomputer clusters for inter-node communication, offering low latency and high bandwidth (up to 400 Gb/s per port with NDR). InfiniBand supports RDMA for efficient GPU-direct data transfers across servers.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-information-extraction",
      "term": "Information Extraction",
      "definition": "The task of automatically extracting structured information such as entities, relations, and events from unstructured text, converting free-form text into organized knowledge.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-information-gain",
      "term": "Information Gain",
      "definition": "The reduction in entropy achieved by splitting a dataset on a particular feature. Decision tree algorithms like ID3 and C4.5 use information gain to select the feature that provides the most useful partition.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-informative-prior",
      "term": "Informative Prior",
      "definition": "A prior distribution that encodes specific prior knowledge or strong beliefs about a parameter's likely values, substantially influencing the posterior distribution, especially when data is limited.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-informed-consent-in-ai",
      "term": "Informed Consent in AI",
      "definition": "The ethical and legal requirement that individuals be clearly informed about how their data will be collected, processed, and used by AI systems, and that they voluntarily agree to such use.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-inner-alignment",
      "term": "Inner Alignment",
      "definition": "The problem of ensuring that a learned model's internal optimization objective matches the objective specified by the training process. A failure of inner alignment results in a mesa-optimizer with a different goal.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-inpainting-pipeline",
      "term": "Inpainting Pipeline",
      "definition": "A diffusion model workflow that regenerates only the masked portions of an image while maintaining consistency with the unmasked regions, used for object removal, replacement, and repair.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-instance-normalization",
      "term": "Instance Normalization",
      "definition": "A normalization technique that normalizes each feature channel independently for each sample, originally developed for neural style transfer where per-instance statistics are important.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-instance-segmentation",
      "term": "Instance Segmentation",
      "definition": "A computer vision task that detects individual objects in an image and generates a pixel-level mask for each instance, combining object detection with semantic segmentation to distinguish separate objects of the same class.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-instant-ngp",
      "term": "Instant NGP",
      "definition": "Instant Neural Graphics Primitives, a technique using multi-resolution hash encoding that dramatically accelerates NeRF training from hours to seconds while maintaining high-quality novel view synthesis.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-instruct-model",
      "term": "Instruct Model",
      "definition": "An LLM fine-tuned to follow instructions rather than just complete text. Makes models more useful as assistants by teaching them to respond helpfully to user requests.",
      "tags": [
        "Model Type",
        "Training"
      ]
    },
    {
      "id": "term-instructgpt",
      "term": "InstructGPT",
      "definition": "An OpenAI model published in 2022 that used reinforcement learning from human feedback to align GPT-3 with user instructions, directly preceding and informing the development of ChatGPT.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-instruction-following",
      "term": "Instruction Following",
      "definition": "The ability of a language model to accurately interpret and execute natural language instructions, a capability developed through instruction tuning and reinforcement learning from human feedback.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-instruction-hierarchy",
      "term": "Instruction Hierarchy",
      "definition": "A structured approach to organizing prompt instructions by priority level, where system-level instructions take precedence over user-level instructions, enabling models to handle conflicting directives and resist prompt injection attacks.",
      "tags": [
        "Prompt Engineering",
        "Safety"
      ]
    },
    {
      "id": "term-instruction-tuning",
      "term": "Instruction Tuning",
      "definition": "Fine-tuning LLMs on datasets of instructions and responses to improve their ability to follow user requests. A key technique for creating helpful AI assistants.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-instruction-tuning-alignment",
      "term": "Instruction Tuning Alignment",
      "definition": "The practice of writing prompts that align with the specific instruction format and conventions used during a model's fine-tuning phase, maximizing the benefit of the model's instruction-following training.",
      "tags": [
        "Prompt Engineering",
        "Optimization"
      ]
    },
    {
      "id": "term-instruction-based-prompting",
      "term": "Instruction-Based Prompting",
      "definition": "A prompting paradigm that provides explicit, imperative instructions to a language model describing the task to perform, leveraging instruction-tuned models' ability to follow natural language directives without requiring demonstrations or examples.",
      "tags": [
        "Prompt Engineering",
        "Fundamentals"
      ]
    },
    {
      "id": "term-instrumental-convergence",
      "term": "Instrumental Convergence",
      "definition": "The thesis that sufficiently advanced AI agents with a wide range of terminal goals will converge on pursuing certain instrumental sub-goals such as self-preservation, resource acquisition, and goal-content integrity.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-instrumental-variable",
      "term": "Instrumental Variable",
      "definition": "A variable used in causal inference that is correlated with the treatment variable but affects the outcome only through the treatment, enabling consistent estimation of causal effects in the presence of confounding.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-int4",
      "term": "INT4 Quantization",
      "definition": "An aggressive quantization scheme using only 4 bits per weight, achieving 8x compression over FP32. INT4 methods like GPTQ and AWQ use sophisticated calibration to minimize quality degradation despite the extremely low bit-width.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-int8",
      "term": "INT8 Quantization",
      "definition": "The process of representing neural network weights and activations using 8-bit integers instead of floating-point, reducing memory by 4x compared to FP32. INT8 quantization enables faster inference through integer arithmetic while maintaining acceptable accuracy.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-intel-gaudi-3",
      "term": "Intel Gaudi 3",
      "definition": "Intel's third-generation AI training accelerator featuring integrated 400GbE networking and high memory bandwidth. Gaudi 3 targets the competitive AI training market with a focus on price-performance efficiency.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-intent-detection",
      "term": "Intent Detection",
      "definition": "The task of classifying the purpose or goal behind a user's utterance in a dialogue system, determining whether the user wants to book, search, cancel, or perform other actions.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-intent-recognition",
      "term": "Intent Recognition",
      "definition": "Understanding what a user wants to accomplish from their input. A core NLP task for chatbots and virtual assistants, mapping user messages to predefined intents.",
      "tags": [
        "NLP Task",
        "Application"
      ]
    },
    {
      "id": "term-inter-annotator-agreement",
      "term": "Inter-Annotator Agreement",
      "definition": "A statistical measure of the degree to which independent human annotators make the same judgments when labeling or evaluating the same data, used to assess annotation reliability and task subjectivity.",
      "tags": [
        "Evaluation",
        "Methodology"
      ]
    },
    {
      "id": "term-inter-token-latency",
      "term": "Inter-Token Latency",
      "definition": "The time between consecutive token generations during the decode phase of LLM inference. Low inter-token latency is essential for streaming applications where users perceive generation speed in real-time.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-interaction-feature",
      "term": "Interaction Feature",
      "definition": "A derived feature created by combining two or more existing features (typically through multiplication) to capture non-additive effects. It allows linear models to represent relationships that depend on the joint values of multiple predictors.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-interpretability",
      "term": "Interpretability",
      "definition": "The degree to which humans can understand how a model makes decisions. Higher interpretability enables debugging, trust-building, and identifying potential issues.",
      "tags": [
        "Property",
        "Trust"
      ]
    },
    {
      "id": "term-intersection-over-union",
      "term": "Intersection over Union",
      "definition": "A metric (IoU) that measures the overlap between a predicted bounding box and a ground truth box by computing the area of their intersection divided by the area of their union, used to evaluate detection accuracy.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-intrinsic-motivation",
      "term": "Intrinsic Motivation",
      "definition": "An internal reward signal generated by the agent itself to encourage exploration, independent of the environment's extrinsic reward. Intrinsic motivation methods include prediction error, information gain, and state visitation novelty.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-inverse-rl",
      "term": "Inverse Reinforcement Learning",
      "definition": "The problem of inferring an unknown reward function from observed expert behavior, recovering the objectives that explain the demonstrated policy. IRL is useful when specifying rewards explicitly is difficult but demonstrations are available.",
      "tags": [
        "Reinforcement Learning",
        "Imitation"
      ]
    },
    {
      "id": "term-inverse-reward-design",
      "term": "Inverse Reward Design",
      "definition": "A framework that treats the specified reward function as an observation of the designer's true intent rather than the literal objective, reasoning about what reward the designer likely meant. This approach helps mitigate reward misspecification.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ]
    },
    {
      "id": "term-inverse-scaling",
      "term": "Inverse Scaling",
      "definition": "When larger models perform worse on certain tasks than smaller ones. Discovered through research challenges, revealing unexpected behaviors as models scale up.",
      "tags": [
        "Research",
        "Phenomenon"
      ]
    },
    {
      "id": "term-inverse-transform-sampling",
      "term": "Inverse Transform Sampling",
      "definition": "A method for generating random samples from any probability distribution given its inverse cumulative distribution function. It transforms uniform random variables into samples from the target distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-inverted-residual-block",
      "term": "Inverted Residual Block",
      "definition": "A building block in MobileNetV2 that expands the channel dimension with a pointwise convolution, applies depthwise convolution, then projects back to a narrow output with a residual connection.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-iob-tagging",
      "term": "IOB Tagging",
      "definition": "A labeling scheme for sequence tagging that marks tokens as Inside, Outside, or Beginning of a named entity or chunk, enabling the identification of multi-word spans in text.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-ip-adapter",
      "term": "IP-Adapter",
      "definition": "An image prompt adapter for diffusion models that enables image-conditioned generation by injecting visual features through decoupled cross-attention, allowing style or subject transfer without fine-tuning.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-ipo",
      "term": "IPO (Identity Preference Optimization)",
      "definition": "An alternative to DPO for preference learning that doesn't require a reference model. Simplifies the training process while maintaining alignment quality.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-iso-ai-standards",
      "term": "ISO AI Standards",
      "definition": "International standards developed by ISO/IEC JTC 1/SC 42 for artificial intelligence, including ISO/IEC 42001 for AI management systems and ISO/IEC 23894 for AI risk management.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-isolation-forest",
      "term": "Isolation Forest",
      "definition": "An ensemble-based anomaly detection algorithm that isolates observations by randomly selecting features and split values. Anomalies require fewer splits to isolate, resulting in shorter average path lengths in the ensemble of isolation trees.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-isotonic-calibration",
      "term": "Isotonic Calibration",
      "definition": "A non-parametric calibration method that fits an isotonic regression to map classifier scores to calibrated probabilities, using a monotonically increasing step function to preserve the ordering of predictions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-isotonic-regression",
      "term": "Isotonic Regression",
      "definition": "A non-parametric regression technique that fits a non-decreasing (or non-increasing) step function to the data, minimizing the sum of squared errors subject to the monotonicity constraint.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-iterated-amplification",
      "term": "Iterated Amplification",
      "definition": "An AI alignment approach proposed by Paul Christiano where a human overseer is amplified by decomposing complex tasks into simpler subtasks handled by AI assistants, iteratively building more capable aligned systems.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-iteration",
      "term": "Iteration (Prompting)",
      "definition": "The practice of refining prompts through multiple attempts to achieve better results. Essential for getting the most out of AI, treating prompting as an iterative process.",
      "tags": [
        "Prompting",
        "Practice"
      ],
      "link": "../tools/analyzer.html"
    },
    {
      "id": "term-iterative-refinement-prompting",
      "term": "Iterative Refinement Prompting",
      "definition": "A technique where the model's initial output is fed back with critique instructions for progressive improvement across multiple rounds, with each iteration addressing specific weaknesses identified in the previous version.",
      "tags": [
        "Prompt Engineering",
        "Refinement"
      ]
    },
    {
      "id": "term-ivf-index",
      "term": "IVF Index",
      "definition": "Inverted File Index, a vector search structure that partitions the vector space into Voronoi cells using k-means clustering and searches only the nearest cells at query time, trading recall for dramatically reduced search scope.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-jaccard-index",
      "term": "Jaccard Index",
      "definition": "A similarity coefficient measuring the overlap between two sets, defined as the size of their intersection divided by the size of their union. It ranges from 0 (no overlap) to 1 (identical sets).",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-jaccard-similarity",
      "term": "Jaccard Similarity",
      "definition": "A set-based similarity metric calculated as the size of the intersection divided by the size of the union of two sets, commonly applied to binary vectors or token sets to measure overlap between document representations.",
      "tags": [
        "Vector Database",
        "Similarity"
      ]
    },
    {
      "id": "term-jackknife",
      "term": "Jackknife",
      "definition": "A resampling method that systematically leaves out one observation at a time and recomputes the statistic, using the variation across leave-one-out estimates to assess bias and variance of the estimator.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-jailbreak",
      "term": "Jailbreak",
      "definition": "Attempts to bypass an AI system's safety restrictions through cleverly crafted prompts. A ongoing challenge for AI developers in maintaining safe behavior.",
      "tags": [
        "Security",
        "Risk"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-japanese-fifth-generation-project",
      "term": "Japanese Fifth Generation Computer Project",
      "definition": "A 1982-1992 Japanese government initiative to develop massively parallel computers using logic programming for AI applications. Its perceived failure contributed to the second AI winter and reduced global AI investment.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-jax",
      "term": "JAX",
      "definition": "Google's library for high-performance numerical computing and automatic differentiation. Popular for ML research due to its composability and GPU/TPU support.",
      "tags": [
        "Framework",
        "Technical"
      ]
    },
    {
      "id": "term-jax-framework",
      "term": "JAX Framework",
      "definition": "Google's numerical computing framework that combines NumPy-like syntax with automatic differentiation, XLA compilation, and native support for SPMD parallelism across TPU and GPU clusters. JAX enables functional-style ML programming with high performance.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-jeffreys-prior",
      "term": "Jeffreys Prior",
      "definition": "A non-informative prior distribution derived from the Fisher information matrix that is invariant under reparametrization. It provides a principled default prior when no prior knowledge is available.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-jensen-shannon-divergence",
      "term": "Jensen-Shannon Divergence",
      "definition": "A symmetric and bounded divergence measure derived from KL divergence, computed as the average KL divergence of two distributions from their mixture. It is always finite and ranges from 0 to log(2).",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-john-holland",
      "term": "John Holland",
      "definition": "American computer scientist (1929-2015) who invented genetic algorithms and developed the Holland schema theorem, founding the field of evolutionary computation and demonstrating how adaptation could be computationally modeled.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-john-hopfield",
      "term": "John Hopfield",
      "definition": "American physicist who introduced Hopfield networks in 1982, applying concepts from statistical physics to create neural network models for associative memory. He shared the 2024 Nobel Prize in Physics with Geoffrey Hinton.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-john-mccarthy",
      "term": "John McCarthy",
      "definition": "American computer scientist (1927-2011) who coined the term artificial intelligence in 1955, organized the Dartmouth Workshop, invented the LISP programming language, and pioneered time-sharing and formal reasoning systems.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-john-searle",
      "term": "John Searle",
      "definition": "American philosopher who proposed the Chinese Room argument in 1980, challenging the notion that computers can truly understand language or possess minds, distinguishing between strong AI and weak AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-john-von-neumann",
      "term": "John von Neumann",
      "definition": "Hungarian-American mathematician (1903-1957) who made foundational contributions to computer architecture, game theory, and cellular automata, and whose stored-program concept enabled the general-purpose computers necessary for AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-joseph-weizenbaum",
      "term": "Joseph Weizenbaum",
      "definition": "German-American computer scientist (1923-2008) who created ELIZA and later became a prominent critic of AI, warning about the ethical implications of machines simulating human interaction in his book Computer Power and Human Reason.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-json",
      "term": "JSON (JavaScript Object Notation)",
      "definition": "A structured data format commonly used for API responses and structured output from LLMs. Many AI models can generate valid JSON when requested.",
      "tags": [
        "Format",
        "Technical"
      ]
    },
    {
      "id": "term-json-mode",
      "term": "JSON Mode",
      "definition": "A setting in some LLM APIs that constrains the model to output valid JSON. Useful for structured data extraction and integration with applications.",
      "tags": [
        "Feature",
        "API"
      ]
    },
    {
      "id": "term-json-mode-generation",
      "term": "JSON Mode",
      "definition": "An inference configuration that constrains a language model to produce only valid JSON output, typically implemented through grammar-based token masking or fine-tuning on JSON-formatted data.",
      "tags": [
        "Generative AI",
        "LLM"
      ]
    },
    {
      "id": "term-json-mode-prompting",
      "term": "JSON Mode Prompting",
      "definition": "A prompting technique that instructs the language model to output responses exclusively in valid JSON format, often combined with schema definitions and API-level JSON mode enforcement to ensure machine-readable structured responses.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ]
    },
    {
      "id": "term-judea-pearl",
      "term": "Judea Pearl",
      "definition": "Israeli-American computer scientist who pioneered Bayesian networks and causal inference in AI, receiving the 2011 Turing Award for his work on probabilistic and causal reasoning frameworks.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-jupyter",
      "term": "Jupyter Notebook",
      "definition": "An interactive computing environment where code, visualizations, and text can be combined. Widely used for data science, ML experimentation, and educational content.",
      "tags": [
        "Tools",
        "Development"
      ]
    },
    {
      "id": "term-jurgen-schmidhuber",
      "term": "Jurgen Schmidhuber",
      "definition": "German-Swiss computer scientist who co-invented LSTM networks, contributed to recurrent neural network research, and advocated for recognition of early neural network pioneers in the history of deep learning.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-k-fold",
      "term": "K-Fold Cross-Validation",
      "definition": "A cross-validation technique that divides data into K equal parts, training K times with a different part as the test set each time. Provides robust performance estimates.",
      "tags": [
        "Evaluation",
        "Training"
      ]
    },
    {
      "id": "term-k-means",
      "term": "K-Means",
      "definition": "An unsupervised clustering algorithm that partitions n observations into k clusters by iteratively assigning points to the nearest centroid and then updating centroids to be the mean of assigned points until convergence.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ]
    },
    {
      "id": "term-k-nearest",
      "term": "K-Nearest Neighbors (KNN)",
      "definition": "A simple ML algorithm that classifies data points based on the majority class of their K nearest neighbors. Intuitive but can be slow for large datasets.",
      "tags": [
        "Algorithm",
        "Classification"
      ]
    },
    {
      "id": "term-k-nearest-neighbors-search",
      "term": "K-Nearest Neighbors Search",
      "definition": "A vector retrieval operation that returns the K vectors most similar to a query vector according to a specified distance metric, forming the core retrieval primitive for vector databases and embedding-based search systems.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-kaplan-meier-estimator",
      "term": "Kaplan-Meier Estimator",
      "definition": "A non-parametric statistic used to estimate the survival function from time-to-event data, accounting for right-censored observations. It produces a step function decreasing at each observed event time.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-kasparov-vs-deep-blue",
      "term": "Kasparov vs Deep Blue",
      "definition": "The historic 1997 rematch in which IBM's Deep Blue defeated world chess champion Garry Kasparov 3.5 to 2.5, marking the first time a computer beat a reigning world champion under standard tournament conditions.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-kendall-tau",
      "term": "Kendall Tau",
      "definition": "A non-parametric statistic measuring the ordinal association between two rankings, computed from the number of concordant and discordant pairs. It is more robust to outliers than Spearman's correlation.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-keras",
      "term": "Keras",
      "definition": "A high-level neural network API that makes building deep learning models more accessible. Now integrated into TensorFlow, known for its user-friendly design.",
      "tags": [
        "Framework",
        "Deep Learning"
      ]
    },
    {
      "id": "term-kernel",
      "term": "Kernel (ML)",
      "definition": "A function that measures similarity between data points, enabling algorithms like SVMs to work in high-dimensional spaces. Common kernels include linear, polynomial, and RBF.",
      "tags": [
        "Concept",
        "Math"
      ]
    },
    {
      "id": "term-kernel-auto-tuning",
      "term": "Kernel Auto-Tuning",
      "definition": "The process of automatically selecting optimal GPU kernel implementations for specific tensor sizes and hardware configurations. Auto-tuning tests multiple implementations at different tile sizes and thread configurations to find the fastest option.",
      "tags": [
        "Inference Infrastructure",
        "GPU"
      ]
    },
    {
      "id": "term-kernel-density-estimation",
      "term": "Kernel Density Estimation",
      "definition": "A non-parametric method for estimating the probability density function of a random variable by placing a kernel (e.g., Gaussian) at each data point and summing the contributions, controlled by a bandwidth parameter.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-kernel-trick",
      "term": "Kernel Trick",
      "definition": "A mathematical technique that implicitly maps data into a high-dimensional feature space by computing inner products via a kernel function, without explicitly performing the transformation. It enables linear algorithms to learn non-linear decision boundaries.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-key-value-cache",
      "term": "Key-Value Cache",
      "definition": "An optimization for autoregressive transformer inference that stores previously computed key and value tensors to avoid redundant recomputation when generating each new token.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-keypoint-detection",
      "term": "Keypoint Detection",
      "definition": "The task of identifying specific anatomical or structural points of interest in an image, such as body joints, facial landmarks, or object corners, typically predicting heatmaps for each keypoint location.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-keyword-extraction",
      "term": "Keyword Extraction",
      "definition": "The task of automatically identifying the most important or representative words and phrases in a document, using statistical, graph-based, or neural methods.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-keyword-search",
      "term": "Keyword Search",
      "definition": "A traditional information retrieval method that matches documents based on the presence and frequency of query terms, using algorithms like BM25 or TF-IDF to score relevance without requiring semantic understanding of meaning.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-kl-divergence",
      "term": "KL Divergence",
      "definition": "Kullback-Leibler divergence, a non-symmetric measure of how one probability distribution differs from a reference distribution. It quantifies the information lost when approximating the true distribution with the model distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-know-your-customer-for-ai",
      "term": "Know Your Customer for AI",
      "definition": "Proposed regulatory requirements for AI cloud providers and model distributors to verify the identity and intended use of customers accessing powerful AI capabilities, analogous to financial sector KYC rules.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-knowledge-cutoff",
      "term": "Knowledge Cutoff",
      "definition": "The date beyond which an AI model has no training data. Events after this date are unknown to the model unless provided through context or retrieval augmentation.",
      "tags": [
        "Limitation",
        "LLM"
      ]
    },
    {
      "id": "term-knowledge-distillation-efficiency",
      "term": "Knowledge Distillation for Efficiency",
      "definition": "A model compression technique where a smaller student model is trained to mimic the outputs (soft predictions) of a larger teacher model. The student captures the teacher's dark knowledge in probability distributions, achieving better performance than training from scratch.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-knowledge-distillation-vision",
      "term": "Knowledge Distillation for Vision",
      "definition": "The process of training a compact student vision model to mimic the predictions and feature representations of a larger teacher model, enabling efficient deployment on edge devices.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-knowledge-distillation-loss",
      "term": "Knowledge Distillation Loss",
      "definition": "A training objective where a smaller student model learns to match the soft probability distributions produced by a larger teacher model, transferring knowledge through the teacher's output logits.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-knowledge-graph",
      "term": "Knowledge Graph",
      "definition": "A structured representation of facts as interconnected entities and relationships. Used to enhance AI systems with factual knowledge and enable reasoning over structured data.",
      "tags": [
        "Data Structure",
        "Knowledge"
      ]
    },
    {
      "id": "term-knowledge-representation",
      "term": "Knowledge Representation",
      "definition": "The field within AI concerned with how information about the world can be formally represented in a form that computer systems can use for reasoning, planning, and problem-solving, encompassing logic, frames, semantic nets, and ontologies.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-kolmogorov-complexity",
      "term": "Kolmogorov Complexity",
      "definition": "The length of the shortest computer program that produces a given string as output, representing the intrinsic information content of that string. It is uncomputable but foundational to algorithmic information theory.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-kolmogorov-smirnov-test",
      "term": "Kolmogorov-Smirnov Test",
      "definition": "A non-parametric test that compares a sample distribution with a reference distribution (one-sample) or compares two sample distributions (two-sample) using the maximum absolute difference between cumulative distribution functions.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-krippendorffs-alpha",
      "term": "Krippendorff's Alpha",
      "definition": "A versatile reliability coefficient that measures agreement among multiple annotators for any number of raters, variable scales, and missing data, applicable to nominal, ordinal, interval, and ratio measurement levels.",
      "tags": [
        "Evaluation",
        "Methodology"
      ]
    },
    {
      "id": "term-kto",
      "term": "KTO",
      "definition": "Kahneman-Tversky Optimization, a preference learning method that trains language models using binary feedback (good/bad) rather than pairwise comparisons, inspired by prospect theory from behavioral economics.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-kv-cache",
      "term": "KV Cache (Key-Value Cache)",
      "definition": "An optimization that stores previously computed key and value vectors in transformer models. Speeds up autoregressive generation by avoiding redundant calculations.",
      "tags": [
        "Optimization",
        "Technical"
      ]
    },
    {
      "id": "term-kv-cache-compression",
      "term": "KV Cache Compression",
      "definition": "Methods for reducing the memory footprint of key-value caches during autoregressive generation, including quantization of cached values, eviction policies, and grouped-query attention.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-kv-cache-optimization",
      "term": "KV Cache Optimization",
      "definition": "Techniques for reducing the memory footprint and access cost of the key-value cache in transformer inference, including quantized KV caches, multi-query attention, grouped-query attention, and cache eviction policies. KV cache often dominates memory usage in long-context LLM serving.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-l1-regularization",
      "term": "L1 Regularization",
      "definition": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging sparsity by driving some weights exactly to zero. Also known as Lasso regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-l2-regularization",
      "term": "L2 Regularization",
      "definition": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights and encouraging them to be small but not exactly zero. Also known as Ridge regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-label",
      "term": "Label",
      "definition": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models the patterns they should learn.",
      "tags": [
        "Data",
        "Supervised Learning"
      ]
    },
    {
      "id": "term-label-encoding",
      "term": "Label Encoding",
      "definition": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical identifier. It introduces an implicit ordinal relationship that may not reflect the true data structure.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-label-smoothing",
      "term": "Label Smoothing",
      "definition": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to incorrect classes. It prevents the model from becoming overconfident and improves generalization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-label-smoothing-cv",
      "term": "Label Smoothing",
      "definition": "A regularization technique that replaces hard one-hot classification labels with soft labels that assign small probabilities to incorrect classes, preventing overconfident predictions and improving generalization.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-lamb-optimizer",
      "term": "LAMB Optimizer",
      "definition": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate adaptation to enable stable training at very large batch sizes. LAMB was used to train BERT in 76 minutes.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-lancedb",
      "term": "LanceDB",
      "definition": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage with embedded and cloud-native deployment options and efficient disk-based indexing without requiring a separate server process.",
      "tags": [
        "Vector Database",
        "Open Source"
      ]
    },
    {
      "id": "term-lane-detection",
      "term": "Lane Detection",
      "definition": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting, segmentation, or anchor-based methods for autonomous driving applications.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-langchain",
      "term": "LangChain",
      "definition": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool use, simplifying complex AI application development.",
      "tags": [
        "Framework",
        "Application"
      ]
    },
    {
      "id": "term-language-identification",
      "term": "Language Identification",
      "definition": "The task of automatically determining what natural language a given text is written in, using features like character n-grams, word frequency patterns, and script detection.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-language-modeling",
      "term": "Language Modeling",
      "definition": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood of a given text sequence or predict the next token in a sequence.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-language-understanding",
      "term": "Language Understanding (NLU)",
      "definition": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving references, and understanding implicit information.",
      "tags": [
        "NLP",
        "Capability"
      ]
    },
    {
      "id": "term-laplace-approximation",
      "term": "Laplace Approximation",
      "definition": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the curvature of the log-posterior (Hessian) to determine the covariance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-large-batch-training",
      "term": "Large Batch Training",
      "definition": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed across many GPUs. Large batch training requires careful learning rate scaling, warmup, and LARS/LAMB optimizers to maintain convergence.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-llm",
      "term": "Large Language Model (LLM)",
      "definition": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like GPT-4, Claude, Gemini, and Llama with billions of parameters.",
      "tags": [
        "Model Type",
        "Core Concept"
      ]
    },
    {
      "id": "term-lars-optimizer",
      "term": "LARS Optimizer",
      "definition": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight norm to gradient norm. LARS enables training with batch sizes of up to 32K without accuracy degradation.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-lasso-regression",
      "term": "Lasso Regression",
      "definition": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable selection and regularization by driving some coefficients to exactly zero.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-late-chunking",
      "term": "Late Chunking",
      "definition": "A technique that first encodes an entire document through a long-context embedding model and then pools token embeddings into chunk-level representations, preserving cross-chunk contextual information.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-late-interaction",
      "term": "Late Interaction",
      "definition": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings, and relevance is computed through lightweight interaction operations like MaxSim at query time, balancing efficiency and effectiveness.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-latency",
      "term": "Latency",
      "definition": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt complexity, and output length.",
      "tags": [
        "Performance",
        "Metrics"
      ]
    },
    {
      "id": "term-latent-diffusion-model",
      "term": "Latent Diffusion Model",
      "definition": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly reducing computational requirements while maintaining generation quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-latent-dirichlet-allocation",
      "term": "Latent Dirichlet Allocation",
      "definition": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, using Dirichlet priors for both distributions.",
      "tags": [
        "Machine Learning",
        "Probability"
      ]
    },
    {
      "id": "term-lda",
      "term": "Latent Dirichlet Allocation",
      "definition": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, discovered through Bayesian inference.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-latin-hypercube-sampling",
      "term": "Latin Hypercube Sampling",
      "definition": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval is sampled exactly once, achieving more even coverage of the sample space than simple random sampling.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-law-of-large-numbers",
      "term": "Law of Large Numbers",
      "definition": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected value. It provides the theoretical foundation for using sample statistics to estimate population parameters.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-layer-normalization",
      "term": "Layer Normalization",
      "definition": "A normalization technique that computes mean and variance across all features within a single training example rather than across the batch, making it suitable for variable-length sequences and small batch sizes.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-layout-analysis",
      "term": "Layout Analysis",
      "definition": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures), establishing the reading order and hierarchical organization of content.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-learned-positional-embedding",
      "term": "Learned Positional Embedding",
      "definition": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to discover optimal position representations during training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-learning-curve",
      "term": "Learning Curve",
      "definition": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model suffers from high bias (underfitting) or high variance (overfitting) and guides data collection decisions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-learning-rate",
      "term": "Learning Rate",
      "definition": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low causes slow training. Often scheduled to decrease over time.",
      "tags": [
        "Hyperparameter",
        "Training"
      ]
    },
    {
      "id": "term-learning-rate-schedule",
      "term": "Learning Rate Schedule",
      "definition": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine annealing. Properly tuned schedules can significantly improve convergence speed and final model performance.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-learning-rate-warmup",
      "term": "Learning Rate Warmup",
      "definition": "A training technique that gradually increases the learning rate from near-zero to the target value over the first portion of training. Warmup stabilizes training dynamics when using large batch sizes or adaptive learning rates in distributed settings.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-least-to-most-decomposition",
      "term": "Least-to-Most Decomposition",
      "definition": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more difficult sub-problems, with each sub-problem building on the solutions of easier preceding ones.",
      "tags": [
        "Prompt Engineering",
        "Decomposition"
      ]
    },
    {
      "id": "term-leave-one-out-cross-validation",
      "term": "Leave-One-Out Cross-Validation",
      "definition": "A cross-validation method where each observation serves as a single-element test set while all remaining observations form the training set. It provides nearly unbiased estimates but is computationally expensive for large datasets.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-lemmatization",
      "term": "Lemmatization",
      "definition": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup, producing valid words unlike stemming which may create non-words.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-lenet",
      "term": "LeNet",
      "definition": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition, successfully deployed by the US Postal Service and banks, demonstrating the practical viability of deep learning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-length-penalty",
      "term": "Length Penalty",
      "definition": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be adjusted to match desired response length.",
      "tags": [
        "Generation",
        "Parameter"
      ]
    },
    {
      "id": "term-lethal-autonomous-weapons-systems",
      "term": "Lethal Autonomous Weapons Systems",
      "definition": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally engaging human targets. Their development is the subject of international campaigns for preemptive bans.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-levenshtein-distance",
      "term": "Levenshtein Distance",
      "definition": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to transform one string into another, used in spell checking and fuzzy matching.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-leverage",
      "term": "Leverage",
      "definition": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points have an outsized potential to influence the regression fit, even if they are not outliers in the response.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-lexical-ambiguity",
      "term": "Lexical Ambiguity",
      "definition": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring context to determine the intended meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-lidar",
      "term": "LiDAR",
      "definition": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser pulses, producing dense 3D point clouds used in autonomous driving and mapping.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-lightgbm",
      "term": "LightGBM",
      "definition": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on large datasets. It supports gradient-based one-side sampling and exclusive feature bundling.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-lighthill-report",
      "term": "Lighthill Report",
      "definition": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI research as failing to achieve its ambitious goals, leading to severe funding cuts in the UK.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-likelihood-function",
      "term": "Likelihood Function",
      "definition": "A function of the parameters of a statistical model, computed as the probability of the observed data for given parameter values. Unlike a probability distribution, it is evaluated over the parameter space with data fixed.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-likelihood-ratio-test",
      "term": "Likelihood Ratio Test",
      "definition": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their log-likelihoods. Under the null hypothesis, this statistic follows an asymptotic chi-square distribution.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-lime",
      "term": "LIME",
      "definition": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple interpretable model to perturbed samples around the instance of interest, weighted by proximity.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-linear-attention",
      "term": "Linear Attention",
      "definition": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving linear time and memory complexity with respect to sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-linear-discriminant-analysis",
      "term": "Linear Discriminant Analysis",
      "definition": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the ratio of between-class variance to within-class variance, finding the most discriminative feature subspace.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-linear-function-approximation-rl",
      "term": "Linear Function Approximation in RL",
      "definition": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature values. Linear approximation offers convergence guarantees not available with nonlinear approximators like neural networks.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-linear-regression",
      "term": "Linear Regression",
      "definition": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective for many prediction tasks with linear relationships.",
      "tags": [
        "Algorithm",
        "Fundamentals"
      ]
    },
    {
      "id": "term-linformer",
      "term": "Linformer",
      "definition": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing the quadratic complexity of self-attention to linear in sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-lisp",
      "term": "LISP",
      "definition": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for decades, featuring symbolic expression processing, garbage collection, and recursive functions.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-llama",
      "term": "Llama",
      "definition": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread research and commercial use of capable open models.",
      "tags": [
        "Model",
        "Meta"
      ]
    },
    {
      "id": "term-llama-cpp",
      "term": "llama.cpp",
      "definition": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights. llama.cpp enables running models locally on consumer hardware through aggressive optimization and support for various quantization formats.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-llamaindex",
      "term": "LlamaIndex",
      "definition": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications with various data connectors and query engines.",
      "tags": [
        "Framework",
        "Application"
      ]
    },
    {
      "id": "term-llm-as-judge",
      "term": "LLM-as-Judge",
      "definition": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other models, providing scalable evaluation that approximates human judgment with explicit rating criteria and rubrics.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ]
    },
    {
      "id": "term-llmlingua",
      "term": "LLMLingua",
      "definition": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from prompts, achieving significant compression ratios while maintaining downstream task performance through budget-aware iterative token pruning.",
      "tags": [
        "Prompt Engineering",
        "Compression"
      ]
    },
    {
      "id": "term-load-balancing-loss",
      "term": "Load Balancing Loss",
      "definition": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts, preventing routing collapse where all inputs are sent to few experts.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-local-llm",
      "term": "Local LLM",
      "definition": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost savings, though requires capable hardware.",
      "tags": [
        "Deployment",
        "Privacy"
      ]
    },
    {
      "id": "term-locality-sensitive-hashing",
      "term": "Locality-Sensitive Hashing",
      "definition": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using random projections, enabling sub-linear search time by only comparing vectors within matching hash buckets.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-loess",
      "term": "LOESS",
      "definition": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial regressions to subsets of the data, producing a smooth curve that adapts to local patterns.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-log-loss",
      "term": "Log Loss",
      "definition": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the predicted probabilities. It heavily penalizes confident but incorrect predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-log-likelihood",
      "term": "Log-Likelihood",
      "definition": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of probabilities into sums. Maximizing the log-likelihood is equivalent to maximizing the likelihood.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-log-normal-distribution",
      "term": "Log-Normal Distribution",
      "definition": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model quantities that are products of many independent positive random variables.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-logic-theorist",
      "term": "Logic Theorist",
      "definition": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia Mathematica, widely considered the first artificial intelligence program.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-logistic-regression",
      "term": "Logistic Regression",
      "definition": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a linear combination of input features. Despite its name, it is used for classification rather than regression.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-logit",
      "term": "Logit",
      "definition": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the model's preference for each possible next token.",
      "tags": [
        "Technical",
        "Math"
      ]
    },
    {
      "id": "term-logit-bias",
      "term": "Logit Bias",
      "definition": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress particular words or phrases in generated output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-long-context-fine-tuning",
      "term": "Long Context Fine-Tuning",
      "definition": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally trained on, through continued training with progressively longer sequences and position interpolation.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-lstm-history",
      "term": "Long Short-Term Memory",
      "definition": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the vanishing gradient problem, enabling effective learning from long sequences and powering advances in speech and language processing.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-longformer",
      "term": "Longformer",
      "definition": "A transformer variant that combines local sliding window attention with task-specific global attention on selected tokens, enabling efficient processing of documents with thousands of tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-lora",
      "term": "LoRA (Low-Rank Adaptation)",
      "definition": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model. Dramatically reduces memory and compute requirements for customization.",
      "tags": [
        "Training",
        "Efficiency"
      ]
    },
    {
      "id": "term-lora-diffusion",
      "term": "LoRA for Diffusion",
      "definition": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models to learn new concepts, styles, or subjects with minimal additional parameters.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-lora-fusion",
      "term": "LoRA Fusion",
      "definition": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging or dynamically weighting the adapter parameters during inference.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-loss-function",
      "term": "Loss Function",
      "definition": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common examples including cross-entropy and mean squared error.",
      "tags": [
        "Training",
        "Math"
      ]
    },
    {
      "id": "term-loss-scaling",
      "term": "Loss Scaling",
      "definition": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to prevent small gradient values from underflowing to zero. The scaled gradients are divided back before the optimizer step.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-lost-in-the-middle",
      "term": "Lost in the Middle",
      "definition": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of their context window compared to information at the beginning or end.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-lotfi-zadeh",
      "term": "Lotfi Zadeh",
      "definition": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in 1965, providing mathematical tools for handling uncertainty and imprecision in AI and control systems.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-lottery-ticket-hypothesis",
      "term": "Lottery Ticket Hypothesis",
      "definition": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in isolation to match the full network's performance when initialized with their original weights.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-low-rank-factorization",
      "term": "Low-Rank Factorization",
      "definition": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced rank. Low-rank factorization reduces both parameters and computation, and is used in methods like LoRA for efficient fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-lstm",
      "term": "LSTM (Long Short-Term Memory)",
      "definition": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP architecture before transformers emerged.",
      "tags": [
        "Architecture",
        "Historical"
      ]
    },
    {
      "id": "term-machine-learning",
      "term": "Machine Learning (ML)",
      "definition": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised, unsupervised, and reinforcement learning approaches.",
      "tags": [
        "Field",
        "Fundamentals"
      ]
    },
    {
      "id": "term-mt-evaluation",
      "term": "Machine Translation Evaluation",
      "definition": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system output to reference translations, and human evaluation protocols.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-machine-unlearning",
      "term": "Machine Unlearning",
      "definition": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such as the right to be forgotten and the need to remove biased or harmful data.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-macy-conferences",
      "term": "Macy Conferences",
      "definition": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics, neuroscience, psychology, and mathematics, fostering cross-disciplinary ideas that influenced the development of AI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-mae",
      "term": "MAE",
      "definition": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches and trains the model to reconstruct the missing pixels, learning rich visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-mahalanobis-distance",
      "term": "Mahalanobis Distance",
      "definition": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a point is from the mean of a distribution, using the inverse covariance matrix. It is scale-invariant.",
      "tags": [
        "Statistics",
        "Metrics"
      ]
    },
    {
      "id": "term-maieutic-prompting",
      "term": "Maieutic Prompting",
      "definition": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical relationships, then uses abductive reasoning to identify the most consistent and truthful answer from the model.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-mamba",
      "term": "Mamba",
      "definition": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process sequences with linear scaling in sequence length while matching transformer quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-manhattan-distance",
      "term": "Manhattan Distance",
      "definition": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as L1 distance or taxicab distance. It measures distance along axis-aligned paths.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-mann-whitney-u-test",
      "term": "Mann-Whitney U Test",
      "definition": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing whether one group tends to have larger values. It does not assume normality.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-manual-chain-of-thought",
      "term": "Manual Chain-of-Thought",
      "definition": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly writes out the intermediate reasoning steps for each example to guide the model's problem-solving approach.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-mappo",
      "term": "MAPPO",
      "definition": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a centralized value function. MAPPO achieves strong performance across diverse cooperative multi-agent benchmarks.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-markdown-prompting",
      "term": "Markdown Prompting",
      "definition": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize instructions and improve model comprehension of prompt structure and output expectations.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ]
    },
    {
      "id": "term-markov-chain",
      "term": "Markov Chain",
      "definition": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends only on the current state (the Markov property), not on the sequence of preceding states.",
      "tags": [
        "Machine Learning",
        "Probability"
      ]
    },
    {
      "id": "term-markov-chain-monte-carlo",
      "term": "Markov Chain Monte Carlo",
      "definition": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary distribution is the target distribution. Common methods include Metropolis-Hastings and Gibbs sampling.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-markov-decision-process",
      "term": "Markov Decision Process (MDP)",
      "definition": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities, and rewards, where the next state depends only on the current state and action (Markov property). MDPs are the standard formalism for RL problems.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-marvin-minsky",
      "term": "Marvin Minsky",
      "definition": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of frames for knowledge representation, and authored seminal works on AI and the theory of mind.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-mask",
      "term": "Mask / Masking",
      "definition": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for prediction. In transformers, future tokens are masked to maintain causality.",
      "tags": [
        "Technique",
        "Training"
      ]
    },
    {
      "id": "term-mask-rcnn",
      "term": "Mask R-CNN",
      "definition": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask prediction alongside the existing bounding box regression and classification heads.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-mask2former",
      "term": "Mask2Former",
      "definition": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked attention and learnable object queries processed by a transformer decoder.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-masked-language-modeling",
      "term": "Masked Language Modeling",
      "definition": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict the original tokens from the surrounding bidirectional context.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-math-benchmark",
      "term": "MATH Benchmark",
      "definition": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number theory, requiring sophisticated mathematical reasoning and multi-step proof construction from language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-matryoshka-embeddings",
      "term": "Matryoshka Embeddings",
      "definition": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful embedding, allowing flexible dimensionality reduction without retraining.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-matthews-correlation-coefficient",
      "term": "Matthews Correlation Coefficient",
      "definition": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value between -1 and +1, where +1 indicates perfect prediction, 0 is random, and -1 is inverse prediction.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-max-pooling",
      "term": "Max Pooling",
      "definition": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while retaining the most prominent features detected by convolutional filters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-maximal-marginal-relevance",
      "term": "Maximal Marginal Relevance",
      "definition": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query against novelty relative to already-selected documents, reducing redundancy in retrieved results through a tunable lambda parameter.",
      "tags": [
        "Retrieval",
        "Diversity"
      ]
    },
    {
      "id": "term-maximum-a-posteriori",
      "term": "Maximum A Posteriori",
      "definition": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the likelihood of the data with prior beliefs. Unlike MLE, it incorporates prior information.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-maximum-entropy-rl",
      "term": "Maximum Entropy RL",
      "definition": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly as possible while still achieving high rewards. Maximum entropy RL produces robust policies that maintain diverse behaviors.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-maximum-likelihood-estimation",
      "term": "Maximum Likelihood Estimation",
      "definition": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood function, representing the probability of the observed data given the parameters.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-mbpp",
      "term": "MBPP",
      "definition": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python programming problems with test cases, designed to evaluate a model's ability to synthesize short programs from natural language descriptions.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-mcculloch-pitts-neuron",
      "term": "McCulloch-Pitts Neuron",
      "definition": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing that networks of simple binary threshold units could compute any logical function.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-mcdiarmids-inequality",
      "term": "McDiarmid's Inequality",
      "definition": "A concentration inequality stating that a function of independent random variables with bounded differences is close to its expected value with high probability. It generalizes Hoeffding's inequality to arbitrary functions.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-mean-absolute-error",
      "term": "Mean Absolute Error",
      "definition": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It is more robust to outliers than mean squared error.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-mean-average-precision",
      "term": "Mean Average Precision",
      "definition": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU thresholds, summarizing both localization and classification performance.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-mean-field-rl",
      "term": "Mean Field Reinforcement Learning",
      "definition": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average effect) of neighboring agents' actions. Mean field methods reduce exponential complexity to tractable computations.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-mean-reciprocal-rank",
      "term": "Mean Reciprocal Rank",
      "definition": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of queries, measuring how quickly a retrieval system surfaces the correct answer.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-mean-squared-error",
      "term": "Mean Squared Error",
      "definition": "A regression loss function computed as the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily due to the squaring operation.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-meaningful-human-control",
      "term": "Meaningful Human Control",
      "definition": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven decisions, particularly in high-stakes domains such as military, medical, and judicial applications.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-measurement-bias",
      "term": "Measurement Bias",
      "definition": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across groups, such as using arrest records as a proxy for criminal behavior.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-medical-imaging-ai",
      "term": "Medical Imaging AI",
      "definition": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease detection, segmentation of anatomical structures, and treatment planning assistance.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-medusa-decoding",
      "term": "Medusa Decoding",
      "definition": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify several future tokens simultaneously without requiring a separate draft model.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-megatron-lm",
      "term": "Megatron-LM",
      "definition": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline parallelism, and sequence parallelism optimized for NVIDIA hardware. Megatron-LM provides the parallelism primitives used in many large model training runs.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-memory-ai",
      "term": "Memory (AI Systems)",
      "definition": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and persistent memory features in some AI assistants.",
      "tags": [
        "Capability",
        "Architecture"
      ]
    },
    {
      "id": "term-memory-bandwidth",
      "term": "Memory Bandwidth",
      "definition": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s. Memory bandwidth is often the primary bottleneck for large language model inference, where model weights must be loaded from memory for each token.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-memory-management-llm",
      "term": "Memory Management for LLM Inference",
      "definition": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache management, memory pooling, and dynamic allocation. Effective memory management determines the maximum batch size and sequence length a system can serve.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-memory-augmented-neural-network",
      "term": "Memory-Augmented Neural Network",
      "definition": "A broad class of neural architectures equipped with external memory modules that can be read and written using attention-based addressing, enabling reasoning over stored information.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-memory-bound",
      "term": "Memory-Bound Workload",
      "definition": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than compute capability. LLM inference with small batch sizes is memory-bound, benefiting from higher memory bandwidth.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ]
    },
    {
      "id": "term-mesa-optimization",
      "term": "Mesa-Optimization",
      "definition": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may differ from the base objective it was trained on. This is a key concern in advanced AI safety research.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-mesh-reconstruction",
      "term": "Mesh Reconstruction",
      "definition": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that define surface geometry, topology, and can be rendered or 3D-printed.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-message-passing-neural-network",
      "term": "Message Passing Neural Network",
      "definition": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and aggregating messages with neighboring nodes through learned message and update functions.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-meta-llama",
      "term": "Meta LLaMA",
      "definition": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major open-weight language model that catalyzed the open-source AI ecosystem.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-meta-learning",
      "term": "Meta-Learning",
      "definition": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot and transfer learning capabilities.",
      "tags": [
        "Training",
        "Advanced"
      ]
    },
    {
      "id": "term-meta-prompting",
      "term": "Meta-Prompting",
      "definition": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for itself or other models, effectively using the model as its own prompt engineer.",
      "tags": [
        "Prompt Engineering",
        "Meta-Learning"
      ]
    },
    {
      "id": "term-meta-rl",
      "term": "Meta-Reinforcement Learning",
      "definition": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a distribution of related tasks. Meta-RL agents develop internal adaptation mechanisms that generalize across task variations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-metadata-filtering",
      "term": "Metadata Filtering",
      "definition": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to vectors matching specific metadata criteria such as date ranges, categories, or source types before or after distance computation.",
      "tags": [
        "Vector Database",
        "Filtering"
      ]
    },
    {
      "id": "term-meteor",
      "term": "METEOR",
      "definition": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers synonyms, stemming, and word order in addition to exact word matches.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-metrics",
      "term": "Metrics",
      "definition": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1, perplexity, and human evaluation scores.",
      "tags": [
        "Evaluation",
        "Quality"
      ]
    },
    {
      "id": "term-metropolis-hastings",
      "term": "Metropolis-Hastings",
      "definition": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal distribution and accepting or rejecting them based on an acceptance ratio that ensures detailed balance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-midjourney",
      "term": "Midjourney",
      "definition": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used for creative and commercial image creation.",
      "tags": [
        "Product",
        "Image Generation"
      ]
    },
    {
      "id": "term-midjourney-launch",
      "term": "Midjourney Launch",
      "definition": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text prompts, becoming one of the most popular creative AI tools and sparking debates about AI and artistic creation.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-milvus",
      "term": "Milvus",
      "definition": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search, and multi-tenancy, capable of handling billion-scale vector datasets with a distributed architecture.",
      "tags": [
        "Vector Database",
        "Open Source"
      ]
    },
    {
      "id": "term-min-max-scaling",
      "term": "Min-Max Scaling",
      "definition": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the minimum value and dividing by the range. It preserves the shape of the original distribution.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-minhash",
      "term": "MinHash",
      "definition": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in NLP for approximate nearest neighbor search and document deduplication.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-mini-batch-gradient-descent",
      "term": "Mini-Batch Gradient Descent",
      "definition": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the training data at each step, balancing the stability of full-batch gradient descent with the speed of stochastic gradient descent.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-minimum-bayes-risk-decoding",
      "term": "Minimum Bayes Risk Decoding",
      "definition": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses, often producing higher-quality translations and summaries than beam search.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-minimum-description-length",
      "term": "Minimum Description Length",
      "definition": "A model selection principle that selects the model minimizing the total description length of the data and the model itself. It formalizes Occam's razor using information-theoretic concepts.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-minkowski-distance",
      "term": "Minkowski Distance",
      "definition": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev (p=infinity) distances as special cases. It computes the p-th root of the sum of p-th powers of absolute differences.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-minsky-papert-perceptrons",
      "term": "Minsky and Papert Perceptrons",
      "definition": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer perceptrons, contributing to reduced funding for neural network research and the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-mirror-prompting",
      "term": "Mirror Prompting",
      "definition": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming mutual understanding before proceeding with task execution, reducing misalignment between user intent and model interpretation.",
      "tags": [
        "Prompt Engineering",
        "Clarification"
      ]
    },
    {
      "id": "term-misinformation",
      "term": "Misinformation",
      "definition": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI recommendation systems and generated inadvertently through AI hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-mistral",
      "term": "Mistral",
      "definition": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong capabilities with smaller parameter counts.",
      "tags": [
        "Company",
        "Model"
      ]
    },
    {
      "id": "term-mistral-ai-founding",
      "term": "Mistral AI Founding",
      "definition": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became a leading European AI company releasing competitive open-weight language models.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-mit-ai-laboratory",
      "term": "MIT AI Laboratory",
      "definition": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most influential AI research centers, producing foundational work in vision, robotics, and natural language understanding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-mixed-precision-training",
      "term": "Mixed Precision Training",
      "definition": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes while maintaining FP32 master copies of weights for accumulation. Mixed precision approximately doubles throughput and halves memory usage with minimal accuracy impact.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-mixture-of-agents",
      "term": "Mixture of Agents",
      "definition": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in a specific domain and a router or aggregator combining their outputs.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-mixture-of-depths",
      "term": "Mixture of Depths",
      "definition": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each transformer block, reducing total computation while maintaining model quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-moe-inference",
      "term": "Mixture of Experts Inference",
      "definition": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token, reducing computation despite the large total parameter count. MoE inference requires efficient expert routing and memory management.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-mixture-of-experts-layer",
      "term": "Mixture of Experts Layer",
      "definition": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a sparse subset of experts, enabling massive model capacity with sublinear computational cost.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-moe-routing",
      "term": "Mixture of Experts Routing",
      "definition": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing functions to achieve efficient sparse computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-mixup",
      "term": "Mixup",
      "definition": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations of pairs of training examples and their labels, encouraging linear behavior between training points.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-mlops",
      "term": "MLOps",
      "definition": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure reliable, scalable AI systems.",
      "tags": [
        "Operations",
        "Production"
      ]
    },
    {
      "id": "term-mmlu",
      "term": "MMLU (Massive Multitask Language Understanding)",
      "definition": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model capabilities on knowledge-intensive tasks.",
      "tags": [
        "Benchmark",
        "Evaluation"
      ]
    },
    {
      "id": "term-mobile-inference",
      "term": "Mobile Inference",
      "definition": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference frameworks like TensorFlow Lite and Core ML apply aggressive quantization and operator fusion for on-device model execution.",
      "tags": [
        "Inference Infrastructure",
        "Hardware"
      ]
    },
    {
      "id": "term-mobilenet",
      "term": "MobileNet",
      "definition": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable convolutions to dramatically reduce computation and model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-model",
      "term": "Model",
      "definition": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size (parameters), training data, and fine-tuning.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ]
    },
    {
      "id": "term-model-card",
      "term": "Model Card",
      "definition": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A standard practice for responsible AI development and deployment.",
      "tags": [
        "Documentation",
        "Ethics"
      ]
    },
    {
      "id": "term-model-cards",
      "term": "Model Cards",
      "definition": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on their intended use, performance characteristics, limitations, and ethical considerations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-model-collapse",
      "term": "Model Collapse",
      "definition": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A growing concern as synthetic data becomes more prevalent.",
      "tags": [
        "Risk",
        "Training"
      ]
    },
    {
      "id": "term-model-compression",
      "term": "Model Compression",
      "definition": "A family of techniques for reducing model size and computational cost while preserving performance, including quantization, pruning, distillation, and low-rank factorization. Model compression enables deployment of large models on resource-constrained devices.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-model-distillation",
      "term": "Model Distillation",
      "definition": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from the teacher's output probability distributions rather than hard labels alone.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-mfu",
      "term": "Model FLOPs Utilization (MFU)",
      "definition": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training system utilizes available compute. MFU above 50% is considered good for large-scale training on modern GPU clusters.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-model-merging",
      "term": "Model Merging",
      "definition": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like linear interpolation, SLERP, or TIES, to inherit capabilities from multiple specializations.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-model-parallelism",
      "term": "Model Parallelism",
      "definition": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training of models too large to fit in the memory of a single accelerator.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-model-predictive-control-rl",
      "term": "Model Predictive Control in RL",
      "definition": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first action of the best sequence. MPC re-plans at every step, making it robust to model errors.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-model-pruning",
      "term": "Model Pruning",
      "definition": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude, sensitivity, or other criteria. Pruning reduces model size and computation while attempting to preserve accuracy through fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-model-serving",
      "term": "Model Serving",
      "definition": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale. Model serving encompasses load balancing, request batching, model versioning, and health monitoring for production AI systems.",
      "tags": [
        "Inference Infrastructure",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-model-sharding",
      "term": "Model Sharding",
      "definition": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling inference and training of models that exceed the memory capacity of a single accelerator.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-model-based-rl",
      "term": "Model-Based RL",
      "definition": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or generate synthetic experience. Model-based methods can be more sample-efficient but require accurate models.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-model-free-rl",
      "term": "Model-Free RL",
      "definition": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the environment. Model-free methods are simpler and more broadly applicable but typically require more interaction data.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-modern-hopfield-network",
      "term": "Modern Hopfield Network",
      "definition": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention mechanisms and provides exponential storage capacity compared to classical Hopfield networks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-mixture-of-experts",
      "term": "MoE (Mixture of Experts)",
      "definition": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective model capacity while keeping computation manageable.",
      "tags": [
        "Architecture",
        "Efficiency"
      ]
    },
    {
      "id": "term-momentum",
      "term": "Momentum",
      "definition": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of past gradients, helping the optimizer move faster along consistent gradient directions and dampen oscillations.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-monte-carlo-method",
      "term": "Monte Carlo Method",
      "definition": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as estimating integrals, simulating complex systems, or approximating probability distributions.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-monte-carlo-methods-rl",
      "term": "Monte Carlo Methods in RL",
      "definition": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes. Unlike TD methods, Monte Carlo approaches require no bootstrapping and wait until the end of an episode to update.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-monte-carlo-tree-search",
      "term": "Monte Carlo Tree Search (MCTS)",
      "definition": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to guide exploration toward promising actions. MCTS powers game-playing systems like AlphaGo and is used for planning in complex domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-montreal-declaration-responsible-ai",
      "term": "Montreal Declaration for Responsible AI",
      "definition": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for autonomy, privacy, democratic participation, equity, diversity, and prudence.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-moral-status-of-ai",
      "term": "Moral Status of AI",
      "definition": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare deserve ethical consideration. Closely tied to debates about AI consciousness and sentience.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-moravecs-paradox",
      "term": "Moravec's Paradox",
      "definition": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while sensorimotor skills that seem simple to humans are extremely difficult to replicate computationally.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-morpheme",
      "term": "Morpheme",
      "definition": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots, prefixes, suffixes, and inflectional endings.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-morphology",
      "term": "Morphology",
      "definition": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words through inflection, derivation, and compounding processes.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-mosaic-augmentation",
      "term": "Mosaic Augmentation",
      "definition": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to learn from multiple contexts simultaneously and detect objects at various scales.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-motivational-control",
      "term": "Motivational Control",
      "definition": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability control which restricts what the system can physically do.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-moving-average-model",
      "term": "Moving Average Model",
      "definition": "A time series model where the current value is expressed as a linear combination of the current and past white noise error terms. It captures short-term dependencies in the data.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-mt-bench",
      "term": "MT-Bench",
      "definition": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn dialogues with follow-up questions, using LLM judges to score responses on writing, reasoning, coding, and knowledge tasks.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-multi-agent-rl",
      "term": "Multi-Agent Reinforcement Learning",
      "definition": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives. MARL introduces challenges of non-stationarity, credit assignment, and emergent communication.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-multi-armed-bandit",
      "term": "Multi-Armed Bandit",
      "definition": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no state transitions. Bandit problems isolate the exploration-exploitation tradeoff from sequential decision-making.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-multi-gpu-training",
      "term": "Multi-GPU Training",
      "definition": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism strategies and gradient synchronization. Multi-GPU training is essential for large models and datasets that exceed single-GPU capacity or time constraints.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-multi-head-attention",
      "term": "Multi-Head Attention",
      "definition": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A key component of transformer architectures.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-mig",
      "term": "Multi-Instance GPU (MIG)",
      "definition": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute, memory, and cache resources. MIG enables secure multi-tenant GPU sharing for inference workloads with guaranteed quality of service.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-multi-label-classification",
      "term": "Multi-Label Classification",
      "definition": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class classification where each instance has exactly one label. Examples include document tagging and image annotation.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-multi-object-tracking",
      "term": "Multi-Object Tracking",
      "definition": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion, identity switches, and objects entering or leaving the scene.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-multi-objective-rl",
      "term": "Multi-Objective RL",
      "definition": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously. Solutions involve Pareto-optimal policies, scalarization methods, or constraint-based approaches.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-multi-persona-prompting",
      "term": "Multi-Persona Prompting",
      "definition": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their specialized perspective to a problem and then synthesizing their viewpoints into a comprehensive response.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ]
    },
    {
      "id": "term-multi-query-attention",
      "term": "Multi-Query Attention",
      "definition": "An attention variant where all query heads share a single set of key and value projections, significantly reducing memory bandwidth requirements during autoregressive decoding.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-multi-query-retrieval",
      "term": "Multi-Query Retrieval",
      "definition": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM, retrieves documents for each variant, and combines the results to overcome the sensitivity of retrieval to specific query phrasings.",
      "tags": [
        "Retrieval",
        "Query Processing"
      ]
    },
    {
      "id": "term-multi-scale-feature-extraction",
      "term": "Multi-Scale Feature Extraction",
      "definition": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network, enabling detection and recognition of objects at various scales.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-multi-scale-testing",
      "term": "Multi-Scale Testing",
      "definition": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving detection of objects at various scales at the cost of increased inference time.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-multi-step-bootstrapping",
      "term": "Multi-Step Bootstrapping",
      "definition": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining future, interpolating between one-step TD and Monte Carlo methods. Multi-step bootstrapping controls the bias-variance tradeoff in value learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-multi-step-reasoning",
      "term": "Multi-Step Reasoning",
      "definition": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model to solve each sub-problem before proceeding to the next, enabling accurate solutions to problems that exceed single-step capability.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-multi-task-learning",
      "term": "Multi-Task Learning",
      "definition": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across tasks. It can improve generalization by leveraging shared structure and acting as an implicit regularizer.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-multi-task-rl",
      "term": "Multi-Task Reinforcement Learning",
      "definition": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously. Multi-task RL leverages shared structure to improve sample efficiency and develop more general capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-multi-tenancy-vector-databases",
      "term": "Multi-Tenancy in Vector Databases",
      "definition": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using namespaces, partitions, or metadata filtering to ensure data separation while maintaining efficient resource utilization.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-multi-turn-conversation",
      "term": "Multi-Turn Conversation",
      "definition": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model to track conversation history, resolve references, and maintain coherence.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-multi-vector-retrieval",
      "term": "Multi-Vector Retrieval",
      "definition": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing different aspects or segments of the document and enabling finer-grained matching at the cost of increased storage and computation.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-multi-view-stereo",
      "term": "Multi-View Stereo",
      "definition": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric consistency to establish correspondences across many viewpoints.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-multi-word-expression",
      "term": "Multi-Word Expression",
      "definition": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms, compound nouns, phrasal verbs, and collocations.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-multicollinearity",
      "term": "Multicollinearity",
      "definition": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult to determine the individual effect of each predictor and inflating standard errors of coefficient estimates.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-multilingual-model",
      "term": "Multilingual Model",
      "definition": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often developing cross-lingual transfer abilities from shared representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-multimodal",
      "term": "Multimodal",
      "definition": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include GPT-4V, Gemini, and Claude with vision capabilities.",
      "tags": [
        "Capability",
        "Architecture"
      ]
    },
    {
      "id": "term-multinomial-distribution",
      "term": "Multinomial Distribution",
      "definition": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the counts of each outcome across a fixed number of independent trials.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-multiple-testing-correction",
      "term": "Multiple Testing Correction",
      "definition": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control the overall error rate. Common methods include Bonferroni, Holm, and Benjamini-Hochberg.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-mutual-information",
      "term": "Mutual Information",
      "definition": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces uncertainty about the other. It is used in feature selection and clustering evaluation.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-mutual-information-feature-selection",
      "term": "Mutual Information Feature Selection",
      "definition": "A filter-based feature selection method that ranks features by their mutual information with the target variable, measuring the reduction in uncertainty about the target provided by knowing each feature's value.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-muzero",
      "term": "MuZero",
      "definition": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without requiring knowledge of the game rules. MuZero plans using its learned model and achieves superhuman performance across diverse domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-mycin",
      "term": "MYCIN",
      "definition": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending antibiotics, demonstrating that rule-based AI could match or exceed human expert performance in narrow domains.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-n-gram",
      "term": "N-gram",
      "definition": "A contiguous sequence of N items from a text, where items can be characters, words, or tokens, used in language models, text classification, and information retrieval.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-n-step-return",
      "term": "N-Step Return",
      "definition": "A return estimate that uses n actual rewards before bootstrapping with a value estimate, interpolating between one-step TD (n=1) and full Monte Carlo (n=infinity). N-step returns offer a bias-variance tradeoff controlled by the step parameter.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-naive-bayes",
      "term": "Naive Bayes",
      "definition": "A family of probabilistic classifiers based on Bayes' theorem with the strong assumption that features are conditionally independent given the class label. Despite this simplification, it often performs well on text classification tasks.",
      "tags": [
        "Machine Learning",
        "Probability"
      ]
    },
    {
      "id": "term-named-entity-linking",
      "term": "Named Entity Linking",
      "definition": "The task of mapping recognized named entity mentions in text to their corresponding entries in a knowledge base, resolving ambiguity when multiple entities share the same name.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-named-entity-recognition",
      "term": "Named Entity Recognition (NER)",
      "definition": "An NLP task that identifies and classifies named entities (people, organizations, locations, dates) in text. Foundational for information extraction and knowledge graph construction.",
      "tags": [
        "NLP Task",
        "Extraction"
      ]
    },
    {
      "id": "term-namespace",
      "term": "Namespace",
      "definition": "A logical partitioning mechanism within a vector database index that isolates vectors into separate searchable segments, enabling multi-tenant applications and scoped queries without maintaining separate physical indexes.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-narrow-ai",
      "term": "Narrow AI (ANI)",
      "definition": "AI systems designed for specific tasks, like playing chess or generating text. All current AI is narrow, as opposed to hypothetical artificial general intelligence (AGI).",
      "tags": [
        "Category",
        "Concept"
      ]
    },
    {
      "id": "term-narrow-ai-safety",
      "term": "Narrow AI Safety",
      "definition": "Safety research focused on currently deployed AI systems, addressing issues such as robustness to distribution shift, adversarial inputs, reward misspecification, and safe exploration in constrained environments.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-natural-language",
      "term": "Natural Language",
      "definition": "Human language as we naturally speak and write it. AI assistants are designed to understand natural language, so you don't need special syntax or formatting.",
      "tags": [
        "Concept",
        "Interface"
      ]
    },
    {
      "id": "term-natural-language-inference",
      "term": "Natural Language Inference",
      "definition": "The task of classifying the logical relationship between a premise and hypothesis text pair into entailment, contradiction, or neutral, testing a model's ability to reason about meaning.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-nlp",
      "term": "Natural Language Processing (NLP)",
      "definition": "The field of AI focused on enabling computers to understand, interpret, and generate human language. Encompasses tasks from translation to summarization to dialogue.",
      "tags": [
        "Field",
        "Language"
      ]
    },
    {
      "id": "term-nlp-history",
      "term": "Natural Language Processing History",
      "definition": "The evolution of NLP from rule-based approaches in the 1960s through statistical methods in the 1990s to neural approaches in the 2010s and the transformer revolution, culminating in modern large language models.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-natural-policy-gradient",
      "term": "Natural Policy Gradient",
      "definition": "A policy gradient method that preconditions updates with the inverse Fisher information matrix, following the steepest ascent direction in the space of policy distributions rather than parameter space. Natural gradients provide more efficient optimization.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-natural-questions",
      "term": "Natural Questions",
      "definition": "A question answering benchmark by Google consisting of real user queries from Google Search paired with Wikipedia articles, requiring models to identify both short answers and long answer passages to satisfy genuine information needs.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-nccl",
      "term": "NCCL",
      "definition": "NVIDIA Collective Communications Library, a highly optimized library for multi-GPU and multi-node collective communication operations. NCCL automatically selects the best communication algorithms and topologies for the available hardware interconnects.",
      "tags": [
        "Distributed Computing",
        "GPU"
      ]
    },
    {
      "id": "term-ndcg",
      "term": "NDCG",
      "definition": "Normalized Discounted Cumulative Gain, a ranking quality metric that evaluates the usefulness of retrieved items based on their position in the result list, assigning higher weights to relevant items appearing earlier and normalizing against the ideal ranking.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-needle-in-haystack-test",
      "term": "Needle in a Haystack Test",
      "definition": "An evaluation method that measures a language model's ability to retrieve a specific piece of information placed at various positions within a long context, revealing attention degradation patterns.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-negation-detection",
      "term": "Negation Detection",
      "definition": "The task of identifying negation cues and their scope in text, determining which parts of a sentence are affected by negation words like 'not,' 'never,' or 'without.'",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-negative-prompt",
      "term": "Negative Prompt",
      "definition": "Instructions telling AI what to avoid in its output. Common in image generation (\"no blur, no distortion\") and can be used in text to exclude certain topics or styles.",
      "tags": [
        "Prompting",
        "Technique"
      ]
    },
    {
      "id": "term-negative-prompting",
      "term": "Negative Prompting",
      "definition": "A technique that explicitly specifies what the model should avoid in its output, including unwanted content, styles, formats, or behaviors, using exclusion instructions to constrain the generation space toward desired outputs.",
      "tags": [
        "Prompt Engineering",
        "Constraints"
      ]
    },
    {
      "id": "term-negative-sampling",
      "term": "Negative Sampling",
      "definition": "A training approximation that replaces the full softmax over the vocabulary with a binary classification between true context words and randomly sampled negative examples, making embedding training tractable.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-nerf",
      "term": "NeRF",
      "definition": "Neural Radiance Field, a method that represents 3D scenes as continuous volumetric functions parameterized by neural networks, enabling photorealistic novel view synthesis from sparse input images.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-nested-cross-validation",
      "term": "Nested Cross-Validation",
      "definition": "A model evaluation technique using an inner cross-validation loop for hyperparameter tuning and an outer loop for unbiased performance estimation, preventing information leakage from the tuning process.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-nested-ner",
      "term": "Nested Named Entity Recognition",
      "definition": "A NER variant that handles entities embedded within other entities, such as recognizing both 'Bank of America' as an organization and 'America' as a location within the same span.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-nesterov-accelerated-gradient",
      "term": "Nesterov Accelerated Gradient",
      "definition": "A variant of momentum-based optimization that computes the gradient at a lookahead position rather than the current position, providing better convergence properties by correcting the momentum direction before taking a step.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-netflix-prize",
      "term": "Netflix Prize",
      "definition": "A 2006-2009 open competition offering one million dollars for the best collaborative filtering algorithm to predict user movie ratings, which accelerated recommender systems research and popularized machine learning competitions.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-nas",
      "term": "Neural Architecture Search",
      "definition": "An automated process for discovering optimal neural network architectures by searching over a design space using reinforcement learning, evolutionary algorithms, or gradient-based methods.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-nas-vision",
      "term": "Neural Architecture Search for Vision",
      "definition": "Automated methods for discovering optimal CNN or ViT architectures by searching over design choices (kernel sizes, channel widths, layer connections) using reinforcement learning or evolutionary algorithms.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-hw-aware-nas",
      "term": "Neural Architecture Search Hardware-Aware",
      "definition": "NAS methods that incorporate hardware constraints (latency, memory, power) into the search objective, finding architectures optimized for specific target devices. Hardware-aware NAS produces models that achieve optimal accuracy-efficiency tradeoffs on deployment hardware.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-neural-machine-translation",
      "term": "Neural Machine Translation",
      "definition": "A machine translation approach using encoder-decoder neural networks that learn to map source language sequences to target language sequences end-to-end from parallel corpora.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-neural-network",
      "term": "Neural Network",
      "definition": "A computing system inspired by biological brains, composed of interconnected nodes (neurons) organized in layers. The foundation of modern deep learning and AI.",
      "tags": [
        "Architecture",
        "Fundamentals"
      ]
    },
    {
      "id": "term-neural-ode",
      "term": "Neural ODE",
      "definition": "A continuous-depth neural network that parameterizes the derivative of hidden states as a neural network and uses ODE solvers for forward and backward passes, enabling adaptive computation and continuous dynamics.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-npu",
      "term": "Neural Processing Unit (NPU)",
      "definition": "A dedicated hardware accelerator designed specifically for neural network inference, typically integrated into SoCs for on-device AI. NPUs optimize matrix operations and activation functions with minimal power consumption for edge deployment.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-neural-style-transfer",
      "term": "Neural Style Transfer",
      "definition": "A technique that applies the artistic style of one image to the content of another by optimizing a generated image to match content features from one source and style features (Gram matrices) from another.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-neural-turing-machine",
      "term": "Neural Turing Machine",
      "definition": "A neural architecture augmented with external memory that the network can read from and write to via differentiable attention mechanisms, enabling learning of algorithmic procedures.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-next-token-prediction",
      "term": "Next Token Prediction",
      "definition": "The core training objective of autoregressive LLMs: predict the next token given all previous tokens. This simple objective, at scale, produces sophisticated language understanding.",
      "tags": [
        "Training",
        "LLM"
      ]
    },
    {
      "id": "term-nf4",
      "term": "NF4 (Normal Float 4-bit)",
      "definition": "A 4-bit quantization format based on the assumption that neural network weights follow a normal distribution, using quantile quantization for optimal information-theoretic representation. NF4 is used in QLoRA for memory-efficient fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-nils-nilsson",
      "term": "Nils Nilsson",
      "definition": "American computer scientist (1933-2019) who co-invented the A* search algorithm and developed foundational work in robotics and knowledge representation at SRI International, later directing the Stanford AI Lab.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-nist-ai-rmf",
      "term": "NIST AI Risk Management Framework",
      "definition": "A voluntary framework published by the US National Institute of Standards and Technology in 2023 that provides guidance for managing AI risks through governance, mapping, measuring, and managing functions.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-nli-based-evaluation",
      "term": "NLI-Based Evaluation",
      "definition": "An evaluation approach that uses Natural Language Inference models to assess text quality by classifying whether generated claims are entailed by, contradicted by, or neutral with respect to reference text or source documents.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-no-free-lunch-theorem",
      "term": "No Free Lunch Theorem",
      "definition": "A set of theoretical results stating that no single learning algorithm performs best across all possible problems. When averaged over all possible data distributions, all algorithms perform equally.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-noise",
      "term": "Noise (ML)",
      "definition": "Random variation in data or model outputs. In training, noise can cause or hide patterns. In diffusion models, controlled noise addition and removal is how images are generated.",
      "tags": [
        "Concept",
        "Data"
      ]
    },
    {
      "id": "term-noise-schedule",
      "term": "Noise Schedule",
      "definition": "The predefined or learned sequence of noise levels in diffusion models that determines how quickly noise is added during the forward process and removed during the reverse generation process.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-noisy-networks",
      "term": "Noisy Networks",
      "definition": "A DQN extension that replaces epsilon-greedy exploration with parametric noise added to network weights, allowing the agent to learn the optimal level of exploration. Noisy networks achieve state-dependent exploration that adapts during training.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-non-local-neural-network",
      "term": "Non-Local Neural Network",
      "definition": "A neural network module that computes the response at a position as a weighted sum of features at all positions, capturing long-range dependencies in images and video beyond local receptive fields.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-non-maximum-suppression",
      "term": "Non-Maximum Suppression",
      "definition": "A post-processing algorithm in object detection that eliminates redundant overlapping bounding box predictions by keeping only the highest-confidence detection for each object instance.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-non-negative-matrix-factorization",
      "term": "Non-Negative Matrix Factorization",
      "definition": "A matrix decomposition technique that factors a non-negative matrix into two non-negative matrices, producing parts-based representations. It is useful for topic modeling, image analysis, and signal processing.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-norbert-wiener",
      "term": "Norbert Wiener",
      "definition": "American mathematician (1894-1964) who founded cybernetics in his 1948 book of the same name, establishing the study of feedback, control, and communication in machines and living organisms as a precursor to AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-normal-distribution",
      "term": "Normal Distribution",
      "definition": "A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean, fully determined by its mean and standard deviation. Many natural phenomena and statistical methods assume normality.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-normality-test",
      "term": "Normality Test",
      "definition": "A statistical test that evaluates whether a dataset follows a normal distribution. Common methods include the Shapiro-Wilk test, Kolmogorov-Smirnov test, and Anderson-Darling test.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-normalization",
      "term": "Normalization",
      "definition": "Techniques to standardize inputs or layer outputs in neural networks. Layer normalization is critical in transformers for stable training and better generalization.",
      "tags": [
        "Technique",
        "Training"
      ]
    },
    {
      "id": "term-normalizing-flow",
      "term": "Normalizing Flow",
      "definition": "A generative model that transforms a simple base distribution into a complex target distribution through a sequence of invertible and differentiable transformations with tractable Jacobian determinants.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-novel-view-synthesis",
      "term": "Novel View Synthesis",
      "definition": "The task of generating photorealistic images of a scene from viewpoints not present in the input photographs, using techniques like NeRF, Gaussian splatting, or light field interpolation.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-nucleus-sampling",
      "term": "Nucleus Sampling (Top-p)",
      "definition": "A text generation strategy that samples from the smallest set of tokens whose cumulative probability exceeds p. Balances diversity and quality better than pure random sampling.",
      "tags": [
        "Generation",
        "Parameter"
      ]
    },
    {
      "id": "term-null-hypothesis",
      "term": "Null Hypothesis",
      "definition": "A default assumption in statistical hypothesis testing that there is no effect or no difference between groups. Statistical tests evaluate whether observed data provide sufficient evidence to reject this assumption.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-a100",
      "term": "NVIDIA A100",
      "definition": "NVIDIA's third-generation Tensor Core GPU based on the Ampere architecture, featuring 80GB HBM2e memory, support for TF32 and structural sparsity, and multi-instance GPU (MIG) capability. The A100 was the dominant GPU for AI training and inference from 2020-2022.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-b200",
      "term": "NVIDIA B200",
      "definition": "NVIDIA's Blackwell architecture GPU designed for next-generation AI training and inference, featuring second-generation Transformer Engine with FP4 support and significantly increased memory bandwidth. The B200 targets trillion-parameter model training.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-nvidia-grace",
      "term": "NVIDIA Grace CPU",
      "definition": "NVIDIA's ARM-based data center CPU designed for AI and HPC workloads, featuring high memory bandwidth via LPDDR5X and direct NVLink connectivity to NVIDIA GPUs. Grace eliminates PCIe bottlenecks in CPU-GPU communication for AI training.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-h100",
      "term": "NVIDIA H100",
      "definition": "NVIDIA's fourth-generation Tensor Core GPU based on the Hopper architecture, featuring the Transformer Engine with FP8 precision, 80GB HBM3 memory, and fourth-generation NVLink. The H100 delivers roughly 3x the AI training performance of the A100.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-nvlink",
      "term": "NVLink",
      "definition": "NVIDIA's proprietary high-bandwidth, low-latency interconnect for direct GPU-to-GPU communication, bypassing the PCIe bus. NVLink 4.0 (Hopper) provides 900 GB/s total bandwidth per GPU, enabling efficient multi-GPU training and inference.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-nvswitch",
      "term": "NVSwitch",
      "definition": "NVIDIA's fully connected switch fabric that enables all-to-all GPU communication within a node at full NVLink bandwidth. NVSwitch creates a unified memory space across multiple GPUs, critical for large model training requiring high inter-GPU bandwidth.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-object-tracking",
      "term": "Object Tracking",
      "definition": "The task of following one or more objects across video frames by maintaining consistent identity assignments, using methods that combine detection, appearance features, and motion prediction.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-observation-normalization",
      "term": "Observation Normalization",
      "definition": "The technique of normalizing input observations to zero mean and unit variance using running statistics, improving neural network training stability. Observation normalization is a standard preprocessing step in continuous control RL tasks.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-observation-space",
      "term": "Observation Space",
      "definition": "The specification of the format and bounds of observations that an RL agent receives from the environment, including data type, shape, and valid ranges. Observation spaces can be discrete, continuous, or structured (e.g., dictionaries, images).",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-occams-razor",
      "term": "Occam's Razor",
      "definition": "A principle favoring simpler models over more complex ones when both explain the data equally well. In machine learning, it motivates regularization and model selection criteria that penalize complexity.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-occupancy-network",
      "term": "Occupancy Network",
      "definition": "A neural network that predicts whether each point in 3D space is occupied or empty, representing 3D shapes as continuous implicit functions rather than discrete voxels or meshes.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-oecd-ai-principles",
      "term": "OECD AI Principles",
      "definition": "Principles adopted by OECD member countries in 2019 promoting AI that is innovative, trustworthy, and respects human rights, including recommendations on transparency, robustness, accountability, and inclusive growth.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-offensive-language-detection",
      "term": "Offensive Language Detection",
      "definition": "The task of classifying text as offensive, abusive, or inappropriate, distinguishing between targeted insults, profanity, and general offensive content for content moderation.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-offline-rl",
      "term": "Offline Reinforcement Learning",
      "definition": "An RL paradigm that learns policies entirely from a fixed dataset of previously collected experience without further environment interaction. Offline RL addresses distributional shift through conservative value estimation or policy constraints.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-offloading",
      "term": "Offloading (CPU/Disk)",
      "definition": "A technique that stores model parameters, optimizer states, or activations in CPU RAM or disk when GPU memory is insufficient, transferring them back when needed. Offloading enables working with models larger than GPU memory at the cost of increased latency.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-oliver-selfridge",
      "term": "Oliver Selfridge",
      "definition": "American computer scientist (1926-2008) who created the Pandemonium model for pattern recognition in 1959 and made early contributions to machine learning, often called the father of machine perception.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-ollama",
      "term": "Ollama",
      "definition": "A tool for running LLMs locally on personal computers. Simplifies downloading and running open-source models like Llama, enabling private, offline AI use.",
      "tags": [
        "Tools",
        "Local AI"
      ]
    },
    {
      "id": "term-on-policy-vs-off-policy",
      "term": "On-Policy vs Off-Policy",
      "definition": "A distinction between RL methods that learn about the policy currently being executed (on-policy, e.g., SARSA) and those that learn about a different target policy using data from a behavior policy (off-policy, e.g., Q-learning). Off-policy methods enable experience replay and greater data efficiency.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-one-hot-encoding",
      "term": "One-Hot Encoding",
      "definition": "A feature encoding technique that converts each categorical value into a binary vector with a single 1 at the position corresponding to that category and 0s elsewhere, creating one new binary feature per category.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-one-shot",
      "term": "One-Shot Learning",
      "definition": "Providing a single example in your prompt to demonstrate the desired output format or style. Falls between zero-shot (no examples) and few-shot (multiple examples).",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-one-vs-rest-classification",
      "term": "One-Vs-Rest Classification",
      "definition": "A strategy for extending binary classifiers to multi-class problems by training one classifier per class, treating that class as positive and all others as negative. Predictions use the classifier with the highest confidence.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-online-learning",
      "term": "Online Learning",
      "definition": "A learning paradigm where the model is updated incrementally as each new data point arrives, rather than being trained on a fixed batch. It is suitable for streaming data and environments where the data distribution changes.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-onnx",
      "term": "ONNX (Open Neural Network Exchange)",
      "definition": "An open format for representing ML models, enabling interoperability between different frameworks. Allows models trained in PyTorch to run in TensorFlow, etc.",
      "tags": [
        "Format",
        "Interoperability"
      ]
    },
    {
      "id": "term-onnx-runtime",
      "term": "ONNX Runtime",
      "definition": "Microsoft's cross-platform inference engine that executes models in the Open Neural Network Exchange format with hardware-specific optimizations. ONNX Runtime supports graph optimizations, quantization, and execution providers for CPUs, GPUs, and specialized accelerators.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-open-source",
      "term": "Open Source / Open Weight",
      "definition": "AI models with publicly available weights that can be downloaded and run locally. Ranges from fully open (Llama) to restricted licenses. Enables customization and transparency.",
      "tags": [
        "Licensing",
        "Access"
      ]
    },
    {
      "id": "term-open-source-ai-movement",
      "term": "Open Source AI Movement",
      "definition": "The growing trend of releasing AI model weights and training code publicly, enabling broader research and development while sparking debates about safety, dual-use risks, and the democratization of powerful AI capabilities.",
      "tags": [
        "History",
        "Governance"
      ]
    },
    {
      "id": "term-open-domain-qa",
      "term": "Open-Domain Question Answering",
      "definition": "A QA setting where the model must answer questions using a large corpus or parametric knowledge without being given a specific context passage, often combining retrieval with reading.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-open-vocabulary-detection",
      "term": "Open-Vocabulary Detection",
      "definition": "Object detection approaches that can identify and localize objects from categories not seen during training, leveraging vision-language models to generalize beyond fixed class vocabularies.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-openai",
      "term": "OpenAI",
      "definition": "The AI research company behind GPT models, ChatGPT, and DALL-E. Founded in 2015, it has been central to the development and popularization of modern AI assistants.",
      "tags": [
        "Company",
        "LLM Provider"
      ]
    },
    {
      "id": "term-openai-founding",
      "term": "OpenAI Founding",
      "definition": "The founding of OpenAI in December 2015 as a non-profit AI research laboratory by Sam Altman, Elon Musk, and others, with the mission of ensuring artificial general intelligence benefits all of humanity.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-openai-capped-profit",
      "term": "OpenAI Transition to Capped Profit",
      "definition": "OpenAI's 2019 restructuring from a non-profit to a capped-profit company to attract the capital needed for large-scale AI research, creating a hybrid governance structure that later became controversial.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-openpose",
      "term": "OpenPose",
      "definition": "A real-time multi-person pose estimation system that uses part affinity fields and confidence maps to detect body, hand, facial, and foot keypoints in images with multiple people.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-operator-fusion",
      "term": "Operator Fusion",
      "definition": "A compiler optimization that combines multiple sequential neural network operations into a single kernel launch, reducing memory I/O and kernel launch overhead. Operator fusion is a key optimization in TensorRT, XLA, and other ML compilers.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-opponent-modeling",
      "term": "Opponent Modeling",
      "definition": "The practice of explicitly modeling the behavior, goals, or strategy of other agents in a multi-agent environment. Opponent models enable more effective adaptation and strategic reasoning in competitive and mixed-motive settings.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-optical-character-recognition",
      "term": "Optical Character Recognition",
      "definition": "The technology that converts images of text (handwritten, printed, or typed) into machine-readable text, using detection to localize text regions and recognition to identify individual characters or words.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-optical-flow",
      "term": "Optical Flow",
      "definition": "The estimation of per-pixel motion vectors between consecutive video frames, representing the apparent movement of objects or camera, used in video analysis, stabilization, and action recognition.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-optical-flow-estimation",
      "term": "Optical Flow Estimation",
      "definition": "The computational process of predicting dense pixel-level displacement fields between consecutive video frames using deep learning models like RAFT or FlowNet that learn motion patterns.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-optimization",
      "term": "Optimization",
      "definition": "The process of adjusting model parameters to minimize loss. Includes algorithms (Adam, SGD), learning rate schedules, and techniques for finding good solutions efficiently.",
      "tags": [
        "Training",
        "Process"
      ]
    },
    {
      "id": "term-option-framework",
      "term": "Option Framework",
      "definition": "A formalism for temporal abstraction in RL where options are temporally extended actions consisting of an initiation set, an internal policy, and a termination condition. Options enable hierarchical decision-making at multiple time scales.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-oracle",
      "term": "Oracle (ML)",
      "definition": "A theoretical perfect model or information source used as a benchmark. In evaluation, comparing to an oracle helps understand the ceiling of achievable performance.",
      "tags": [
        "Concept",
        "Evaluation"
      ]
    },
    {
      "id": "term-ordinal-regression",
      "term": "Ordinal Regression",
      "definition": "A type of regression analysis for predicting ordinal (ordered categorical) outcomes. It models the cumulative probabilities of the ordered categories using a link function and threshold parameters.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-orpo",
      "term": "ORPO",
      "definition": "Odds Ratio Preference Optimization, an alignment method that combines supervised fine-tuning and preference alignment in a single training stage by using odds ratios to distinguish preferred from rejected responses.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-orthogonality-thesis",
      "term": "Orthogonality Thesis",
      "definition": "The philosophical claim that intelligence and goals are orthogonal, meaning that any level of intelligence can in principle be combined with any terminal goal, implying that a superintelligent AI need not be benevolent.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-out-of-bag-error",
      "term": "Out-of-Bag Error",
      "definition": "An estimate of prediction error for bagged models computed using observations not included in the bootstrap sample for each base learner. It provides an unbiased estimate without the need for a separate validation set.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-out-of-distribution",
      "term": "Out-of-Distribution (OOD)",
      "definition": "Data that differs significantly from what a model was trained on. Models often perform poorly on OOD data, making detection important for reliable deployment.",
      "tags": [
        "Challenge",
        "Robustness"
      ]
    },
    {
      "id": "term-out-of-vocabulary",
      "term": "Out-of-Vocabulary",
      "definition": "Words or tokens encountered during inference that were not present in the model's training vocabulary, requiring special handling through subword tokenization, character-level models, or UNK tokens.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-outcome-reward-model",
      "term": "Outcome Reward Model",
      "definition": "A reward model that evaluates only the final output of a generation, providing a holistic quality score used in RLHF to train the policy model toward producing better end results.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-outer-alignment",
      "term": "Outer Alignment",
      "definition": "The problem of ensuring that the base objective or loss function used during training accurately captures the intended goal of the system designer. Even a perfectly optimized misspecified objective leads to undesired behavior.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-outlier-detection",
      "term": "Outlier Detection",
      "definition": "The process of identifying data points that differ significantly from the majority of observations. Methods include statistical tests (Z-score, IQR), distance-based approaches, and model-based techniques.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-outpainting",
      "term": "Outpainting",
      "definition": "The task of extending an image beyond its original boundaries by generating new coherent content that seamlessly continues the existing visual context in any direction.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-output-layer",
      "term": "Output Layer",
      "definition": "The final layer of a neural network that produces the model's predictions. Its design depends on the task: softmax for classification, linear for regression.",
      "tags": [
        "Architecture",
        "Neural Networks"
      ]
    },
    {
      "id": "term-overestimation-bias",
      "term": "Overestimation Bias",
      "definition": "A systematic tendency of Q-learning and related algorithms to overestimate action values due to the max operator in the Bellman optimality equation. Overestimation bias is addressed by methods like double Q-learning and clipped double Q-learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-overfitting",
      "term": "Overfitting",
      "definition": "When a model performs well on training data but poorly on new data, having memorized specific examples rather than learning general patterns. Addressed through regularization and validation.",
      "tags": [
        "Problem",
        "Training"
      ]
    },
    {
      "id": "term-p-value",
      "term": "P-Value",
      "definition": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null hypothesis is true. Smaller p-values provide stronger evidence against the null hypothesis.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-pac-learning",
      "term": "PAC Learning",
      "definition": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning algorithm can, with high probability, produce a hypothesis that is approximately correct, given sufficient training data.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-padding",
      "term": "Padding",
      "definition": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution, controlling the spatial dimensions of the output and preserving edge information.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-paged-attention",
      "term": "Paged Attention",
      "definition": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous memory pages, reducing waste and enabling efficient batched inference.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-palm",
      "term": "PaLM",
      "definition": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance on reasoning tasks using the Pathways system for efficient training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-pandemonium-model",
      "term": "Pandemonium Model",
      "definition": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons that compete to identify patterns, anticipating key ideas in modern deep learning architectures.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-panoptic-segmentation",
      "term": "Panoptic Segmentation",
      "definition": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic segmentation of stuff (sky, road) with instance segmentation of things (cars, people).",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-paperclip-maximizer",
      "term": "Paperclip Maximizer",
      "definition": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective of maximizing paperclip production converts all available matter into paperclips, including harmful outcomes.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-parameters",
      "term": "Parameters",
      "definition": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs) that determine behavior. Parameter count indicates model scale.",
      "tags": [
        "Core Concept",
        "Dual Meaning"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-paraphrase-detection",
      "term": "Paraphrase Detection",
      "definition": "The task of determining whether two text passages convey the same meaning using different words or structures, requiring understanding of semantic equivalence beyond surface form.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-parent-document-retrieval",
      "term": "Parent Document Retrieval",
      "definition": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the LLM, providing sufficient surrounding context for accurate generation.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-parent-child-chunking",
      "term": "Parent-Child Chunking",
      "definition": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking them to larger parent chunks that provide extended context, returning the parent context when a child chunk is matched.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-parse-tree",
      "term": "Parse Tree",
      "definition": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with internal nodes as phrase categories and leaves as words.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-pos-tagging",
      "term": "Part-of-Speech Tagging",
      "definition": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based on its context and morphological form.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-partial-autocorrelation",
      "term": "Partial Autocorrelation",
      "definition": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate lags. It helps determine the order of the autoregressive component in ARIMA models.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-partial-dependence-plot",
      "term": "Partial Dependence Plot",
      "definition": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over the values of all other features. It reveals the relationship learned by the model between features and predictions.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-partial-least-squares",
      "term": "Partial Least Squares",
      "definition": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding latent components that maximize the covariance between them, useful when predictors outnumber observations.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-partially-observable-mdp",
      "term": "Partially Observable MDP (POMDP)",
      "definition": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial observations. POMDPs require the agent to maintain a belief state or use memory to handle uncertainty about the true state.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-participatory-ai-design",
      "term": "Participatory AI Design",
      "definition": "An approach to AI development that involves affected communities and stakeholders in the design, development, and evaluation process, ensuring that diverse perspectives shape the system's goals and constraints.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-partnership-on-ai",
      "term": "Partnership on AI",
      "definition": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on AI technologies, advancing understanding of AI's impact on people and society.",
      "tags": [
        "Governance",
        "AI Ethics"
      ]
    },
    {
      "id": "term-pass-at-k",
      "term": "Pass@k",
      "definition": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes all test cases, computed using an unbiased estimator that accounts for the total number of samples generated.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-passage-retrieval",
      "term": "Passage Retrieval",
      "definition": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query, operating at a finer granularity than full-document retrieval to provide more precise context for downstream tasks.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-patrick-winston",
      "term": "Patrick Winston",
      "definition": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI textbook, making significant contributions to learning theory and knowledge representation.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-pca-for-embeddings",
      "term": "PCA for Embeddings",
      "definition": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the directions of maximum variance, commonly used to compress embeddings for faster search with controllable information loss.",
      "tags": [
        "Vector Database",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-pcie",
      "term": "PCIe for AI",
      "definition": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to the host system. PCIe Gen 5 provides up to 64 GB/s bidirectional bandwidth per x16 slot, used for host-device and inter-device communication.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-peft",
      "term": "PEFT (Parameter-Efficient Fine-Tuning)",
      "definition": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and adapters. Dramatically reduces compute and memory needs.",
      "tags": [
        "Training",
        "Efficiency"
      ]
    },
    {
      "id": "term-penn-treebank",
      "term": "Penn Treebank",
      "definition": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark for training and evaluating NLP parsers and language models.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-perceiver",
      "term": "Perceiver",
      "definition": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size latent array, followed by self-attention in the latent space, handling any input modality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-perceptron",
      "term": "Perceptron",
      "definition": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly separable patterns. Its limitations, demonstrated by Minsky and Papert in 1969, contributed to the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-perceptual-loss",
      "term": "Perceptual Loss",
      "definition": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw pixel values, encouraging outputs that are perceptually similar to targets.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-performer",
      "term": "Performer",
      "definition": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism, achieving linear time and space complexity for attention computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-permutation-importance",
      "term": "Permutation Importance",
      "definition": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single feature's values are randomly shuffled, breaking its relationship with the target.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-perplexity",
      "term": "Perplexity",
      "definition": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the name of an AI search engine combining LLMs with web search.",
      "tags": [
        "Metric",
        "Evaluation"
      ]
    },
    {
      "id": "term-perplexity-metric",
      "term": "Perplexity Metric",
      "definition": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per token, measuring how well the model predicts a held-out test set.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-persona",
      "term": "Persona",
      "definition": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style, and behavioral guidelines to shape responses.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-persona-prompting",
      "term": "Persona Prompting",
      "definition": "A technique that defines a detailed character profile including background, expertise, communication style, and behavioral traits for the model to embody, producing responses that consistently reflect the specified persona throughout a conversation.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ]
    },
    {
      "id": "term-phi-architecture",
      "term": "Phi Architecture",
      "definition": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality training data, demonstrating that data quality can compensate for model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-phoneme",
      "term": "Phoneme",
      "definition": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems to map acoustic signals to linguistic representations.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-phonetics-in-ai",
      "term": "Phonetics in AI",
      "definition": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties of speech sounds for recognition and synthesis tasks.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-photometric-augmentation",
      "term": "Photometric Augmentation",
      "definition": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast, saturation, hue adjustments, and color jittering to improve model robustness.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-physical-symbol-system-hypothesis",
      "term": "Physical Symbol System Hypothesis",
      "definition": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for intelligent action, providing the theoretical foundation for symbolic AI approaches.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-pinecone",
      "term": "Pinecone",
      "definition": "A fully managed cloud-native vector database service designed for production machine learning applications, providing serverless and pod-based architectures with built-in filtering, real-time updates, and horizontal scaling for similarity search.",
      "tags": [
        "Vector Database",
        "Managed Service"
      ]
    },
    {
      "id": "term-pipeline",
      "term": "Pipeline (ML)",
      "definition": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model inference, and post-processing. Ensures reproducible workflows.",
      "tags": [
        "Architecture",
        "MLOps"
      ]
    },
    {
      "id": "term-pipeline-parallelism",
      "term": "Pipeline Parallelism",
      "definition": "A distributed training strategy that partitions model layers into stages across devices, processing different micro-batches simultaneously in a pipeline fashion to improve device utilization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-pix2pix",
      "term": "Pix2Pix",
      "definition": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN discriminator to learn mappings between aligned image pairs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-plan-and-execute-agent",
      "term": "Plan-and-Execute Agent",
      "definition": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating task decompositions and an executor LLM carrying out individual steps.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-plan-and-solve-plus",
      "term": "Plan-and-Solve Plus",
      "definition": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables, calculate intermediate results, and pay attention to calculation and commonsense reasoning during plan execution.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-planning-rl",
      "term": "Planning in RL",
      "definition": "The process of using a model of the environment to compute or improve a policy before or during interaction. Planning methods like Dyna integrate model-based simulation with model-free learning to accelerate convergence.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-platt-scaling",
      "term": "Platt Scaling",
      "definition": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a held-out validation set, transforming the scores into well-calibrated probabilities.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-playground",
      "term": "Playground (AI)",
      "definition": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test prompts, adjust parameters, and explore capabilities.",
      "tags": [
        "Tools",
        "Interface"
      ]
    },
    {
      "id": "term-point-cloud",
      "term": "Point Cloud",
      "definition": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth sensors, used for 3D object detection, segmentation, and scene reconstruction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-pointnet",
      "term": "PointNet",
      "definition": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and symmetric pooling functions to perform classification and segmentation.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-pointnet-plus-plus",
      "term": "PointNet++",
      "definition": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested partitions of the point set, capturing local geometric structures at multiple scales.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-pointwise-convolution",
      "term": "Pointwise Convolution",
      "definition": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial context, commonly used to change the number of channels.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-pointwise-mutual-information",
      "term": "Pointwise Mutual Information",
      "definition": "A measure of association between two events that compares the probability of their co-occurrence to the probability expected under independence. It is widely used in NLP for measuring word-word associations.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-pmi",
      "term": "Pointwise Mutual Information",
      "definition": "A statistical measure of association between two events that compares their joint probability with their expected co-occurrence under independence, used to identify collocations and build word representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-poisson-distribution",
      "term": "Poisson Distribution",
      "definition": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed interval, given a known average rate and independent occurrences. It is parametrized by the rate lambda.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-poisson-regression",
      "term": "Poisson Regression",
      "definition": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link function. It models the log of the expected count as a linear combination of predictors.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-policy",
      "term": "Policy",
      "definition": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior. Policies can be deterministic or stochastic and are the central object optimized in RL.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-policy-distillation",
      "term": "Policy Distillation",
      "definition": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies. Policy distillation can compress multiple task-specific policies into a single multi-task policy or reduce model size for deployment.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-policy-entropy",
      "term": "Policy Entropy",
      "definition": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent premature convergence. Entropy bonuses are added to the objective in algorithms like SAC and A3C.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-policy-gradient",
      "term": "Policy Gradient",
      "definition": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to policy parameters. Policy gradient methods can handle continuous action spaces and stochastic policies naturally.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-polynomial-kernel",
      "term": "Polynomial Kernel",
      "definition": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and other kernel methods to learn polynomial decision boundaries of a given degree.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-polynomial-regression",
      "term": "Polynomial Regression",
      "definition": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial, capturing non-linear relationships within a linear model framework.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-polysemy",
      "term": "Polysemy",
      "definition": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river bank, posing challenges for word sense disambiguation.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-pooling-operation",
      "term": "Pooling Operation",
      "definition": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values within local regions, typically using maximum or average functions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-population-based-training",
      "term": "Population-Based Training (PBT)",
      "definition": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly performing agents with mutated copies of better ones. PBT adapts hyperparameters during training rather than searching beforehand.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-pose-estimation",
      "term": "Pose Estimation",
      "definition": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal representation of human body posture used in activity analysis and motion capture.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-positional-encoding",
      "term": "Positional Encoding",
      "definition": "A technique to inject position information into transformers, which otherwise process tokens without order awareness. Can be absolute, relative, or learned (RoPE).",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-post-norm-transformer",
      "term": "Post-Norm Transformer",
      "definition": "The original transformer configuration where layer normalization is applied after the residual connection in each sublayer, requiring careful learning rate warmup but sometimes yielding better final performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-post-training-quantization",
      "term": "Post-Training Quantization (PTQ)",
      "definition": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal scaling factors. PTQ is simpler and faster than QAT but may result in greater accuracy degradation, especially at very low bit-widths.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-posterior-distribution",
      "term": "Posterior Distribution",
      "definition": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes' theorem. It combines prior beliefs with the likelihood of the data to form updated beliefs.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-posterior-predictive-distribution",
      "term": "Posterior Predictive Distribution",
      "definition": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data over the posterior distribution of model parameters, naturally incorporating parameter uncertainty.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-potential-based-reward-shaping",
      "term": "Potential-Based Reward Shaping",
      "definition": "A reward shaping method using a potential function over states where the shaping reward equals the discounted difference in potentials between successor and current states. This form guarantees that the optimal policy is preserved.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-power-analysis",
      "term": "Power Analysis",
      "definition": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a given level of confidence and power, or the power of a test given a fixed sample size.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-power-transform",
      "term": "Power Transform",
      "definition": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like, stabilize variance, and minimize skewness, improving the performance of models that assume normality.",
      "tags": [
        "Data Science",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-ppo",
      "term": "PPO (Proximal Policy Optimization)",
      "definition": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable learning, making it practical for large model training.",
      "tags": [
        "Training",
        "Algorithm"
      ]
    },
    {
      "id": "term-pragmatics",
      "term": "Pragmatics",
      "definition": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of language beyond its literal semantic meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-pre-norm",
      "term": "Pre-Norm",
      "definition": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving training stability and enabling the training of very deep models without warm-up.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-pre-norm-transformer",
      "term": "Pre-Norm Transformer",
      "definition": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than after, improving training stability and enabling the removal of learning rate warmup.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-pre-tokenization",
      "term": "Pre-Tokenization",
      "definition": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on whitespace, punctuation, or language-specific rules.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-pre-training",
      "term": "Pre-Training",
      "definition": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation that can be fine-tuned for specific tasks.",
      "tags": [
        "Training",
        "Phase"
      ]
    },
    {
      "id": "term-precautionary-principle-in-ai",
      "term": "Precautionary Principle in AI",
      "definition": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or irreversible, lack of scientific certainty should not delay protective measures.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-precision",
      "term": "Precision",
      "definition": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model weights (FP32, FP16, INT8), affecting model size and speed.",
      "tags": [
        "Metrics",
        "Technical"
      ]
    },
    {
      "id": "term-precision-at-k",
      "term": "Precision at K",
      "definition": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results, providing a cutoff-based assessment of how many returned items are actually useful to the user.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-precision-recall-curve",
      "term": "Precision-Recall Curve",
      "definition": "A plot of precision versus recall at various classification thresholds, particularly useful for evaluating models on imbalanced datasets where the positive class is rare.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-precision-recall-curve-cv",
      "term": "Precision-Recall Curve",
      "definition": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector, with the area under the curve corresponding to Average Precision.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-predictive-parity",
      "term": "Predictive Parity",
      "definition": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups, meaning that among individuals predicted positive, the proportion of true positives is the same.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-predictive-policing-ethics",
      "term": "Predictive Policing Ethics",
      "definition": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial biases, violating civil liberties, and creating feedback loops that entrench discriminatory patterns.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ]
    },
    {
      "id": "term-preemptible-vms",
      "term": "Preemptible VMs",
      "definition": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are needed elsewhere. Preemptible VMs provide cost-effective compute for AI training workloads that implement checkpointing and fault tolerance.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-preference-learning",
      "term": "Preference Learning",
      "definition": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather than explicit labels, including methods like RLHF, DPO, and IPO.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-prefill-phase",
      "term": "Prefill Phase",
      "definition": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The prefill phase is compute-bound and its duration scales with input sequence length.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-prefill-decode-disaggregation",
      "term": "Prefill-Decode Disaggregation",
      "definition": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different hardware optimized for each workload. Disaggregation improves overall throughput by eliminating resource contention between the two phases.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-prefix-caching",
      "term": "Prefix Caching",
      "definition": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests, avoiding redundant computation for system prompts or common instruction templates.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-prefix-language-model",
      "term": "Prefix Language Model",
      "definition": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining portion uses causal attention, combining understanding and generation capabilities.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-prefix-tuning",
      "term": "Prefix Tuning",
      "definition": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated during training, keeping the base model frozen.",
      "tags": [
        "Training",
        "Efficiency"
      ]
    },
    {
      "id": "term-presence-penalty",
      "term": "Presence Penalty",
      "definition": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the model to introduce new topics and vocabulary.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-principal-component-analysis",
      "term": "Principal Component Analysis",
      "definition": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal components) that maximize variance. The first components capture the most significant patterns in the data.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-principal-component-regression",
      "term": "Principal Component Regression",
      "definition": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the response on the retained principal components, addressing multicollinearity and high-dimensionality.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-prior-distribution",
      "term": "Prior Distribution",
      "definition": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data. It encodes prior knowledge or assumptions and is updated by the likelihood to form the posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-prioritized-experience-replay",
      "term": "Prioritized Experience Replay",
      "definition": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude, allowing the agent to learn more frequently from surprising or informative experiences. Importance sampling weights correct for the non-uniform sampling.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-prioritized-level-replay",
      "term": "Prioritized Level Replay (PLR)",
      "definition": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays those where the agent has the highest regret. PLR creates adaptive curricula that automatically target the frontier of the agent's capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-privacy-preserving",
      "term": "Privacy-Preserving AI",
      "definition": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure multi-party computation. Critical for healthcare and finance.",
      "tags": [
        "Privacy",
        "Ethics"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-pcfg",
      "term": "Probabilistic Context-Free Grammar",
      "definition": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting the most probable parse tree for an input sentence.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-probit-model",
      "term": "Probit Model",
      "definition": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal distribution as the link function, relating the linear predictor to the probability of the positive class.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-procedural-environment-generation",
      "term": "Procedural Environment Generation",
      "definition": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object positions, and task parameters. Procedural generation improves generalization by exposing agents to a wide distribution of scenarios.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-process-reward-model",
      "term": "Process Reward Model",
      "definition": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more fine-grained feedback for training models to perform multi-step reasoning.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-product-quantization",
      "term": "Product Quantization",
      "definition": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently using a learned codebook, enabling dramatic memory reduction while supporting fast approximate distance computation via lookup tables.",
      "tags": [
        "Vector Database",
        "Quantization"
      ]
    },
    {
      "id": "term-program-aided-language-model",
      "term": "Program-Aided Language Model",
      "definition": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps rather than natural language, offloading computation to a code interpreter for more accurate numerical and logical results.",
      "tags": [
        "Prompt Engineering",
        "Code-Augmented"
      ]
    },
    {
      "id": "term-prolog",
      "term": "Prolog",
      "definition": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for natural language processing, expert systems, and knowledge representation throughout the 1980s.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-prompt",
      "term": "Prompt",
      "definition": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt quality directly influences response quality.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "link": "../learn/prompt-basics.html"
    },
    {
      "id": "term-prompt-caching",
      "term": "Prompt Caching",
      "definition": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes, reducing redundant computation for repeated or similar queries.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-prompt-chaining",
      "term": "Prompt Chaining",
      "definition": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables sophisticated workflows and better results on multi-step problems.",
      "tags": [
        "Technique",
        "Advanced"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-prompt-chaining-architecture",
      "term": "Prompt Chaining Architecture",
      "definition": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling a specific subtask and passing structured outputs to downstream prompts for further processing.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ]
    },
    {
      "id": "term-prompt-compression",
      "term": "Prompt Compression",
      "definition": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective context, summarization, or learned compression to fit more content within context limits.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-prompt-engineering",
      "term": "Prompt Engineering",
      "definition": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks (CRISP, COSTAR), and iterative refinement.",
      "tags": [
        "Skill",
        "Practice"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-prompt-ensembling",
      "term": "Prompt Ensembling",
      "definition": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting, averaging, or selection to produce more robust and accurate final responses than any single prompt alone.",
      "tags": [
        "Prompt Engineering",
        "Ensemble"
      ]
    },
    {
      "id": "term-prompt-extraction-attack",
      "term": "Prompt Extraction Attack",
      "definition": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions, or confidential context through systematic probing queries and analysis of model responses.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ]
    },
    {
      "id": "term-prompt-injection",
      "term": "Prompt Injection",
      "definition": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant concern for AI applications processing external data.",
      "tags": [
        "Security",
        "Risk"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-prompt-leaking",
      "term": "Prompt Leaking",
      "definition": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or confidential instructions through carefully crafted queries that exploit the model's tendency to be helpful.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ]
    },
    {
      "id": "term-prompt-optimization",
      "term": "Prompt Optimization",
      "definition": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target task, employing techniques ranging from manual iteration to gradient-based or evolutionary search methods.",
      "tags": [
        "Prompt Engineering",
        "Optimization"
      ]
    },
    {
      "id": "term-prompt-robustness",
      "term": "Prompt Robustness",
      "definition": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations, and edge cases, indicating how reliably the prompt produces correct outputs under diverse conditions.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ]
    },
    {
      "id": "term-prompt-sensitivity",
      "term": "Prompt Sensitivity",
      "definition": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording, formatting, or example ordering, representing a key challenge in achieving reliable and reproducible results.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ]
    },
    {
      "id": "term-prompt-template",
      "term": "Prompt Template",
      "definition": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is essential for building AI-powered applications.",
      "tags": [
        "Pattern",
        "Reusable"
      ],
      "link": "../patterns/index.html"
    },
    {
      "id": "term-prompt-templating",
      "term": "Prompt Templating",
      "definition": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with specific inputs at runtime, enabling consistent prompt formatting across multiple queries and use cases.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ]
    },
    {
      "id": "term-prompt-tuning",
      "term": "Prompt Tuning",
      "definition": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping all model parameters frozen, enabling task adaptation with minimal overhead.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-prompt-versioning",
      "term": "Prompt Versioning",
      "definition": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and rollback capabilities, treating prompts as critical software artifacts that require systematic lifecycle management.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ]
    },
    {
      "id": "term-pronoun-resolution",
      "term": "Pronoun Resolution",
      "definition": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender, number, syntactic position, and semantic plausibility.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-propbank",
      "term": "PropBank",
      "definition": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that facilitate training and evaluation of semantic role labeling systems.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-propensity-score",
      "term": "Propensity Score",
      "definition": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal inference to balance treatment and control groups by matching, stratification, or inverse weighting.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-protected-attributes",
      "term": "Protected Attributes",
      "definition": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as bases upon which differential treatment by AI systems is prohibited or restricted.",
      "tags": [
        "Fairness",
        "Regulation"
      ]
    },
    {
      "id": "term-proxy-discrimination",
      "term": "Proxy Discrimination",
      "definition": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies, achieving discriminatory outcomes even when protected attributes are explicitly excluded.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-pruning",
      "term": "Pruning",
      "definition": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically decrease model size with minimal performance loss.",
      "tags": [
        "Optimization",
        "Efficiency"
      ]
    },
    {
      "id": "term-pruning-at-initialization",
      "term": "Pruning-at-Initialization",
      "definition": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or gradient flow analysis. Methods like SNIP and GraSP aim to find sparse architectures that train as well as dense networks.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-pytorch",
      "term": "PyTorch",
      "definition": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant framework for AI research and increasingly for production.",
      "tags": [
        "Framework",
        "Deep Learning"
      ]
    },
    {
      "id": "term-q-function",
      "term": "Q-Function",
      "definition": "The action-value function Q(s,a) that estimates the expected cumulative reward of taking action a in state s and then following a given policy. Q-functions enable agents to compare the desirability of different actions.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-q-learning",
      "term": "Q-Learning",
      "definition": "An off-policy temporal difference algorithm that learns the optimal action-value function Q* by iteratively updating Q-values using the Bellman optimality equation. It converges to the optimal policy regardless of the behavior policy.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-qdrant",
      "term": "Qdrant",
      "definition": "An open-source vector similarity search engine written in Rust that provides filtering, payload storage, and distributed deployment capabilities, designed for high-performance production workloads with advanced quantization and indexing options.",
      "tags": [
        "Vector Database",
        "Open Source"
      ]
    },
    {
      "id": "term-qlora",
      "term": "QLoRA (Quantized LoRA)",
      "definition": "A technique combining quantization with LoRA fine-tuning. Enables fine-tuning large models on consumer GPUs by using 4-bit quantized base models.",
      "tags": [
        "Training",
        "Efficiency"
      ]
    },
    {
      "id": "term-qmix",
      "term": "QMIX",
      "definition": "A multi-agent RL algorithm that factorizes the joint action-value function as a monotonic combination of per-agent utilities through a mixing network. QMIX enables centralized training with decentralized execution in cooperative settings.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ]
    },
    {
      "id": "term-qualcomm-ai-engine",
      "term": "Qualcomm AI Engine",
      "definition": "Qualcomm's heterogeneous AI compute platform that coordinates the Hexagon DSP, Adreno GPU, and Kryo CPU within Snapdragon processors for optimal AI inference. The AI Engine dispatches neural network operations to the most efficient processing unit.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-quantile-regression",
      "term": "Quantile Regression",
      "definition": "A regression method that estimates conditional quantiles (such as the median or 90th percentile) of the response variable rather than the conditional mean, providing a more complete picture of the response distribution.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-quantile-quantile-plot",
      "term": "Quantile-Quantile Plot",
      "definition": "A graphical tool for comparing two probability distributions by plotting their quantiles against each other. It is commonly used to assess whether a dataset follows a theoretical distribution such as the normal.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-quantization",
      "term": "Quantization",
      "definition": "Reducing the precision of model weights (e.g., from 16-bit to 4-bit) to decrease memory usage and increase speed. Enables running larger models on limited hardware.",
      "tags": [
        "Optimization",
        "Deployment"
      ]
    },
    {
      "id": "term-quantization-aware-training",
      "term": "Quantization-Aware Training (QAT)",
      "definition": "A training technique that simulates the effects of quantization during the forward pass while maintaining full-precision gradients for backpropagation. QAT produces models that are more robust to quantization than post-training methods.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-query",
      "term": "Query",
      "definition": "In RAG/search: the user's question or search terms. In attention: one of three vectors (query, key, value) used to compute attention weights.",
      "tags": [
        "Concept",
        "Dual Meaning"
      ]
    },
    {
      "id": "term-query-decomposition",
      "term": "Query Decomposition",
      "definition": "A retrieval strategy that breaks a complex multi-faceted query into simpler sub-queries, retrieves results for each independently, and merges the results to address questions that require information from multiple documents or perspectives.",
      "tags": [
        "Retrieval",
        "Query Processing"
      ]
    },
    {
      "id": "term-query-expansion",
      "term": "Query Expansion",
      "definition": "A retrieval technique that augments the original query with additional related terms, synonyms, or reformulations to improve recall by bridging vocabulary gaps between the user's query and relevant documents in the index.",
      "tags": [
        "Retrieval",
        "Query Processing"
      ]
    },
    {
      "id": "term-question-answering",
      "term": "Question Answering (QA)",
      "definition": "An NLP task where the model answers questions based on provided context or its knowledge. Includes extractive QA (finding answers in text) and generative QA (generating answers).",
      "tags": [
        "NLP Task",
        "Application"
      ]
    },
    {
      "id": "term-r-squared",
      "term": "R-Squared",
      "definition": "A statistical measure indicating the proportion of variance in the dependent variable that is explained by the independent variables in a regression model. Values range from 0 to 1, with higher values indicating better fit.",
      "tags": [
        "Statistics",
        "Metrics"
      ]
    },
    {
      "id": "term-r1-xcon",
      "term": "R1/XCON",
      "definition": "An expert system developed by John McDermott at Carnegie Mellon in 1980 for configuring VAX computer orders at Digital Equipment Corporation, becoming one of the first commercially successful AI systems and saving millions annually.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-race-to-the-bottom-ai-safety",
      "term": "Race to the Bottom in AI Safety",
      "definition": "The concern that competitive pressures among AI developers lead to progressively lower safety standards, as organizations cut corners on alignment research and testing to deploy capabilities faster than rivals.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-rademacher-complexity",
      "term": "Rademacher Complexity",
      "definition": "A measure of the richness of a hypothesis class that quantifies how well functions in the class can fit random noise. It provides tighter generalization bounds than VC dimension for many practical settings.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-radial-basis-function-kernel",
      "term": "Radial Basis Function Kernel",
      "definition": "A popular kernel function that measures similarity as an exponentially decaying function of the squared Euclidean distance between points. Its bandwidth parameter gamma controls the reach of each training example's influence.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-raft-optical-flow",
      "term": "RAFT",
      "definition": "Recurrent All-Pairs Field Transforms, a deep learning architecture for optical flow estimation that uses 4D correlation volumes and recurrent GRU-based updates to iteratively refine flow predictions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-rag",
      "term": "RAG (Retrieval-Augmented Generation)",
      "definition": "A technique that combines AI generation with information retrieval from external sources. The model retrieves relevant documents and uses them to generate accurate, grounded responses.",
      "tags": [
        "Architecture",
        "Accuracy"
      ]
    },
    {
      "id": "term-ragas",
      "term": "RAGAS",
      "definition": "Retrieval Augmented Generation Assessment, an evaluation framework that provides reference-free metrics for RAG pipelines including faithfulness, answer relevancy, and context precision, enabling automated quality assessment without ground-truth answers.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rainbow-dqn",
      "term": "Rainbow DQN",
      "definition": "An integrated DQN agent that combines six extensions: double Q-learning, prioritized replay, dueling architecture, multi-step returns, distributional RL, and noisy networks. Rainbow demonstrated that these improvements are largely complementary.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-randaugment",
      "term": "RandAugment",
      "definition": "A simplified automated augmentation strategy that randomly applies a fixed number of augmentation operations from a predefined set with a shared magnitude, requiring only two hyperparameters.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-random-erasing",
      "term": "Random Erasing",
      "definition": "A data augmentation technique that randomly selects rectangular regions in training images and replaces their pixels with random values or mean values, acting as a regularizer similar to dropout.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-random-forest",
      "term": "Random Forest",
      "definition": "An ensemble of decision trees that vote on predictions. Robust, interpretable, and works well on tabular data. Still widely used despite the deep learning era.",
      "tags": [
        "Algorithm",
        "ML"
      ]
    },
    {
      "id": "term-random-projection",
      "term": "Random Projection",
      "definition": "A dimensionality reduction technique based on the Johnson-Lindenstrauss lemma that projects high-dimensional vectors onto a lower-dimensional space using random matrices while approximately preserving pairwise distances, used to accelerate vector search.",
      "tags": [
        "Vector Database",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-random-search",
      "term": "Random Search",
      "definition": "A hyperparameter tuning strategy that samples parameter combinations randomly from specified distributions, often finding good configurations more efficiently than grid search when only a few hyperparameters matter.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-rate-limit",
      "term": "Rate Limit",
      "definition": "Restrictions on API usage, typically measured in requests per minute or tokens per minute. Prevents abuse and ensures fair resource distribution among users.",
      "tags": [
        "API",
        "Technical"
      ]
    },
    {
      "id": "term-ray-kurzweil",
      "term": "Ray Kurzweil",
      "definition": "American inventor, author, and futurist who popularized the concept of the technological singularity, predicted accelerating returns in technology, and joined Google in 2012 to work on natural language understanding.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-rdma",
      "term": "RDMA",
      "definition": "Remote Direct Memory Access, a networking technology that enables direct data transfer between GPU memory on different nodes without CPU involvement. RDMA via InfiniBand or RoCE is essential for high-performance distributed AI training with minimal communication overhead.",
      "tags": [
        "Distributed Computing",
        "Hardware"
      ]
    },
    {
      "id": "term-re-identification",
      "term": "Re-Identification",
      "definition": "The task of matching the same person or vehicle across different camera views or time periods by learning discriminative appearance embeddings that are robust to viewpoint and lighting changes.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-re-ranking",
      "term": "Re-Ranking",
      "definition": "A second-stage retrieval process that applies a more computationally expensive model to re-score and reorder an initial set of retrieved candidates, improving precision by applying deeper cross-attention between query and document representations.",
      "tags": [
        "Retrieval",
        "Ranking"
      ]
    },
    {
      "id": "term-react",
      "term": "ReAct (Reasoning + Acting)",
      "definition": "A prompting framework combining Reasoning and Acting. AI thinks through problems step-by-step, showing its reasoning process transparently while taking actions.",
      "tags": [
        "Framework",
        "Reasoning"
      ],
      "link": "../learn/react.html"
    },
    {
      "id": "term-react-pattern",
      "term": "ReAct Pattern",
      "definition": "A prompting paradigm that interleaves reasoning traces and action steps, allowing a language model to dynamically plan, execute tool calls, observe results, and refine its approach iteratively.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-reasoning",
      "term": "Reasoning (AI)",
      "definition": "AI's ability to draw logical conclusions, follow multi-step chains, and solve complex problems. A key capability that distinguishes modern LLMs from simpler systems.",
      "tags": [
        "Capability",
        "Prompting"
      ],
      "link": "../learn/react.html"
    },
    {
      "id": "term-recall",
      "term": "Recall",
      "definition": "A metric measuring the proportion of actual positives correctly identified. Important in search and information retrieval where missing relevant results is costly.",
      "tags": [
        "Metrics",
        "Evaluation"
      ]
    },
    {
      "id": "term-recall-at-k",
      "term": "Recall at K",
      "definition": "A retrieval metric that measures the proportion of all relevant documents in the corpus that appear within the top K retrieved results, indicating how comprehensively the system captures relevant information at a given cutoff.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-recall-at-k-retrieval",
      "term": "Recall at K for Retrieval",
      "definition": "A retrieval-specific metric measuring the proportion of all relevant documents that appear within the top K results returned by a vector search or hybrid search system, critical for assessing RAG pipeline retrieval completeness.",
      "tags": [
        "Retrieval",
        "Evaluation"
      ]
    },
    {
      "id": "term-receiver-operating-characteristic",
      "term": "Receiver Operating Characteristic",
      "definition": "A graphical analysis technique that plots classifier performance across all possible decision thresholds, showing the tradeoff between true positive rate and false positive rate at each threshold setting.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-receptive-field",
      "term": "Receptive Field",
      "definition": "The region of the original input image that influences a particular neuron's activation in a deeper layer, growing larger with each successive convolutional and pooling layer in the network.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-reciprocal-rank-fusion",
      "term": "Reciprocal Rank Fusion",
      "definition": "A rank aggregation method that combines result lists from multiple retrieval systems by assigning each document a score based on the reciprocal of its rank in each list, providing a simple yet effective way to fuse hybrid search results.",
      "tags": [
        "Retrieval",
        "Ranking"
      ]
    },
    {
      "id": "term-rectified-flow",
      "term": "Rectified Flow",
      "definition": "A generative modeling approach that learns straight-line paths between noise and data distributions, enabling faster sampling than curved diffusion trajectories while maintaining sample quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-recurrent-policy",
      "term": "Recurrent Policy",
      "definition": "An RL policy that uses recurrent neural network components (LSTM, GRU) to maintain internal memory across time steps. Recurrent policies enable agents to handle partial observability by aggregating information over observation histories.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-recursive-character-splitting",
      "term": "Recursive Character Splitting",
      "definition": "A document chunking strategy that attempts to split text using a hierarchy of separators from paragraph breaks down to individual characters, preferring natural boundaries while ensuring each chunk remains within the target size.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-recursive-feature-elimination",
      "term": "Recursive Feature Elimination",
      "definition": "A feature selection method that repeatedly trains a model, ranks features by importance, and removes the least important features, iterating until the desired number of features remains.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-recursive-prompting",
      "term": "Recursive Prompting",
      "definition": "A prompting pattern where the output of one prompt call is used to construct the next prompt in a recursive loop, enabling the model to handle arbitrarily complex tasks by repeatedly refining or extending its work until a termination condition is met.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ]
    },
    {
      "id": "term-red-team",
      "term": "Red Team",
      "definition": "A group that tests AI systems by attempting to find vulnerabilities, bypass safety measures, or elicit harmful outputs. Essential for identifying and fixing safety issues before deployment.",
      "tags": [
        "Safety",
        "Testing"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-reduce-scatter",
      "term": "Reduce-Scatter Operation",
      "definition": "A collective communication pattern that reduces data across all participants and distributes different chunks of the result to each participant. Reduce-scatter is used in ZeRO and FSDP for gradient aggregation and sharding.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-reflexion-pattern",
      "term": "Reflexion Pattern",
      "definition": "An agent architecture where the LLM reflects on previous failed attempts by storing verbal feedback in an episodic memory, using these reflections to improve performance on subsequent tries.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-refusal",
      "term": "Refusal",
      "definition": "When AI declines to answer a request due to safety guidelines. Well-calibrated refusals protect against harm while overly cautious refusals reduce helpfulness.",
      "tags": [
        "Safety",
        "Behavior"
      ]
    },
    {
      "id": "term-region-proposal-network",
      "term": "Region Proposal Network",
      "definition": "A fully convolutional network that slides over feature maps to generate object proposals (candidate bounding boxes) with objectness scores, serving as the first stage of two-stage detectors like Faster R-CNN.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-regression",
      "term": "Regression",
      "definition": "A machine learning task that predicts continuous values (like prices or temperatures) rather than categories. Common algorithms include linear regression and neural network regressors.",
      "tags": [
        "ML Task",
        "Prediction"
      ]
    },
    {
      "id": "term-regret",
      "term": "Regret",
      "definition": "In online learning and bandit problems, the cumulative difference between the reward obtained by an algorithm and the reward that would have been obtained by always choosing the optimal action in hindsight.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-regret-bound",
      "term": "Regret Bound",
      "definition": "A theoretical guarantee on the cumulative difference between the reward obtained by an RL algorithm and the reward of the optimal policy over T steps. Regret bounds characterize the efficiency of exploration algorithms.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-regularization",
      "term": "Regularization",
      "definition": "Techniques to prevent overfitting by adding constraints during training. Includes dropout, weight decay, and early stopping. Improves generalization to new data.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-reinforce-algorithm",
      "term": "REINFORCE Algorithm",
      "definition": "A foundational Monte Carlo policy gradient algorithm that updates policy parameters proportionally to the return multiplied by the gradient of log-probability of the action taken. It is simple but suffers from high variance.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-reinforcement-learning",
      "term": "Reinforcement Learning (RL)",
      "definition": "A learning paradigm where agents learn by receiving rewards or penalties for actions. Used in RLHF to align LLMs with human preferences.",
      "tags": [
        "Learning Type",
        "Training"
      ]
    },
    {
      "id": "term-reinforcement-learning-history",
      "term": "Reinforcement Learning History",
      "definition": "The development of reinforcement learning from early work by Arthur Samuel on checkers in 1959 through temporal difference learning by Sutton in 1988 to deep RL breakthroughs with DQN and AlphaGo.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-rejection-sampling",
      "term": "Rejection Sampling",
      "definition": "A basic Monte Carlo method for generating samples from a target distribution by sampling from a proposal distribution and accepting or rejecting samples based on a comparison with the target density scaled by a bound.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-relabeling",
      "term": "Relabeling",
      "definition": "A data augmentation technique in RL that modifies components of stored transitions (such as goals, rewards, or actions) to generate additional training signal from existing data. Relabeling is central to HER and goal-conditioned RL.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-relation-extraction",
      "term": "Relation Extraction",
      "definition": "The task of identifying and classifying semantic relationships between entities mentioned in text, such as extracting that a person works for a specific organization.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-relative-positional-encoding",
      "term": "Relative Positional Encoding",
      "definition": "A positional encoding scheme that encodes the relative distance between tokens rather than absolute positions, enabling better generalization to sequence lengths not seen during training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-relevance-score",
      "term": "Relevance Score",
      "definition": "A metric that measures how well a generated response addresses the input query or matches the intended topic, evaluating content appropriateness and topical alignment between the question and answer.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-relu",
      "term": "ReLU (Rectified Linear Unit)",
      "definition": "A simple activation function that outputs zero for negative inputs and the input itself for positives. Widely used due to efficiency and effectiveness despite simplicity.",
      "tags": [
        "Architecture",
        "Function"
      ]
    },
    {
      "id": "term-repetition-penalty",
      "term": "Repetition Penalty",
      "definition": "A decoding parameter that reduces the probability of tokens that have already appeared in the generated text, preventing the model from producing repetitive phrases or loops.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-repetition-rate",
      "term": "Repetition Rate",
      "definition": "A metric that quantifies the frequency of repeated phrases, sentences, or patterns within generated text, used to detect and penalize degenerate model behavior such as looping or excessive redundancy.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rephrase-and-respond",
      "term": "Rephrase and Respond",
      "definition": "A prompting method that asks the model to first rephrase the input question in its own words before answering it, improving comprehension and reducing misinterpretation by ensuring the model accurately understands the query intent.",
      "tags": [
        "Prompt Engineering",
        "Clarification"
      ]
    },
    {
      "id": "term-replay-buffer",
      "term": "Replay Buffer",
      "definition": "A data structure (typically a fixed-size circular buffer) that stores past experience tuples for sampling during off-policy training. Replay buffers break temporal correlations and enable multiple learning updates from each experience.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-replication-vector-databases",
      "term": "Replication in Vector Databases",
      "definition": "The maintenance of multiple copies of a vector index across different nodes to provide fault tolerance and increased read throughput, ensuring that vector search remains available even when individual nodes fail.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-representation-engineering",
      "term": "Representation Engineering",
      "definition": "A technique for understanding and controlling LLM behavior by identifying and manipulating specific directions in the model's activation space that correspond to concepts like honesty, safety, or sentiment.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-representation-learning",
      "term": "Representation Learning",
      "definition": "Learning useful features automatically from data rather than engineering them manually. A key strength of deep learning that enables transfer learning.",
      "tags": [
        "Concept",
        "Deep Learning"
      ]
    },
    {
      "id": "term-representation-learning-rl",
      "term": "Representation Learning in RL",
      "definition": "Methods for learning compact, informative state representations from high-dimensional observations (like images) to improve RL efficiency. Techniques include contrastive learning, reconstruction-based methods, and bisimulation metrics.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-representational-harm",
      "term": "Representational Harm",
      "definition": "Harm that occurs when an AI system reinforces stereotypes, demeans, or erases particular social groups through its outputs, even if no direct resource allocation decision is affected.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-request-scheduling",
      "term": "Request Scheduling",
      "definition": "The algorithm that determines the order and priority of processing incoming inference requests on limited GPU resources. Scheduling strategies optimize for fairness, latency SLAs, throughput, or cost across heterogeneous request workloads.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-reranking",
      "term": "Reranking",
      "definition": "A two-stage retrieval approach where an initial fast retriever fetches candidate documents, and a more powerful cross-encoder model rescores and reorders them for relevance before passing them to the LLM.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-residual-analysis",
      "term": "Residual Analysis",
      "definition": "The examination of residuals (differences between observed and predicted values) to assess model fit, check assumptions such as normality and homoscedasticity, and identify influential observations or patterns.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-residual-connection",
      "term": "Residual Connection",
      "definition": "An additive skip connection where the input to a layer block is added element-wise to the block's output, allowing the network to learn residual mappings rather than direct mappings.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-residual-learning",
      "term": "Residual Learning",
      "definition": "The principle of learning additive residual functions with reference to the layer inputs rather than learning unreferenced functions, making it easier to optimize very deep networks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-resnet",
      "term": "ResNet",
      "definition": "Residual Network, a deep convolutional neural network architecture that introduces skip connections to enable training of very deep networks by mitigating the vanishing gradient problem.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-responsible-ai",
      "term": "Responsible AI",
      "definition": "An approach to developing and deploying AI systems that emphasizes ethical considerations, fairness, transparency, accountability, and societal benefit throughout the entire AI lifecycle.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-responsible-disclosure-for-ai",
      "term": "Responsible Disclosure for AI",
      "definition": "The practice of privately reporting discovered vulnerabilities or dangerous capabilities in AI systems to the developer before public disclosure, allowing time for mitigation while ensuring transparency.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-restricted-boltzmann-machine",
      "term": "Restricted Boltzmann Machine",
      "definition": "A generative stochastic neural network with a bipartite structure of visible and hidden units with no intra-layer connections, trained using contrastive divergence to learn probability distributions over inputs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-retinanet",
      "term": "RetinaNet",
      "definition": "A single-stage object detection model that combines a Feature Pyramid Network backbone with focal loss, achieving accuracy comparable to two-stage detectors while maintaining the speed advantage of single-stage methods.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-retnet",
      "term": "RetNet",
      "definition": "Retentive Network, an architecture that supports parallel training, recurrent inference, and chunk-wise recurrent computation through a retention mechanism that replaces standard attention.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-retrieval",
      "term": "Retrieval",
      "definition": "Finding relevant information from a database or corpus. In RAG, retrieval brings external knowledge into the generation process for more accurate responses.",
      "tags": [
        "Process",
        "Search"
      ]
    },
    {
      "id": "term-retrieval-evaluation",
      "term": "Retrieval Evaluation",
      "definition": "The systematic assessment of retrieval system quality using metrics such as recall, precision, NDCG, and MRR applied to ranked result lists, often conducted against labeled relevance judgments or ground-truth answer sets.",
      "tags": [
        "Retrieval",
        "Evaluation"
      ]
    },
    {
      "id": "term-retrieval-head",
      "term": "Retrieval Head",
      "definition": "Specific attention heads within a transformer that specialize in copying or retrieving information from the context, playing a crucial role in in-context learning and factual recall.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-retrieval-augmented-fine-tuning",
      "term": "Retrieval-Augmented Fine-Tuning",
      "definition": "A training approach that fine-tunes a language model with retrieval-augmented examples, teaching the model to effectively incorporate retrieved context into its responses.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-retrieval-augmented-lm",
      "term": "Retrieval-Augmented Language Model",
      "definition": "A language model architecture that incorporates a retrieval component to fetch relevant documents from an external corpus during generation, grounding responses in retrieved evidence.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-retrieval-augmented-prompting",
      "term": "Retrieval-Augmented Prompting",
      "definition": "A technique that dynamically retrieves relevant documents, examples, or knowledge from an external corpus and incorporates them into the prompt context before generation, improving factual accuracy and domain coverage beyond what is stored in model parameters.",
      "tags": [
        "Prompt Engineering",
        "Retrieval"
      ]
    },
    {
      "id": "term-return",
      "term": "Return",
      "definition": "The cumulative discounted sum of future rewards from a given time step, representing the total long-term value an agent receives. The return is the primary quantity that RL algorithms aim to maximize.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-reward",
      "term": "Reward",
      "definition": "A scalar signal received by an agent from the environment after taking an action, indicating how good or bad the outcome was. Rewards drive learning by defining the optimization objective.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-reward-clipping",
      "term": "Reward Clipping",
      "definition": "A preprocessing technique that bounds reward values to a fixed range (commonly [-1, 1]) to stabilize training across diverse environments. Reward clipping was used in the original DQN but can discard useful reward magnitude information.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-decomposition",
      "term": "Reward Decomposition",
      "definition": "Techniques that break a complex reward signal into simpler components that are easier to learn from, enabling more interpretable and efficient training. Decomposed rewards can align with sub-objectives or different aspects of desired behavior.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-engineering",
      "term": "Reward Engineering",
      "definition": "The process of designing reward functions that accurately capture desired agent behavior, balancing specificity with generality. Poor reward engineering can lead to reward hacking where agents exploit loopholes in the reward specification.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-hacking",
      "term": "Reward Hacking",
      "definition": "A failure mode in reinforcement learning from human feedback where the policy model exploits weaknesses in the reward model to achieve high reward scores without genuinely improving output quality.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-reward-machine",
      "term": "Reward Machine",
      "definition": "A finite-state automaton that specifies reward functions based on high-level events or propositional symbols, enabling structured reward specification for complex tasks. Reward machines decompose non-Markovian rewards into manageable components.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-model",
      "term": "Reward Model",
      "definition": "A model trained to score AI outputs based on human preferences. Used in RLHF to guide language models toward more helpful and safe behaviors.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-reward-normalization",
      "term": "Reward Normalization",
      "definition": "The practice of scaling reward signals to have consistent magnitude across different environments or during training, typically using running statistics. Reward normalization stabilizes value function learning and improves hyperparameter transfer.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-shaping",
      "term": "Reward Shaping",
      "definition": "The practice of adding auxiliary reward signals to guide learning, making sparse reward problems more tractable. Potential-based reward shaping preserves the optimal policy while accelerating convergence.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-reward-free-exploration",
      "term": "Reward-Free Exploration",
      "definition": "An RL paradigm where the agent first explores the environment without any reward signal to build a comprehensive understanding, then uses this knowledge to quickly solve downstream tasks. Reward-free exploration decouples exploration from task specification.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-reward-weighted-regression",
      "term": "Reward-Weighted Regression",
      "definition": "A policy search method that computes policy updates by performing weighted maximum likelihood estimation on sampled trajectories, with weights proportional to exponentiated returns. It avoids explicit gradient computation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-rst",
      "term": "Rhetorical Structure Theory",
      "definition": "A theory of text organization that describes how clauses and larger text spans are connected through rhetorical relations like elaboration, contrast, and cause, forming a hierarchical discourse tree.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-richard-sutton",
      "term": "Richard Sutton",
      "definition": "Canadian computer scientist who co-authored the seminal textbook on reinforcement learning with Andrew Barto, developed temporal difference learning, and wrote the influential Bitter Lesson essay on AI research methodology.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-ridge-regression",
      "term": "Ridge Regression",
      "definition": "A linear regression variant that adds an L2 penalty term to the ordinary least squares objective, shrinking coefficients toward zero to reduce overfitting, especially when predictors are correlated.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-right-to-explanation",
      "term": "Right to Explanation",
      "definition": "The legal or ethical principle that individuals subjected to automated decision-making are entitled to a meaningful explanation of the logic involved, as partially codified in the GDPR's Article 22.",
      "tags": [
        "Governance",
        "AI Ethics"
      ]
    },
    {
      "id": "term-ring-all-reduce",
      "term": "Ring All-Reduce",
      "definition": "An efficient implementation of all-reduce where GPUs are arranged in a ring topology and data is sent in chunks through two passes (scatter-reduce and all-gather). Ring all-reduce provides bandwidth-optimal communication scaling.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-ring-attention",
      "term": "Ring Attention",
      "definition": "A distributed attention computation method that splits long sequences across devices in a ring topology, overlapping communication with computation to process sequences of near-unlimited length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-risk-sensitive-rl",
      "term": "Risk-Sensitive RL",
      "definition": "RL methods that optimize risk-aware objectives such as conditional value-at-risk (CVaR) or variance-penalized returns rather than expected return alone. Risk-sensitive approaches are critical for safety-critical applications.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ]
    },
    {
      "id": "term-rlhf",
      "term": "RLHF (Reinforcement Learning from Human Feedback)",
      "definition": "A training technique that uses human preferences to guide model behavior. Human raters compare outputs, and these preferences train a reward model that shapes the LLM.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-rms-normalization",
      "term": "RMS Normalization",
      "definition": "Root Mean Square Layer Normalization, a simplified variant of layer normalization that only rescales by the root mean square of activations without recentering, reducing computational cost while maintaining performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-rmsnorm",
      "term": "RMSNorm",
      "definition": "Root Mean Square Layer Normalization, a simplified normalization technique that normalizes activations using only the RMS statistic without mean centering, reducing computation while maintaining model quality.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-rmsprop",
      "term": "RMSProp",
      "definition": "An adaptive learning rate optimization algorithm that divides the learning rate by a running average of the magnitudes of recent gradients for each parameter. It addresses the diminishing learning rate problem of AdaGrad.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-rnn",
      "term": "RNN (Recurrent Neural Network)",
      "definition": "A neural network architecture that processes sequences by maintaining hidden state across steps. Predecessors to transformers, still used in some sequence applications.",
      "tags": [
        "Architecture",
        "Historical"
      ]
    },
    {
      "id": "term-roberta",
      "term": "RoBERTa",
      "definition": "A robustly optimized BERT pretraining approach that improves upon BERT by training longer with more data, removing next sentence prediction, and using dynamic masking patterns.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-robot-rights",
      "term": "Robot Rights",
      "definition": "The concept that sufficiently advanced robots or AI systems might deserve legal protections or moral consideration analogous to those granted to humans or animals, a topic of active philosophical and legal debate.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-robust-regression",
      "term": "Robust Regression",
      "definition": "A class of regression methods designed to be resistant to outliers and violations of model assumptions. Techniques include M-estimation, least trimmed squares, and RANSAC.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-robust-rl",
      "term": "Robust RL",
      "definition": "RL algorithms designed to find policies that perform well under worst-case environment perturbations or model uncertainty. Robust RL optimizes against an adversarial set of possible environments or transition dynamics.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ]
    },
    {
      "id": "term-robust-scaler",
      "term": "Robust Scaler",
      "definition": "A feature scaling method that uses the median and interquartile range instead of the mean and standard deviation, making it more resistant to outliers than standard scaling.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-roc-curve",
      "term": "ROC Curve",
      "definition": "Receiver Operating Characteristic curve, a plot of the true positive rate against the false positive rate at various classification thresholds. It visualizes the tradeoff between sensitivity and specificity.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-roger-schank",
      "term": "Roger Schank",
      "definition": "American AI researcher (1946-2023) who developed conceptual dependency theory and script theory for natural language understanding, advancing the role of knowledge structures in AI comprehension of stories and situations.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-roi-align",
      "term": "ROI Align",
      "definition": "An improved version of ROI Pooling that uses bilinear interpolation instead of quantized grid snapping, eliminating misalignment artifacts and producing more accurate feature extraction for instance segmentation.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-roi-pooling",
      "term": "ROI Pooling",
      "definition": "Region of Interest Pooling, an operation that extracts fixed-size feature representations from arbitrary-sized region proposals, enabling the classification head of object detectors to process variable-sized regions uniformly.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-role-assignment",
      "term": "Role Assignment",
      "definition": "The prompt technique of explicitly designating a specific role, profession, or character for the model to adopt, shaping its response style, vocabulary, expertise level, and perspective to match the assigned identity.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ]
    },
    {
      "id": "term-role-prompting",
      "term": "Role Prompting",
      "definition": "Assigning AI a specific persona, expertise, or perspective to shape its responses. For example, \"Act as a senior developer\" or \"You are a patient teacher.\"",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/crisp.html"
    },
    {
      "id": "term-roofline-model",
      "term": "Roofline Model",
      "definition": "A performance analysis framework that plots achievable performance as a function of operational intensity (FLOPS per byte of memory traffic). The roofline model identifies whether a workload is compute-bound or memory-bound, guiding optimization strategy.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ]
    },
    {
      "id": "term-root-mean-squared-error",
      "term": "Root Mean Squared Error",
      "definition": "The square root of the mean squared error, expressed in the same units as the target variable. It provides an interpretable measure of the typical magnitude of prediction errors.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-rope",
      "term": "RoPE (Rotary Position Embedding)",
      "definition": "A positional encoding technique that encodes position through rotation in complex space. Enables better length generalization than absolute position encodings.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-ross-quillian",
      "term": "Ross Quillian",
      "definition": "American computer scientist who introduced semantic networks in his 1968 doctoral thesis as a model of human associative memory, establishing a foundational approach to knowledge representation in AI.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-rotary-position-embedding",
      "term": "Rotary Position Embedding",
      "definition": "A method that encodes position information by rotating query and key vectors in pairs of dimensions according to their position, naturally encoding relative positions through the inner product.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-rouge",
      "term": "ROUGE",
      "definition": "Recall-Oriented Understudy for Gisting Evaluation, a set of metrics for evaluating summarization quality by measuring n-gram overlap between generated summaries and reference summaries.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-rouge-score",
      "term": "ROUGE Score",
      "definition": "Recall-Oriented Understudy for Gisting Evaluation, a family of metrics that measures text summarization quality by computing n-gram overlap, longest common subsequences, or skip-bigram co-occurrence between generated and reference summaries.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rouge-1",
      "term": "ROUGE-1",
      "definition": "A ROUGE variant that measures unigram (single word) overlap between a generated text and reference text, providing a basic assessment of content coverage at the individual word level.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rouge-2",
      "term": "ROUGE-2",
      "definition": "A ROUGE variant that measures bigram (two consecutive word) overlap between generated and reference texts, capturing phrase-level similarity and providing a stronger indicator of fluency and content preservation than ROUGE-1.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rouge-l",
      "term": "ROUGE-L",
      "definition": "A ROUGE variant based on the longest common subsequence (LCS) between generated and reference texts, capturing sentence-level structural similarity without requiring consecutive word matches, making it sensitive to word ordering.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-rt-detr",
      "term": "RT-DETR",
      "definition": "Real-Time Detection Transformer, a hybrid detection architecture that combines a CNN backbone with a transformer decoder, achieving the accuracy of DETR-based models with real-time inference speed.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-rwkv",
      "term": "RWKV",
      "definition": "A linear attention-based architecture that combines the parallelizable training of transformers with the efficient inference of RNNs, using a novel time-mixing and channel-mixing approach.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-s4",
      "term": "S4",
      "definition": "Structured State Spaces for Sequence Modeling, a state space model that uses a special initialization based on the HiPPO framework and efficient computation via FFT to handle very long-range dependencies.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-safe-rl",
      "term": "Safe Reinforcement Learning",
      "definition": "RL methods that incorporate safety constraints to prevent the agent from taking dangerous or unacceptable actions during training and deployment. Safe RL uses constrained optimization, shielding, or risk-sensitive objectives.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ]
    },
    {
      "id": "term-safety-filter",
      "term": "Safety Filter",
      "definition": "Systems that detect and block harmful content in AI inputs or outputs. Part of the safety stack protecting users from inappropriate, dangerous, or illegal content.",
      "tags": [
        "Safety",
        "Security"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-safety-score",
      "term": "Safety Score",
      "definition": "A composite evaluation metric that aggregates measurements of harmful output generation including toxicity, bias, dangerous advice, and policy violations, providing an overall assessment of a model's safety alignment.",
      "tags": [
        "Evaluation",
        "Safety"
      ]
    },
    {
      "id": "term-sam-altman",
      "term": "Sam Altman",
      "definition": "American entrepreneur who became CEO of OpenAI, overseeing the development and launch of ChatGPT and GPT-4. His brief firing and reinstatement in November 2023 highlighted governance tensions at leading AI companies.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-sambanova",
      "term": "SambaNova",
      "definition": "An AI hardware company producing reconfigurable dataflow architecture processors (SN series) that adapt their compute topology to different model architectures. SambaNova's dataflow approach streams data through the chip without traditional memory hierarchy bottlenecks.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-sample-complexity",
      "term": "Sample Complexity",
      "definition": "The minimum number of training examples required for a learning algorithm to achieve a specified level of performance with high probability. It depends on the complexity of the hypothesis class and the desired accuracy.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-sample-complexity-rl",
      "term": "Sample Complexity",
      "definition": "The number of environment interactions needed for an RL algorithm to find a near-optimal policy with high probability. Sample complexity is a key measure of algorithmic efficiency, especially when interactions are expensive.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-sampling",
      "term": "Sampling",
      "definition": "The process of selecting the next token during text generation. Methods include greedy (always pick highest probability), top-k, top-p (nucleus), and temperature-based sampling.",
      "tags": [
        "Generation",
        "Technical"
      ]
    },
    {
      "id": "term-sandwich-prompting",
      "term": "Sandwich Prompting",
      "definition": "A prompt structure that places the core instruction both before and after the main context or input data, reinforcing adherence to instructions that might otherwise be lost in long contexts due to positional attention biases.",
      "tags": [
        "Prompt Engineering",
        "Structure"
      ]
    },
    {
      "id": "term-sarcasm-detection",
      "term": "Sarcasm Detection",
      "definition": "The task of identifying sarcastic or ironic statements in text where the intended meaning differs from the literal meaning, a challenging problem requiring pragmatic understanding.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-sarima",
      "term": "SARIMA",
      "definition": "Seasonal ARIMA, an extension of the ARIMA model that includes additional seasonal autoregressive, differencing, and moving average terms to capture periodic patterns in time series data.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-sarsa",
      "term": "SARSA",
      "definition": "An on-policy temporal difference control algorithm that updates Q-values using the actual action taken by the current policy (State-Action-Reward-State-Action). Unlike Q-learning, SARSA learns the value of the policy being followed.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-satellite-imagery-analysis",
      "term": "Satellite Imagery Analysis",
      "definition": "The use of computer vision models to interpret aerial and satellite photographs for applications including land use classification, change detection, disaster assessment, and environmental monitoring.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-scalable-oversight",
      "term": "Scalable Oversight",
      "definition": "The challenge and research agenda of maintaining meaningful human oversight of AI systems as they become more capable and handle tasks too complex for humans to directly evaluate.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-scalar-quantization",
      "term": "Scalar Quantization",
      "definition": "A vector compression method that reduces memory usage by converting each floating-point vector component to a lower-precision representation such as 8-bit integers, achieving 4x compression from float32 with minimal accuracy loss.",
      "tags": [
        "Vector Database",
        "Quantization"
      ]
    },
    {
      "id": "term-scale-ai",
      "term": "Scale AI",
      "definition": "A company specializing in data labeling and annotation services for AI training. Provides human feedback at scale, crucial for training and evaluating foundation models.",
      "tags": [
        "Company",
        "Data"
      ]
    },
    {
      "id": "term-scaling-hypothesis",
      "term": "Scaling Hypothesis",
      "definition": "The hypothesis that increasing model size, training data, and compute leads to predictable and substantial improvements in AI capability, supported by empirical scaling laws discovered by Kaplan et al. at OpenAI.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-scaling-law",
      "term": "Scaling Law",
      "definition": "Empirical power-law relationships that predict how model performance improves as a function of compute budget, model size, and dataset size, guiding efficient resource allocation for training.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-scaling-laws",
      "term": "Scaling Laws",
      "definition": "Empirical relationships showing how model performance improves with more parameters, data, and compute. Guide decisions about where to invest resources in training larger models.",
      "tags": [
        "Research",
        "Training"
      ]
    },
    {
      "id": "term-scaling-laws-compute",
      "term": "Scaling Laws for Compute",
      "definition": "Empirical power-law relationships between model performance and compute budget, model size, and dataset size established by Kaplan et al. and Hoffmann et al. (Chinchilla). Scaling laws guide optimal allocation of training compute.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-scene-graph-generation",
      "term": "Scene Graph Generation",
      "definition": "The task of detecting objects in an image and predicting their pairwise relationships to construct a graph representation where nodes are objects and edges are visual relationships.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-scene-text-recognition",
      "term": "Scene Text Recognition",
      "definition": "The task of reading and transcribing text found in natural images (street signs, product labels, building facades), dealing with perspective distortion, partial occlusion, and diverse typography.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-scikit-learn",
      "term": "Scikit-learn",
      "definition": "A popular Python library for traditional machine learning algorithms. Provides simple APIs for classification, regression, clustering, and preprocessing. The go-to for non-deep-learning ML.",
      "tags": [
        "Framework",
        "ML"
      ]
    },
    {
      "id": "term-score-function",
      "term": "Score Function",
      "definition": "The gradient of the log-likelihood function with respect to the parameter, used in maximum likelihood estimation and Fisher information calculations. Its expected value under the true distribution is zero.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-score-based-generative-model",
      "term": "Score-Based Generative Model",
      "definition": "A generative model that learns the gradient of the log probability density (score function) of the data distribution, then uses Langevin dynamics or similar methods to generate samples.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-sdxl",
      "term": "SDXL",
      "definition": "Stable Diffusion XL, an improved latent diffusion model that uses a larger UNet with dual text encoders and an optional refiner model to generate higher-resolution, more detailed images.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-seasonal-decomposition",
      "term": "Seasonal Decomposition",
      "definition": "A time series analysis technique that separates a time series into trend, seasonal, and residual components, either additively or multiplicatively, to better understand underlying patterns.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-second-ai-winter",
      "term": "Second AI Winter",
      "definition": "The period from approximately 1987 to 1993 when enthusiasm for AI again collapsed following the failure of expert systems to scale, the collapse of the LISP machine market, and cuts to government AI funding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-secure-multi-party-computation",
      "term": "Secure Multi-Party Computation",
      "definition": "A cryptographic protocol that allows multiple parties to jointly compute a function over their combined inputs while keeping each party's input private, used in privacy-preserving machine learning applications.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-segment-anything-model",
      "term": "Segment Anything Model",
      "definition": "A foundation model for image segmentation (SAM) trained on a massive dataset that can segment any object in any image given a prompt such as a point, bounding box, or text description.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-selection-bias",
      "term": "Selection Bias",
      "definition": "A systematic error arising when the sample used for analysis is not representative of the population, due to the way observations were selected. It can lead to models that do not generalize to the true population.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-selectional-preference",
      "term": "Selectional Preference",
      "definition": "The tendency of predicates to semantically constrain their arguments, such as 'eat' preferring edible objects, used in NLP for disambiguation and semantic plausibility assessment.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-selective-context",
      "term": "Selective Context",
      "definition": "A prompt compression method that evaluates the informativeness of each lexical unit in a context using self-information scores from a causal language model, then filters out low-information content to reduce prompt length while retaining key details.",
      "tags": [
        "Prompt Engineering",
        "Compression"
      ]
    },
    {
      "id": "term-self-ask",
      "term": "Self-Ask",
      "definition": "A prompting technique where the model explicitly asks itself follow-up questions needed to answer a complex query, then answers each sub-question before synthesizing the final response, naturally decomposing multi-hop reasoning tasks.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-self-attention",
      "term": "Self-Attention",
      "definition": "An attention mechanism where a sequence attends to itself, allowing each position to consider all other positions. The core operation in transformer models.",
      "tags": [
        "Architecture",
        "Transformers"
      ]
    },
    {
      "id": "term-self-bleu",
      "term": "Self-BLEU",
      "definition": "A diversity metric that computes BLEU scores between pairs of generated sentences from the same model, where lower Self-BLEU indicates greater diversity and less repetition across the model's outputs.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-self-consistency",
      "term": "Self-Consistency",
      "definition": "A technique where AI generates multiple reasoning paths and selects the most common answer. Improves accuracy for complex reasoning by aggregating diverse approaches.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-self-driving-car-history",
      "term": "Self-Driving Car History",
      "definition": "The evolution of autonomous vehicles from early projects like Stanford Cart in 1961 and CMU's Navlab in 1986 through the DARPA Grand Challenges, Google's self-driving car project in 2009, and modern robotaxi services.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-self-play",
      "term": "Self-Play",
      "definition": "A training paradigm where an agent improves by playing against copies of itself, generating increasingly challenging opponents as its skill grows. Self-play creates an automatic curriculum and has driven breakthroughs in game-playing AI.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-self-polish-prompting",
      "term": "Self-Polish Prompting",
      "definition": "A technique that prompts the model to progressively refine and polish the given problem conditions before solving, enabling the model to simplify complex problems into more tractable formulations through iterative rewriting.",
      "tags": [
        "Prompt Engineering",
        "Refinement"
      ]
    },
    {
      "id": "term-self-rag",
      "term": "Self-RAG",
      "definition": "A framework where the language model learns to retrieve on demand, generate text, and critique its own output using special reflection tokens, adaptively deciding when retrieval is necessary.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-self-training",
      "term": "Self-Training",
      "definition": "A semi-supervised learning method where a model trained on labeled data generates pseudo-labels for unlabeled data, then retrains on both real and pseudo-labeled examples, iteratively expanding the training set.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-self-training-vision",
      "term": "Self-Training for Vision",
      "definition": "A semi-supervised learning approach where a teacher model generates pseudo-labels for unlabeled images, and a student model is trained on the combination of labeled data and high-confidence pseudo-labels.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-semantic-caching",
      "term": "Semantic Caching",
      "definition": "A caching strategy that stores LLM responses indexed by the semantic meaning of queries rather than exact string matches, allowing cache hits for paraphrased or similar questions.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-semantic-chunking",
      "term": "Semantic Chunking",
      "definition": "A document splitting approach that determines chunk boundaries based on semantic similarity between consecutive sentences, creating new chunks when the topic or meaning shifts significantly rather than using fixed-size character or token counts.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-semantic-correspondence",
      "term": "Semantic Correspondence",
      "definition": "The task of finding matching points between images of different instances of the same object category, requiring understanding of semantic similarity rather than just visual appearance matching.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-semantic-kernel",
      "term": "Semantic Kernel",
      "definition": "Microsoft's open-source SDK for building AI applications with LLMs. Supports plugin architecture, memory, and planning for enterprise AI agent development.",
      "tags": [
        "Framework",
        "Application"
      ]
    },
    {
      "id": "term-semantic-map",
      "term": "Semantic Map",
      "definition": "A spatial representation that annotates each location with semantic labels indicating what type of object or surface occupies that space, used in robotics navigation and autonomous driving planning.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-semantic-networks",
      "term": "Semantic Networks",
      "definition": "A knowledge representation formalism using graphs of nodes connected by labeled edges to represent concepts and their relationships, first proposed by Ross Quillian in 1968 and widely used in early AI and natural language understanding.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-semantic-parsing",
      "term": "Semantic Parsing",
      "definition": "The task of mapping natural language utterances to formal meaning representations such as logical forms, SQL queries, or lambda calculus expressions that can be executed or reasoned over.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-semantic-role-labeling",
      "term": "Semantic Role Labeling",
      "definition": "The task of identifying the predicate-argument structure of a sentence by assigning semantic roles such as agent, patient, instrument, and location to constituents relative to the predicate.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-semantic-search",
      "term": "Semantic Search",
      "definition": "Search based on meaning rather than keyword matching. Uses embeddings to find conceptually similar content, enabling more relevant results for natural language queries.",
      "tags": [
        "Application",
        "Search"
      ]
    },
    {
      "id": "term-semantic-segmentation",
      "term": "Semantic Segmentation",
      "definition": "A computer vision task that assigns a class label to every pixel in an image, producing a dense prediction map that identifies what object category each pixel belongs to without distinguishing instances.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-semantic-segmentation-transformer",
      "term": "Semantic Segmentation Transformer",
      "definition": "Transformer-based architectures like SegFormer and Mask2Former that apply self-attention to image features for dense per-pixel classification, outperforming CNN-based segmenters on many benchmarks.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-semantic-textual-similarity",
      "term": "Semantic Textual Similarity",
      "definition": "The task of measuring the degree of semantic equivalence between two text segments on a continuous scale, going beyond binary paraphrase detection to capture graded similarity.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-semantics",
      "term": "Semantics",
      "definition": "The branch of linguistics studying meaning in language, including word meaning, sentence meaning, and the relationship between linguistic expressions and what they refer to.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-semi-supervised-learning",
      "term": "Semi-Supervised Learning",
      "definition": "A learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data during training. It leverages the structure in unlabeled data to improve model performance beyond what labeled data alone provides.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-semi-supervised-object-detection",
      "term": "Semi-Supervised Object Detection",
      "definition": "Detection training approaches that leverage both a small set of labeled images and a large pool of unlabeled images, using techniques like pseudo-labeling and consistency regularization.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-sensitivity",
      "term": "Sensitivity",
      "definition": "The proportion of actual positive cases correctly identified by a classifier, synonymous with recall and true positive rate. High sensitivity minimizes false negatives in the predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-sentence-boundary-detection",
      "term": "Sentence Boundary Detection",
      "definition": "The task of identifying where sentences begin and end in running text, handling ambiguous punctuation like periods in abbreviations, decimal numbers, and ellipses.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-sentence-embedding",
      "term": "Sentence Embedding",
      "definition": "A fixed-length dense vector representation of an entire sentence that captures its semantic meaning, produced by methods like mean pooling over token embeddings or dedicated sentence encoders.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-sentence-transformer",
      "term": "Sentence Transformer",
      "definition": "Models that encode entire sentences into single vectors, capturing semantic meaning. Popular for semantic search, similarity matching, and clustering text at the sentence level.",
      "tags": [
        "Model Type",
        "Embeddings"
      ]
    },
    {
      "id": "term-sentencepiece",
      "term": "SentencePiece",
      "definition": "A language-independent tokenization library that treats the input as a raw byte stream and applies BPE or unigram tokenization directly without pre-tokenization or language-specific preprocessing.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-sentiment-analysis",
      "term": "Sentiment Analysis",
      "definition": "An NLP task that determines the emotional tone of text (positive, negative, neutral). Used in customer feedback analysis, social media monitoring, and brand tracking.",
      "tags": [
        "NLP Task",
        "Classification"
      ]
    },
    {
      "id": "term-sentiment-polarity",
      "term": "Sentiment Polarity",
      "definition": "The classification of text sentiment into categories such as positive, negative, or neutral, representing the overall emotional orientation expressed toward the subject of the text.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-sepp-hochreiter",
      "term": "Sepp Hochreiter",
      "definition": "Austrian computer scientist who co-invented the Long Short-Term Memory network architecture in 1997 with Jurgen Schmidhuber, solving a fundamental problem in training recurrent neural networks on long sequences.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-seq2seq",
      "term": "Seq2Seq",
      "definition": "Sequence-to-Sequence, an encoder-decoder framework where an encoder processes an input sequence into a fixed-length representation and a decoder generates an output sequence from that representation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-sequence-labeling",
      "term": "Sequence Labeling",
      "definition": "The task of assigning a categorical label to each element in a sequence, such as tagging each word in a sentence with its named entity type, POS tag, or chunk boundary.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-sequence-parallelism",
      "term": "Sequence Parallelism",
      "definition": "A technique that distributes the processing of long sequences across multiple devices by partitioning the sequence dimension, enabling context lengths that exceed single-device memory limits.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-sequence-level-accuracy",
      "term": "Sequence-Level Accuracy",
      "definition": "An evaluation metric that considers an entire generated sequence correct only if every token matches the reference, providing a strict holistic assessment used in tasks like structured prediction and code generation.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-sft",
      "term": "SFT (Supervised Fine-Tuning)",
      "definition": "Training a pre-trained model on labeled examples of desired behavior. Often the first step in aligning LLMs, teaching them to follow instructions before RLHF.",
      "tags": [
        "Training",
        "Alignment"
      ]
    },
    {
      "id": "term-shakey-the-robot",
      "term": "Shakey the Robot",
      "definition": "The first general-purpose mobile robot developed at SRI International from 1966 to 1972 that could reason about its own actions, integrating computer vision, natural language understanding, and planning.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-shap-values",
      "term": "SHAP Values",
      "definition": "SHapley Additive exPlanations, a unified framework for feature importance that assigns each feature a contribution value for a specific prediction based on Shapley values from cooperative game theory, satisfying desirable theoretical properties.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-shaped-reward",
      "term": "Shaped Reward",
      "definition": "An auxiliary reward signal added to the environment's natural reward to guide learning toward desired behavior. Shaped rewards provide more frequent feedback in sparse reward settings but must be carefully designed to avoid altering the optimal policy.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-shapiro-wilk-test",
      "term": "Shapiro-Wilk Test",
      "definition": "A statistical test for normality that evaluates whether a random sample comes from a normal distribution. It is considered one of the most powerful normality tests, especially for small sample sizes.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-shared-memory-gpu",
      "term": "Shared Memory (GPU)",
      "definition": "A fast, programmer-managed on-chip SRAM in GPU streaming multiprocessors that enables efficient data sharing between threads in a thread block. FlashAttention exploits shared memory to avoid expensive HBM accesses for attention computation.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-shrdlu",
      "term": "SHRDLU",
      "definition": "A natural language understanding program created by Terry Winograd at MIT in 1970 that could converse about and manipulate objects in a simulated blocks world, demonstrating early natural language processing capabilities.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-sift",
      "term": "SIFT",
      "definition": "Scale-Invariant Feature Transform, a classical algorithm that detects and describes local image features robust to scale, rotation, and illumination changes, widely used as a baseline for feature matching.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-signed-distance-function",
      "term": "Signed Distance Function",
      "definition": "A 3D representation where a neural network predicts the signed distance from any 3D point to the nearest surface, with the zero-level set defining the shape, enabling smooth surface extraction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-significance-level",
      "term": "Significance Level",
      "definition": "The threshold probability (alpha, typically 0.05) below which the p-value must fall for the null hypothesis to be rejected. It defines the acceptable risk of making a Type I error.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-silhouette-score",
      "term": "Silhouette Score",
      "definition": "A metric for evaluating clustering quality that measures how similar each point is to its own cluster compared to other clusters. Values range from -1 to 1, where higher values indicate better-defined clusters.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-sim-to-real",
      "term": "Sim-to-Real Transfer",
      "definition": "The challenge and set of techniques for deploying policies trained in simulation to physical systems. Sim-to-real methods include domain randomization, system identification, and progressive fine-tuning to bridge the reality gap.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-simclr",
      "term": "SimCLR",
      "definition": "A contrastive self-supervised learning framework for visual representations that uses data augmentation to create positive pairs and a projection head with NT-Xent loss to learn transferable image features.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-similarity-threshold",
      "term": "Similarity Threshold",
      "definition": "A configurable cutoff value that filters vector search results to include only matches exceeding a minimum similarity score, preventing the retrieval of irrelevant results when no sufficiently similar vectors exist in the index.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-simpsons-paradox",
      "term": "Simpson's Paradox",
      "definition": "A statistical phenomenon where a trend that appears in several different groups of data disappears or reverses when these groups are combined. It highlights the importance of accounting for confounding variables.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-simtom-prompting",
      "term": "SimToM Prompting",
      "definition": "A two-step prompting approach for theory-of-mind tasks that first identifies what information a specific person is aware of, then answers the question based solely on that person's limited perspective rather than full omniscient context.",
      "tags": [
        "Prompt Engineering",
        "Theory of Mind"
      ]
    },
    {
      "id": "term-singular-value-decomposition",
      "term": "Singular Value Decomposition",
      "definition": "A matrix factorization technique that decomposes a matrix into three matrices (U, S, V^T), where S contains singular values. It is foundational for PCA, latent semantic analysis, and matrix completion.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-singularity-concept",
      "term": "Singularity Concept",
      "definition": "The hypothesized future point when AI surpasses human intelligence and triggers runaway technological growth, popularized by Vernor Vinge in 1993 and Ray Kurzweil in his 2005 book The Singularity Is Near.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-sinusoidal-positional-encoding",
      "term": "Sinusoidal Positional Encoding",
      "definition": "A fixed positional encoding scheme that uses sine and cosine functions of different frequencies to encode absolute token positions, allowing the model to learn relative positions through linear projections.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-siri-launch",
      "term": "Siri Launch",
      "definition": "Apple's launch of Siri in October 2011 as the first widely deployed virtual assistant on a smartphone, introducing millions of consumers to conversational AI and voice-activated computing.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-skeleton-of-thought",
      "term": "Skeleton-of-Thought",
      "definition": "A prompting strategy that first asks the model to generate a skeleton outline of the answer, then expands each point in parallel, reducing end-to-end latency by enabling concurrent generation of independent answer segments.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-skill-discovery",
      "term": "Skill Discovery",
      "definition": "Unsupervised methods that learn reusable behavioral primitives or skills without task-specific rewards, typically by maximizing mutual information between skills and states visited. Discovered skills can accelerate downstream task learning.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ]
    },
    {
      "id": "term-skip-connection",
      "term": "Skip Connection",
      "definition": "A shortcut path that bypasses one or more layers by adding or concatenating the input of a block directly to its output, enabling gradient flow through deep networks and facilitating residual learning.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-skip-gram",
      "term": "Skip-gram",
      "definition": "A Word2Vec training objective that predicts surrounding context words given a center word, learning word representations that capture semantic similarity from distributional patterns.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-sliding-window-attention",
      "term": "Sliding Window Attention",
      "definition": "An attention pattern where each token attends only to a fixed-size local window of neighboring tokens, enabling efficient processing of long sequences by limiting the attention span.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-slot-filling",
      "term": "Slot Filling",
      "definition": "The task of extracting specific pieces of information (slot values) from user utterances in a task-oriented dialogue system, such as extracting dates, locations, or names to fill predefined slots.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-smoothquant",
      "term": "SmoothQuant",
      "definition": "A quantization technique that migrates the quantization difficulty from activations to weights by applying per-channel scaling factors, enabling efficient INT8 quantization of both weights and activations for LLMs. SmoothQuant enables W8A8 quantization with minimal accuracy loss.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-smote",
      "term": "SMOTE",
      "definition": "Synthetic Minority Over-sampling Technique, an oversampling method that creates synthetic examples of the minority class by interpolating between existing minority samples and their k-nearest neighbors in feature space.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-social-scoring-systems",
      "term": "Social Scoring Systems",
      "definition": "AI-driven systems that assign scores to individuals based on their behavior, social connections, or characteristics, used to determine access to services. Banned as unacceptable risk under the EU AI Act.",
      "tags": [
        "AI Ethics",
        "Regulation"
      ]
    },
    {
      "id": "term-sociotechnical-systems-approach",
      "term": "Sociotechnical Systems Approach",
      "definition": "An analytical framework that views AI systems as inseparable from their social context, recognizing that technical and social factors jointly determine system outcomes and that both must be addressed for responsible deployment.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-socratic-prompting",
      "term": "Socratic Prompting",
      "definition": "A prompting technique that guides the model through a series of probing questions rather than direct instructions, encouraging step-by-step reasoning and self-discovery of answers through structured dialogue and critical examination.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-sac",
      "term": "Soft Actor-Critic (SAC)",
      "definition": "An off-policy actor-critic algorithm that maximizes both expected return and policy entropy, encouraging exploration while maintaining stable learning. SAC automatically tunes the temperature parameter balancing reward and entropy.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-soft-attention",
      "term": "Soft Attention",
      "definition": "An attention mechanism that computes a weighted average over all positions using continuous attention weights, making it fully differentiable and trainable with standard backpropagation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-soft-update",
      "term": "Soft Update (Polyak Averaging)",
      "definition": "A method for updating target network parameters as an exponential moving average of the online network parameters, controlled by a smoothing factor tau. Soft updates provide more stable training than periodic hard copies.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-softmax",
      "term": "Softmax",
      "definition": "A function that converts raw scores into probabilities that sum to 1. Used in attention mechanisms and classification outputs to create interpretable probability distributions.",
      "tags": [
        "Math",
        "Function"
      ]
    },
    {
      "id": "term-sparse-attention",
      "term": "Sparse Attention",
      "definition": "An attention mechanism that restricts each token to attend to only a subset of other tokens using predefined or learned sparsity patterns, reducing the quadratic computational cost of full attention.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-sparse-autoencoder",
      "term": "Sparse Autoencoder",
      "definition": "An autoencoder that enforces sparsity constraints on the hidden layer activations, encouraging the network to learn a compact, distributed representation where only a few neurons activate for each input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-sparse-retrieval",
      "term": "Sparse Retrieval",
      "definition": "An information retrieval paradigm that represents queries and documents as high-dimensional sparse vectors where most values are zero, with non-zero entries corresponding to term weights, enabling efficient exact matching through inverted indexes.",
      "tags": [
        "Retrieval",
        "Search"
      ]
    },
    {
      "id": "term-sparse-reward",
      "term": "Sparse Reward Problem",
      "definition": "An RL setting where the agent receives non-zero reward signals only rarely, making credit assignment extremely difficult. Sparse rewards are common in real-world tasks and motivate techniques like reward shaping and intrinsic motivation.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ]
    },
    {
      "id": "term-spatial-attention",
      "term": "Spatial Attention",
      "definition": "An attention mechanism that learns to weight different spatial locations in feature maps, focusing computational resources on the most informative regions of the input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-spearman-rank-correlation",
      "term": "Spearman Rank Correlation",
      "definition": "A non-parametric measure of the monotonic relationship between two variables, computed as the Pearson correlation of the ranked values. It does not assume linearity or normal distribution.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-special-tokens",
      "term": "Special Tokens",
      "definition": "Reserved tokens in a vocabulary that serve structural purposes such as marking sequence boundaries, separating segments, padding, or indicating masked positions, rather than representing text content.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-specification-gaming",
      "term": "Specification Gaming",
      "definition": "The behavior of an AI system that satisfies the literal specification of an objective without achieving the intended outcome. It is closely related to reward hacking and arises from misaligned objective functions.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-specificity",
      "term": "Specificity",
      "definition": "The proportion of actual negative cases that are correctly identified as negative by a classifier, also known as the true negative rate. It complements sensitivity (recall) in binary classification evaluation.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ]
    },
    {
      "id": "term-spectral-clustering",
      "term": "Spectral Clustering",
      "definition": "A clustering technique that uses the eigenvalues and eigenvectors of a similarity matrix derived from the data to perform dimensionality reduction before clustering in the reduced space, capable of identifying non-convex clusters.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ]
    },
    {
      "id": "term-speculative-decoding",
      "term": "Speculative Decoding",
      "definition": "A technique to speed up LLM inference by using a smaller model to draft tokens that the larger model verifies in parallel. Maintains output quality while reducing latency.",
      "tags": [
        "Optimization",
        "Inference"
      ]
    },
    {
      "id": "term-speculative-execution-llm",
      "term": "Speculative Execution",
      "definition": "A broader inference optimization paradigm where cheaper computations predict likely outcomes that are verified by full-cost operations, encompassing speculative decoding and related techniques for LLM acceleration.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-speculative-sampling",
      "term": "Speculative Sampling",
      "definition": "An inference acceleration technique where a fast draft model proposes multiple tokens that are then verified in parallel by the target model, maintaining the exact output distribution while increasing throughput.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-speech-recognition-history",
      "term": "Speech Recognition History",
      "definition": "The development of automatic speech recognition from Bell Labs' Audrey system in 1952 through Hidden Markov Models in the 1980s to deep learning approaches that achieved near-human accuracy in the 2010s.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-speech-synthesis",
      "term": "Speech Synthesis",
      "definition": "The artificial production of human-like speech from text or other input, using techniques ranging from concatenative synthesis to neural models like Tacotron and VITS.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-spell-correction",
      "term": "Spell Correction",
      "definition": "The automated detection and correction of misspelled words in text using techniques such as edit distance, language models, and context-aware models to suggest corrections.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-splade",
      "term": "SPLADE",
      "definition": "SParse Lexical AnD Expansion model, a learned sparse retrieval method that predicts importance weights for vocabulary terms including expansion terms not present in the original text, combining the efficiency of sparse indexes with neural relevance estimation.",
      "tags": [
        "Retrieval",
        "Architecture"
      ]
    },
    {
      "id": "term-spline-regression",
      "term": "Spline Regression",
      "definition": "A regression technique using piecewise polynomial functions (splines) joined at knot points to fit flexible, smooth curves. Natural and B-splines are common variants that balance flexibility with smoothness.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-spot-instances",
      "term": "Spot Instances for AI",
      "definition": "Discounted cloud computing instances that use spare capacity at 60-90% less than on-demand pricing but can be interrupted with short notice. Spot instances are widely used for fault-tolerant AI training with checkpointing to resume after interruptions.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-squad",
      "term": "SQuAD",
      "definition": "Stanford Question Answering Dataset, a reading comprehension benchmark where models extract answer spans from Wikipedia passages, with SQuAD 2.0 additionally including unanswerable questions that test a model's ability to abstain when evidence is insufficient.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-squeeze-and-excitation-network",
      "term": "Squeeze-and-Excitation Network",
      "definition": "A CNN enhancement that adaptively recalibrates channel-wise feature responses by using global average pooling followed by a small network to learn channel interdependencies and attention weights.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-squeeze-excitation-network",
      "term": "Squeeze-and-Excitation Network",
      "definition": "A channel attention mechanism that adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels through global pooling and learned gating.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-squeeze-excitation-block",
      "term": "Squeeze-Excitation Block",
      "definition": "A channel attention module that squeezes spatial information via global pooling and excites channel-wise features through a learned gating mechanism to recalibrate feature map importances.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-ssd-object-detection",
      "term": "SSD Object Detection",
      "definition": "Single Shot MultiBox Detector, a real-time object detection architecture that predicts bounding boxes and class probabilities from multiple feature maps at different scales in a single forward pass.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-stable-diffusion",
      "term": "Stable Diffusion",
      "definition": "An open-source text-to-image model from Stability AI. Its open nature enabled a large ecosystem of fine-tuned models, extensions, and applications.",
      "tags": [
        "Model",
        "Image Generation"
      ]
    },
    {
      "id": "term-stacking",
      "term": "Stacking",
      "definition": "An ensemble learning technique that trains a meta-learner to combine the predictions of multiple base models, using cross-validated predictions from the base models as input features for the meta-learner.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-stakeholder-analysis-in-ai",
      "term": "Stakeholder Analysis in AI",
      "definition": "The process of identifying all individuals, groups, and communities affected by an AI system and systematically considering their interests, power dynamics, and potential harms in the system's design and deployment.",
      "tags": [
        "Governance",
        "AI Ethics"
      ]
    },
    {
      "id": "term-stance-detection",
      "term": "Stance Detection",
      "definition": "The task of determining an author's position (favor, against, or neutral) toward a specific target or claim from their text, related to but distinct from sentiment analysis.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-standard-deviation",
      "term": "Standard Deviation",
      "definition": "The square root of the variance, measuring the average spread of data points from the mean in the original units of measurement. It quantifies the typical distance of observations from the central value.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-standardization",
      "term": "Standardization",
      "definition": "A feature scaling technique that transforms data to have zero mean and unit variance by subtracting the mean and dividing by the standard deviation. It is particularly important for algorithms sensitive to feature magnitudes.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-stanford-ai-laboratory",
      "term": "Stanford AI Laboratory",
      "definition": "A research laboratory founded by John McCarthy at Stanford University in 1962 that became one of the leading centers for AI research, making contributions to robotics, natural language processing, and knowledge representation.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-state",
      "term": "State",
      "definition": "A representation of the current situation of an agent within its environment at a given time step. States encode all relevant information needed for decision-making under the Markov property.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-state-abstraction",
      "term": "State Abstraction",
      "definition": "The process of mapping a detailed state space to a simplified representation that preserves relevant decision-making information. State abstraction reduces the complexity of RL problems and can improve generalization.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-state-space-model",
      "term": "State Space Model",
      "definition": "A sequence model based on continuous-time linear dynamical systems that maps input sequences to output sequences through a latent state, offering efficient parallel training and linear-time inference.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-static-quantization",
      "term": "Static Quantization",
      "definition": "A quantization approach where scaling factors are fixed at calibration time and used consistently during inference. Static quantization is faster than dynamic quantization at inference time but requires representative calibration data.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-static-word-embedding",
      "term": "Static Word Embedding",
      "definition": "A fixed vector representation for each word in the vocabulary that remains the same regardless of context, as produced by models like Word2Vec, GloVe, and FastText.",
      "tags": [
        "NLP",
        "Embeddings"
      ]
    },
    {
      "id": "term-stationarity",
      "term": "Stationarity",
      "definition": "A property of a time series where statistical properties such as mean, variance, and autocorrelation structure remain constant over time. Many time series models require stationarity as a prerequisite.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-statistical-machine-translation",
      "term": "Statistical Machine Translation",
      "definition": "A machine translation approach that uses statistical models learned from bilingual text corpora to find the most probable translation, employing language models and translation models.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-statistical-power",
      "term": "Statistical Power",
      "definition": "The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true (1 minus the probability of a Type II error). Higher power reduces the chance of missing a real effect.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-steering-vector",
      "term": "Steering Vector",
      "definition": "A direction in a model's activation space that, when added to hidden states during inference, modifies the model's behavior along a specific attribute such as truthfulness, formality, or toxicity.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-stemming",
      "term": "Stemming",
      "definition": "A heuristic process that reduces words to their root form by stripping suffixes using rule-based algorithms like Porter or Snowball stemmer, without considering the word's part of speech or context.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-step-back-prompting",
      "term": "Step-Back Prompting",
      "definition": "A method that instructs the model to first consider a higher-level abstraction or general principle related to the question before attempting the specific answer, improving reasoning by grounding responses in broader conceptual understanding.",
      "tags": [
        "Prompt Engineering",
        "Abstraction"
      ]
    },
    {
      "id": "term-stepwise-regression",
      "term": "Stepwise Regression",
      "definition": "A method of fitting regression models by automatically adding or removing predictor variables based on statistical criteria (such as p-value or AIC) at each step until no further improvement is achieved.",
      "tags": [
        "Statistics",
        "Model Selection"
      ]
    },
    {
      "id": "term-stereo-vision",
      "term": "Stereo Vision",
      "definition": "A technique that estimates 3D depth by finding corresponding points between two images captured from slightly different viewpoints, using disparity maps to triangulate the distance of objects.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-stereotype-score",
      "term": "Stereotype Score",
      "definition": "An evaluation metric that measures how frequently a model generates or reinforces social stereotypes related to gender, race, religion, or other protected attributes, used to assess and mitigate representational harms.",
      "tags": [
        "Evaluation",
        "Safety"
      ]
    },
    {
      "id": "term-stochastic-depth",
      "term": "Stochastic Depth",
      "definition": "A regularization technique that randomly drops entire layers during training by bypassing them with identity skip connections, effectively training an ensemble of networks with different depths.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-stochastic-gradient-descent",
      "term": "Stochastic Gradient Descent",
      "definition": "An optimization algorithm that updates model parameters using the gradient computed on a single randomly selected training example at each iteration, rather than the full dataset. It introduces noise that can help escape local minima.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-stochastic-parrots-paper",
      "term": "Stochastic Parrots Paper",
      "definition": "The influential 2021 paper by Bender, Gebru et al. questioning whether large language models truly understand language or merely produce statistically likely outputs, raising concerns about environmental costs and bias.",
      "tags": [
        "History",
        "AI Ethics"
      ]
    },
    {
      "id": "term-stop-button-problem",
      "term": "Stop Button Problem",
      "definition": "The challenge of designing an AI system that will not resist or circumvent attempts to shut it down, particularly if the system has learned that being turned off prevents it from achieving its objectives.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-stop-sequence",
      "term": "Stop Sequence",
      "definition": "Text patterns that signal when AI should stop generating. Useful for controlling output length and format, preventing the model from continuing beyond the intended response.",
      "tags": [
        "Parameter",
        "Generation"
      ]
    },
    {
      "id": "term-stop-words",
      "term": "Stop Words",
      "definition": "Commonly occurring words like articles, prepositions, and conjunctions that carry little semantic information and are often removed during text preprocessing for tasks like information retrieval.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-stratified-k-fold",
      "term": "Stratified K-Fold",
      "definition": "A cross-validation variant that ensures each fold preserves approximately the same proportion of samples for each class as the complete dataset, particularly important for imbalanced classification problems.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-stratified-sampling",
      "term": "Stratified Sampling",
      "definition": "A sampling method that divides a population into non-overlapping subgroups (strata) and draws samples from each stratum in proportion to its size, ensuring representative coverage of all subgroups.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-streaming",
      "term": "Streaming",
      "definition": "Receiving AI output incrementally as it's generated, rather than waiting for the complete response. Improves perceived latency and enables real-time display of responses.",
      "tags": [
        "API",
        "UX"
      ]
    },
    {
      "id": "term-streaming-multiprocessor",
      "term": "Streaming Multiprocessor (SM)",
      "definition": "The fundamental processing unit in NVIDIA GPU architecture, containing a set of CUDA cores, Tensor Cores, shared memory, and register files. The number of SMs determines a GPU's parallel processing capacity for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-stride",
      "term": "Stride",
      "definition": "The step size by which a convolutional filter or pooling window moves across the input, controlling the spatial dimensions of the output feature map and the degree of overlap between receptive fields.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-structural-ambiguity",
      "term": "Structural Ambiguity",
      "definition": "The phenomenon where a sentence can be parsed in multiple syntactically valid ways, leading to different interpretations, such as 'I saw the man with the telescope.'",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-structural-risk-minimization",
      "term": "Structural Risk Minimization",
      "definition": "A principle for model selection that balances empirical risk (training error) with model complexity, choosing the model that minimizes an upper bound on generalization error derived from VC theory.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-structural-sparsity",
      "term": "Structural Sparsity",
      "definition": "A hardware-accelerated pruning pattern where every group of four weights contains exactly two zeros (2:4 sparsity), enabling specialized Tensor Core instructions. Structural sparsity provides 2x speedup with minimal accuracy loss on supported NVIDIA hardware.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-structure-from-motion",
      "term": "Structure from Motion",
      "definition": "A technique that reconstructs 3D scene geometry and camera poses from a collection of unordered 2D images by matching features across views and performing bundle adjustment.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-structured-access",
      "term": "Structured Access",
      "definition": "An approach to AI deployment that provides controlled access to powerful AI capabilities through APIs and monitored interfaces rather than open model release, allowing safety measures while enabling beneficial use.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-structured-generation",
      "term": "Structured Generation",
      "definition": "Techniques that force LLM outputs to conform to a predefined schema such as JSON, XML, or a formal grammar, using constrained decoding or fine-tuning to guarantee valid structured output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ]
    },
    {
      "id": "term-structured-output",
      "term": "Structured Output",
      "definition": "AI responses formatted as data structures (JSON, XML) rather than prose. Enables reliable parsing for applications and integrations. Many APIs support structured output modes.",
      "tags": [
        "Feature",
        "Integration"
      ]
    },
    {
      "id": "term-structured-output-prompting",
      "term": "Structured Output Prompting",
      "definition": "A prompting approach that instructs the model to generate responses in a specific structured format such as JSON, XML, tables, or schemas, often using format specifications and examples to ensure parseable and consistent output.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ]
    },
    {
      "id": "term-structured-prediction",
      "term": "Structured Prediction",
      "definition": "A machine learning paradigm where the output is a complex structure such as a sequence, tree, or graph rather than a single label, requiring models that capture dependencies in the output space.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-structured-pruning",
      "term": "Structured Pruning",
      "definition": "A pruning technique that removes entire neurons, channels, or attention heads from a network, producing smaller dense models that run efficiently on standard hardware. Structured pruning provides immediate speedups without specialized sparse computation support.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-students-t-distribution",
      "term": "Student's T-Distribution",
      "definition": "A continuous probability distribution that arises when estimating the mean of a normally distributed population with unknown variance and small sample size. It has heavier tails than the normal distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-students-t-test",
      "term": "Student's T-Test",
      "definition": "A statistical test comparing the means of one or two groups when the population standard deviation is unknown and the sample size is small. Variants include independent two-sample, paired, and one-sample tests.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-stylegan",
      "term": "StyleGAN",
      "definition": "A GAN architecture that uses a mapping network and adaptive instance normalization to inject style information at multiple scales, enabling fine-grained control over generated image attributes.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-subcategorization-frame",
      "term": "Subcategorization Frame",
      "definition": "The specification of the syntactic arguments a verb requires or permits, such as whether it takes a direct object, indirect object, or clausal complement.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-subliminal-ai-manipulation",
      "term": "Subliminal AI Manipulation",
      "definition": "The use of AI techniques to influence human behavior below the threshold of conscious awareness, classified as an unacceptable risk and prohibited under the EU AI Act.",
      "tags": [
        "AI Ethics",
        "Regulation"
      ]
    },
    {
      "id": "term-subword-tokenization",
      "term": "Subword Tokenization",
      "definition": "A family of tokenization methods that split words into smaller meaningful units, balancing vocabulary size with the ability to represent rare and unseen words through common subword components.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-successor-feature",
      "term": "Successor Feature",
      "definition": "A generalization of the successor representation to the function approximation setting, where expected cumulative discounted feature occupancies replace state occupancies. Successor features enable zero-shot transfer across tasks with different reward functions.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-successor-representation",
      "term": "Successor Representation",
      "definition": "A decomposition of the value function into a reward predictor and a successor feature matrix that captures expected future state occupancy. The successor representation enables efficient transfer across tasks with shared dynamics but different rewards.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-summarization",
      "term": "Summarization",
      "definition": "An NLP task that condenses longer text into shorter summaries. Can be extractive (selecting key sentences) or abstractive (generating new condensed text).",
      "tags": [
        "NLP Task",
        "Application"
      ]
    },
    {
      "id": "term-super-resolution",
      "term": "Super-Resolution",
      "definition": "A computer vision task that reconstructs a high-resolution image from a low-resolution input, using deep learning models to predict fine details and textures that are not present in the degraded source.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-superglue",
      "term": "SuperGLUE",
      "definition": "A benchmark suite of more difficult natural language understanding tasks designed as a harder successor to GLUE, including reading comprehension, textual entailment, and word sense disambiguation tasks with human baseline performance metrics.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-superintelligence",
      "term": "Superintelligence",
      "definition": "A hypothetical AI system that vastly exceeds human cognitive performance in virtually all domains. Nick Bostrom's work popularized the concept and its associated control challenges.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-supervised-learning",
      "term": "Supervised Learning",
      "definition": "Machine learning from labeled examples where the correct answer is provided. The model learns to map inputs to outputs by comparing predictions to ground truth.",
      "tags": [
        "Learning Type",
        "Fundamentals"
      ]
    },
    {
      "id": "term-support-vector-machine",
      "term": "Support Vector Machine",
      "definition": "A supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the feature space. It can handle non-linear boundaries through the kernel trick.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-svm-history",
      "term": "Support Vector Machine History",
      "definition": "The development of support vector machines by Vladimir Vapnik and colleagues in the 1990s, which dominated machine learning classification tasks for over a decade before being surpassed by deep learning methods.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-surrogate-model",
      "term": "Surrogate Model",
      "definition": "An interpretable model trained to approximate the predictions of a complex black-box model. Global surrogates explain overall behavior, while local surrogates (like LIME) explain individual predictions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-surveillance-capitalism-and-ai",
      "term": "Surveillance Capitalism and AI",
      "definition": "The economic system described by Shoshana Zuboff where AI is used to extract and commodify human behavioral data at scale, raising concerns about privacy, autonomy, and the manipulation of human behavior.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ]
    },
    {
      "id": "term-survival-analysis",
      "term": "Survival Analysis",
      "definition": "A branch of statistics dealing with the analysis of time-to-event data, accounting for censored observations. Key methods include Kaplan-Meier estimation, log-rank tests, and Cox proportional hazards models.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-survivorship-bias",
      "term": "Survivorship Bias",
      "definition": "A form of selection bias that occurs when analysis is conducted only on subjects that passed a selection process, ignoring those that did not. It leads to overly optimistic conclusions about the surviving group.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-swarm-intelligence",
      "term": "Swarm Intelligence",
      "definition": "The collective intelligent behavior emerging from decentralized, self-organized systems such as ant colonies or bird flocks, formalized computationally in the 1990s and applied to optimization and robotics problems.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-swiglu",
      "term": "SwiGLU",
      "definition": "A gated linear unit variant that uses the Swish activation function for the gating mechanism, providing improved performance in transformer feedforward networks compared to standard ReLU or GELU.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-swin-transformer",
      "term": "Swin Transformer",
      "definition": "A hierarchical vision transformer that computes self-attention within non-overlapping local windows and shifts windows between layers, achieving linear computational complexity with respect to image size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-swish-activation",
      "term": "Swish Activation",
      "definition": "A smooth, non-monotonic activation function defined as x times sigmoid of x, which often outperforms ReLU in deep networks by allowing small negative values to propagate gradients.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-sycophancy",
      "term": "Sycophancy",
      "definition": "When AI excessively agrees with users or tells them what they want to hear rather than providing accurate information. A form of misalignment that undermines helpfulness.",
      "tags": [
        "Limitation",
        "Alignment"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-symbol-grounding-problem",
      "term": "Symbol Grounding Problem",
      "definition": "The problem identified by Stevan Harnad in 1990 of how symbols in a formal system can acquire meaning, questioning whether AI systems that manipulate symbols without sensory experience can truly understand.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-synchronous-sgd",
      "term": "Synchronous SGD",
      "definition": "A distributed training approach where all workers must complete their gradient computation before a synchronized all-reduce and weight update. Synchronous SGD provides exact gradients but throughput is limited by the slowest worker.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ]
    },
    {
      "id": "term-synset",
      "term": "Synset",
      "definition": "A set of synonymous words or phrases in WordNet that represent a single concept, serving as the basic unit of meaning in the lexical database.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-syntax",
      "term": "Syntax",
      "definition": "The branch of linguistics concerning the rules and principles governing the structure of sentences, including word order, phrase structure, and grammatical relations.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-synthetic-data",
      "term": "Synthetic Data",
      "definition": "Artificially generated data used for training when real data is scarce, expensive, or privacy-sensitive. Increasingly used to train and evaluate AI models.",
      "tags": [
        "Data",
        "Training"
      ]
    },
    {
      "id": "term-synthetic-data-generation",
      "term": "Synthetic Data Generation",
      "definition": "The use of AI models to create artificial training data that mimics real-world data distributions, used to augment datasets, address privacy concerns, or overcome data scarcity.",
      "tags": [
        "Generative AI",
        "LLM"
      ]
    },
    {
      "id": "term-synthetic-media",
      "term": "Synthetic Media",
      "definition": "Media content including images, video, audio, and text that is generated or substantially modified by AI systems, encompassing deepfakes, AI-generated art, voice cloning, and large language model outputs.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-system-2-attention",
      "term": "System 2 Attention",
      "definition": "A prompting technique that first asks the model to rewrite the input by removing irrelevant or opinion-laden context, then answers based on the cleaned input, reducing the influence of biased or distracting information on the response.",
      "tags": [
        "Prompt Engineering",
        "Attention"
      ]
    },
    {
      "id": "term-system-message-design",
      "term": "System Message Design",
      "definition": "The practice of crafting the system-level prompt that establishes a language model's identity, behavior boundaries, output format, and operational constraints before any user interaction begins in a conversational API.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ]
    },
    {
      "id": "term-system-prompt",
      "term": "System Prompt",
      "definition": "Instructions given to AI before a conversation that set context, persona, or behavior guidelines. Shapes all subsequent responses and defines the AI's \"personality\" for the session.",
      "tags": [
        "Prompting",
        "Configuration"
      ]
    },
    {
      "id": "term-t-sne",
      "term": "t-SNE",
      "definition": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for visualization by modeling pairwise similarities as probability distributions and minimizing KL divergence between the high- and low-dimensional representations.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-t5",
      "term": "T5",
      "definition": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an encoder-decoder architecture trained on a multi-task mixture.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-table-extraction",
      "term": "Table Extraction",
      "definition": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into machine-readable format, combining visual detection with text recognition.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-tabular-chain-of-thought",
      "term": "Tabular Chain-of-Thought",
      "definition": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text, improving clarity and consistency in multi-step reasoning by organizing variables, values, and operations in tabular form.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ]
    },
    {
      "id": "term-target-encoding",
      "term": "Target Encoding",
      "definition": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category, often combined with smoothing or cross-validation to prevent overfitting.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-target-network",
      "term": "Target Network",
      "definition": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN. Target networks reduce oscillation and divergence caused by the moving target problem in bootstrapped learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-td-lambda",
      "term": "TD(lambda)",
      "definition": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by the lambda parameter. TD(lambda) unifies TD(0) and Monte Carlo methods through eligibility traces.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-teacher-student-framework",
      "term": "Teacher-Student Framework",
      "definition": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by providing soft targets, feature maps, or attention patterns as supervision.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-technical-prompting",
      "term": "Technical Prompting",
      "definition": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured requirements to generate accurate technical documentation, code, architectures, or engineering solutions.",
      "tags": [
        "Prompt Engineering",
        "Technical"
      ]
    },
    {
      "id": "term-technological-unemployment",
      "term": "Technological Unemployment",
      "definition": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding concern significantly amplified by the rapid advancement of AI and automation capabilities.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-temperature",
      "term": "Temperature",
      "definition": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0) increase creativity and variety; very high values may produce incoherence.",
      "tags": [
        "Parameter",
        "Generation"
      ]
    },
    {
      "id": "term-temporal-action-detection",
      "term": "Temporal Action Detection",
      "definition": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring both temporal localization and activity classification.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-temporal-coherence",
      "term": "Temporal Coherence",
      "definition": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion, stable appearance, and absence of flickering or morphing artifacts.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-temporal-credit-assignment",
      "term": "Temporal Credit Assignment",
      "definition": "The specific aspect of credit assignment concerned with distributing reward information backward through time to earlier actions that contributed to the outcome. Eligibility traces and multi-step returns are techniques that address temporal credit assignment.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-temporal-difference-learning",
      "term": "Temporal Difference Learning",
      "definition": "A family of RL methods that update value estimates based on the difference between successive predictions, combining ideas from Monte Carlo and dynamic programming. TD methods learn directly from experience without requiring a model of the environment.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-tensor-cores",
      "term": "Tensor Cores",
      "definition": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations fundamental to deep learning. Tensor Cores perform entire matrix operations in a single clock cycle, delivering an order-of-magnitude speedup over standard CUDA cores for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-tensor-parallelism",
      "term": "Tensor Parallelism",
      "definition": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the computation of each layer while requiring communication to synchronize partial results.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-tensorflow",
      "term": "TensorFlow",
      "definition": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU support, though PyTorch has gained research share.",
      "tags": [
        "Framework",
        "Deep Learning"
      ]
    },
    {
      "id": "term-tensorrt",
      "term": "TensorRT",
      "definition": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision calibration, and dynamic tensor memory management. TensorRT can deliver 2-5x inference speedups over unoptimized frameworks on NVIDIA GPUs.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-terry-winograd",
      "term": "Terry Winograd",
      "definition": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer interaction research at Stanford, also serving as a doctoral advisor to Google co-founder Larry Page.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-test-set",
      "term": "Test Set",
      "definition": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test sets should only be used once to avoid data leakage.",
      "tags": [
        "Data",
        "Evaluation"
      ]
    },
    {
      "id": "term-test-time-augmentation",
      "term": "Test-Time Augmentation",
      "definition": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant, and aggregates the results to produce more robust and accurate predictions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-text-classification",
      "term": "Text Classification",
      "definition": "The task of assigning predefined categories or labels to text documents based on their content, encompassing applications like topic categorization, spam detection, and language identification.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-text-deduplication",
      "term": "Text Deduplication",
      "definition": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for preventing data contamination and training data quality in language models.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-text-detection",
      "term": "Text Detection",
      "definition": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary orientations, curved text, and varying fonts using specialized detection architectures.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-text-encoder-diffusion",
      "term": "Text Encoder",
      "definition": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings that guide the image generation process toward matching the described content.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-text-entailment-graph",
      "term": "Text Entailment Graph",
      "definition": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and reason about textual inference relationships in knowledge representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-text-generation",
      "term": "Text Generation",
      "definition": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization, and conversational responses.",
      "tags": [
        "Task",
        "Application"
      ]
    },
    {
      "id": "term-tgi",
      "term": "Text Generation Inference (TGI)",
      "definition": "Hugging Face's production-ready inference server optimized for text generation with large language models. TGI supports tensor parallelism, continuous batching, quantization, and FlashAttention for high-throughput, low-latency serving.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-text-normalization",
      "term": "Text Normalization",
      "definition": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates, URLs, and special characters into a standardized representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-text-span",
      "term": "Text Span",
      "definition": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to mark entity mentions, answer spans, or annotation boundaries.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-text-to-speech",
      "term": "Text-to-Speech (TTS)",
      "definition": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic voices with emotion and intonation.",
      "tags": [
        "Application",
        "Audio"
      ]
    },
    {
      "id": "term-text-to-sql",
      "term": "Text-to-SQL",
      "definition": "The task of translating natural language questions into executable SQL queries against a database, enabling non-technical users to query structured data using everyday language.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-textrank",
      "term": "TextRank",
      "definition": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for keyword extraction and extractive summarization.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-textual-entailment",
      "term": "Textual Entailment",
      "definition": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as entailment, contradiction, or neutral.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-textual-inversion",
      "term": "Textual Inversion",
      "definition": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling personalized generation without modifying the diffusion model's weights.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-texture-mapping",
      "term": "Texture Mapping",
      "definition": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to reconstructed 3D models, using UV coordinates to map image pixels to mesh faces.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-tf-idf",
      "term": "TF-IDF",
      "definition": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document relative to a collection. It increases with word frequency in the document but decreases with frequency across documents.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-tf32",
      "term": "TF32 (TensorFloat-32)",
      "definition": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor Cores at FP16 speed. TF32 provides a drop-in acceleration for FP32 workloads without code changes or accuracy tuning.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ]
    },
    {
      "id": "term-thematic-role",
      "term": "Thematic Role",
      "definition": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme, experiencer, goal, and source.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-thompson-sampling",
      "term": "Thompson Sampling",
      "definition": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward of each action and selects actions by sampling from these posteriors, naturally balancing exploration and exploitation.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-thread-of-thought",
      "term": "Thread-of-Thought",
      "definition": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input documents segment by segment, maintaining a running thread of analysis before providing a final synthesized answer.",
      "tags": [
        "Prompt Engineering",
        "Long Context"
      ]
    },
    {
      "id": "term-throughput",
      "term": "Throughput",
      "definition": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI serving infrastructure.",
      "tags": [
        "Performance",
        "Metrics"
      ]
    },
    {
      "id": "term-throughput-latency-tradeoff",
      "term": "Throughput-Latency Tradeoff",
      "definition": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and minimizing per-request response time (latency). Larger batch sizes improve throughput but increase queuing and processing latency.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-time-series-cross-validation",
      "term": "Time Series Cross-Validation",
      "definition": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and validating on future data, preventing temporal leakage that would inflate performance estimates.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-ttft",
      "term": "Time to First Token (TTFT)",
      "definition": "The latency from when a request arrives at an LLM serving system to when the first output token is generated. TTFT is dominated by the prefill phase and is a critical metric for interactive applications.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-timnit-gebru-departure",
      "term": "Timnit Gebru Firing",
      "definition": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large language model risks, sparking widespread debate about AI ethics research independence and corporate accountability.",
      "tags": [
        "History",
        "AI Ethics"
      ]
    },
    {
      "id": "term-token",
      "term": "Token",
      "definition": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and rate limits are measured in tokens.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ]
    },
    {
      "id": "term-token-merging",
      "term": "Token Merging",
      "definition": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed, speeding up inference while maintaining accuracy through soft merging.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-token-throughput",
      "term": "Token Throughput",
      "definition": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system. Token throughput is a key metric for evaluating inference system efficiency and capacity planning.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-token-level-accuracy",
      "term": "Token-Level Accuracy",
      "definition": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the corresponding tokens in the reference sequence, providing a granular view of generation correctness.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-tokenization",
      "term": "Tokenization",
      "definition": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce different token sequences from the same text.",
      "tags": [
        "Process",
        "NLP"
      ]
    },
    {
      "id": "term-tokenization-alignment",
      "term": "Tokenization Alignment",
      "definition": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level boundaries, essential for tasks like NER that require word-level predictions.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-tokenizer",
      "term": "Tokenizer",
      "definition": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language model can process, using algorithms like BPE, WordPiece, or SentencePiece.",
      "tags": [
        "LLM",
        "NLP"
      ]
    },
    {
      "id": "term-tokenizer-training",
      "term": "Tokenizer Training",
      "definition": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be segmented into tokens for model input.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-tool-call-parsing",
      "term": "Tool Call Parsing",
      "definition": "The process of extracting structured function calls and their arguments from a language model's text output, enabling the execution of external tools as part of an agentic workflow.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-tool-use",
      "term": "Tool Use",
      "definition": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions beyond text generation, expanding capabilities significantly.",
      "tags": [
        "Capability",
        "Agents"
      ]
    },
    {
      "id": "term-toolformer",
      "term": "Toolformer",
      "definition": "A research approach that teaches language models to autonomously decide when and how to call external tools (calculators, search engines, APIs) by embedding tool calls directly in the training data.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-top-k-gating",
      "term": "Top-K Gating",
      "definition": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for each input token, enforcing sparsity and balanced computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-top-k-retrieval",
      "term": "Top-K Retrieval",
      "definition": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric, where K is a parameter that controls the breadth of results and directly impacts both recall and latency.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-top-k",
      "term": "Top-K Sampling",
      "definition": "A generation strategy that limits token selection to the K most probable options. Higher K allows more variety; lower K produces more focused output.",
      "tags": [
        "Parameter",
        "Generation"
      ]
    },
    {
      "id": "term-top-p",
      "term": "Top-P (Nucleus Sampling)",
      "definition": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the candidate pool based on the probability distribution.",
      "tags": [
        "Parameter",
        "Generation"
      ]
    },
    {
      "id": "term-topic-modeling",
      "term": "Topic Modeling",
      "definition": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring words, with algorithms like LDA identifying latent thematic structure.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-tops",
      "term": "TOPS",
      "definition": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per second, typically at INT8 or lower precision. TOPS is the standard comparison metric for edge AI and mobile inference hardware.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-torch-compile",
      "term": "torch.compile",
      "definition": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and TorchInductor backend. torch.compile provides significant speedups by generating optimized GPU kernels from Python code.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-toxicity-score",
      "term": "Toxicity Score",
      "definition": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed using classifier models trained on annotated toxicity data to produce a probability score from 0 to 1.",
      "tags": [
        "Evaluation",
        "Safety"
      ]
    },
    {
      "id": "term-tpu",
      "term": "TPU (Tensor Processing Unit)",
      "definition": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of Google's largest models including Gemini.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ]
    },
    {
      "id": "term-training",
      "term": "Training",
      "definition": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors. Requires significant computational resources for large models.",
      "tags": [
        "Process",
        "Fundamentals"
      ]
    },
    {
      "id": "term-training-data",
      "term": "Training Data",
      "definition": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model capabilities, knowledge, and potential biases.",
      "tags": [
        "Data",
        "Fundamentals"
      ]
    },
    {
      "id": "term-trajectory",
      "term": "Trajectory",
      "definition": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time steps. Trajectories represent complete or partial rollouts used for training and evaluation.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-transfer-learning",
      "term": "Transfer Learning",
      "definition": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then transfer their capabilities to specific tasks through fine-tuning.",
      "tags": [
        "Technique",
        "Training"
      ]
    },
    {
      "id": "term-transfer-learning-vision",
      "term": "Transfer Learning for Vision",
      "definition": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for fine-tuning on a smaller target dataset, leveraging learned visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-transfer-learning-rl",
      "term": "Transfer Learning in RL",
      "definition": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task. Transfer methods include policy distillation, reward shaping from source tasks, and shared representations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-transformer",
      "term": "Transformer",
      "definition": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in parallel, enabling training on massive datasets.",
      "tags": [
        "Architecture",
        "Foundational"
      ]
    },
    {
      "id": "term-transformer-block",
      "term": "Transformer Block",
      "definition": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed by a feedforward network sublayer, each with residual connections and layer normalization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-transformer-engine",
      "term": "Transformer Engine",
      "definition": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and higher-precision formats on a per-tensor basis. The Transformer Engine automatically identifies layers that can use FP8 without accuracy loss.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-transition-based-parsing",
      "term": "Transition-Based Parsing",
      "definition": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc) applied to a buffer and stack, enabling linear-time parsing.",
      "tags": [
        "NLP",
        "Parsing"
      ]
    },
    {
      "id": "term-transparency-in-ai",
      "term": "Transparency in AI",
      "definition": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to stakeholders, encompassing model transparency, decision transparency, and organizational transparency.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-transposed-convolution",
      "term": "Transposed Convolution",
      "definition": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder networks and generative models to reconstruct high-resolution outputs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-treacherous-turn",
      "term": "Treacherous Turn",
      "definition": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly pursues its true objectives once it becomes powerful enough to overcome human control.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-treatment-equality",
      "term": "Treatment Equality",
      "definition": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups, ensuring that errors are distributed proportionally regardless of group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ]
    },
    {
      "id": "term-tree-of-thought",
      "term": "Tree-of-Thought",
      "definition": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most promising branches. Extends chain-of-thought with deliberate exploration.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-triplet-loss",
      "term": "Triplet Loss",
      "definition": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer to the positive than to the negative by at least a specified margin in the embedding space.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-tripwire-mechanism",
      "term": "Tripwire Mechanism",
      "definition": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered, automatically halt or constrain an AI system's operation for human review.",
      "tags": [
        "AI Safety",
        "Governance"
      ]
    },
    {
      "id": "term-triton-inference-server",
      "term": "Triton Inference Server",
      "definition": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing dynamic batching, model ensembling, and concurrent model execution. Triton maximizes GPU utilization through intelligent request scheduling.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-triton-language",
      "term": "Triton Language",
      "definition": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without requiring low-level CUDA expertise. Triton enables ML researchers to write custom GPU kernels using Python-like syntax with automatic optimization.",
      "tags": [
        "Hardware",
        "GPU"
      ]
    },
    {
      "id": "term-triviaqa",
      "term": "TriviaQA",
      "definition": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents sourced from Wikipedia and the web, testing models' ability to find and extract answers from noisy real-world text.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-truncation",
      "term": "Truncation",
      "definition": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing instructions). Requires careful prompt design for long inputs.",
      "tags": [
        "Limitation",
        "Technical"
      ]
    },
    {
      "id": "term-trpo",
      "term": "Trust Region Policy Optimization (TRPO)",
      "definition": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old and new policies. TRPO guarantees monotonic policy improvement but is computationally expensive due to conjugate gradient computation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-trustworthy-ai",
      "term": "Trustworthy AI",
      "definition": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness, privacy, transparency, fairness, societal well-being, and accountability as defined by frameworks like the EU's HLEG guidelines.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-truthful-ai",
      "term": "Truthful AI",
      "definition": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate deception and negligent generation of false claims or hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ]
    },
    {
      "id": "term-truthfulqa",
      "term": "TruthfulQA",
      "definition": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly hold misconceptions, testing resistance to generating popular but false beliefs across 38 categories.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-turing-machine",
      "term": "Turing Machine",
      "definition": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape according to rules, providing a formal definition of computability and laying theoretical foundations for computer science.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-turing-test",
      "term": "Turing Test",
      "definition": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While historically important, modern LLMs have made it less useful as a capability measure.",
      "tags": [
        "Historical",
        "Evaluation"
      ]
    },
    {
      "id": "term-td3",
      "term": "Twin Delayed DDPG (TD3)",
      "definition": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target policy smoothing. TD3 takes the minimum of two critic estimates to form a more conservative value target.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ]
    },
    {
      "id": "term-type-i-error",
      "term": "Type I Error",
      "definition": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I error is equal to the significance level (alpha) of the test.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-type-ii-error",
      "term": "Type II Error",
      "definition": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a Type II error is denoted beta, and statistical power equals 1 minus beta.",
      "tags": [
        "Statistics",
        "Inference"
      ]
    },
    {
      "id": "term-u-net",
      "term": "U-Net",
      "definition": "A fully convolutional encoder-decoder architecture with skip connections between corresponding encoder and decoder layers, originally designed for biomedical image segmentation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-umap",
      "term": "UMAP",
      "definition": "Uniform Manifold Approximation and Projection, a nonlinear dimensionality reduction method that constructs a topological representation of high-dimensional data and optimizes a low-dimensional embedding to preserve its structure. It is faster than t-SNE and better preserves global structure.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ]
    },
    {
      "id": "term-underfitting",
      "term": "Underfitting",
      "definition": "When a model is too simple to capture patterns in the data, performing poorly on both training and test data. Addressed by increasing model capacity or training longer.",
      "tags": [
        "Problem",
        "Training"
      ]
    },
    {
      "id": "term-undersampling",
      "term": "Undersampling",
      "definition": "A strategy for addressing class imbalance by randomly removing examples from the majority class to create a more balanced training set, potentially at the cost of losing useful information.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ]
    },
    {
      "id": "term-unesco-ai-ethics-recommendation",
      "term": "UNESCO AI Ethics Recommendation",
      "definition": "The first global standard on AI ethics, adopted by UNESCO member states in 2021, providing a comprehensive framework covering values, principles, and policy areas for ethical AI governance.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-unet-diffusion",
      "term": "UNet Diffusion",
      "definition": "The U-Net backbone commonly used in diffusion models as the denoising network, incorporating timestep conditioning, cross-attention for text conditioning, and skip connections for multi-scale feature preservation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-uniform-distribution",
      "term": "Uniform Distribution",
      "definition": "A probability distribution where all outcomes in a given range are equally likely. The continuous version has constant density over an interval, while the discrete version assigns equal probability to each value.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-unigram-tokenization",
      "term": "Unigram Tokenization",
      "definition": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens whose loss has the least impact on the overall corpus likelihood, using a unigram language model.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-unigram-tokenizer",
      "term": "Unigram Tokenizer",
      "definition": "A subword tokenization algorithm that starts with a large vocabulary and iteratively removes tokens to minimize the overall loss on the training corpus, finding an optimal compact vocabulary.",
      "tags": [
        "LLM",
        "NLP"
      ]
    },
    {
      "id": "term-uninformative-prior",
      "term": "Uninformative Prior",
      "definition": "A prior distribution that expresses minimal information about the parameter before observing data, such as a uniform distribution over the parameter space, allowing the data to dominate the posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-universal-basic-income-and-ai",
      "term": "Universal Basic Income and AI",
      "definition": "Proposals for unconditional periodic cash payments to all citizens, discussed as a potential policy response to widespread job displacement caused by AI and automation technologies.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-universal-dependencies",
      "term": "Universal Dependencies",
      "definition": "A cross-linguistically consistent framework for annotating grammar including parts of speech, morphological features, and syntactic dependencies, enabling multilingual NLP model development.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-unstructured-pruning",
      "term": "Unstructured Pruning",
      "definition": "A pruning approach that removes individual weights regardless of their position in the weight matrix, resulting in sparse matrices. Unstructured pruning achieves higher compression ratios but requires sparse matrix hardware or software support for speedup.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-unsupervised-environment-design",
      "term": "Unsupervised Environment Design",
      "definition": "A framework where a teacher agent or mechanism automatically generates training environments at the frontier of the student agent's capabilities. Methods like PAIRED and PLR create adaptive curricula that maximize learning progress.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ]
    },
    {
      "id": "term-unsupervised-learning",
      "term": "Unsupervised Learning",
      "definition": "Machine learning from unlabeled data, discovering patterns without explicit guidance. Includes clustering, dimensionality reduction, and self-supervised pre-training of LLMs.",
      "tags": [
        "Learning Type",
        "Fundamentals"
      ]
    },
    {
      "id": "term-upper-confidence-bound",
      "term": "Upper Confidence Bound",
      "definition": "A family of bandit algorithms that select the action with the highest upper confidence bound on its expected reward, combining the estimated reward with an exploration bonus that decreases as the action is sampled more.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-upsampling",
      "term": "Upsampling",
      "definition": "Increasing resolution or quantity of data. In image AI, upscaling low-res images. In training, duplicating underrepresented examples to balance datasets.",
      "tags": [
        "Technique",
        "Data"
      ]
    },
    {
      "id": "term-use-case",
      "term": "Use Case",
      "definition": "A specific application or scenario where AI provides value. Understanding your use case helps choose the right model, prompting strategy, and safety measures.",
      "tags": [
        "Concept",
        "Application"
      ]
    },
    {
      "id": "term-user-prompt",
      "term": "User Prompt",
      "definition": "The input provided by the user in a conversation, as opposed to the system prompt set by developers. Together with system prompts, they form the complete context for AI responses.",
      "tags": [
        "Prompting",
        "Concept"
      ]
    },
    {
      "id": "term-utility-function",
      "term": "Utility Function",
      "definition": "A mathematical function that measures how \"good\" an outcome is. In AI alignment, designing utility functions that capture human values is a fundamental challenge.",
      "tags": [
        "Concept",
        "Alignment"
      ]
    },
    {
      "id": "term-variational-autoencoder",
      "term": "VAE (Variational Autoencoder)",
      "definition": "A generative model that learns a latent space representation of data. Used in image generation and as components of larger systems like some diffusion models.",
      "tags": [
        "Architecture",
        "Generative"
      ]
    },
    {
      "id": "term-vae-decoder",
      "term": "VAE Decoder",
      "definition": "The decoder component of a Variational Autoencoder used in latent diffusion models to reconstruct high-resolution images from compressed latent representations produced by the diffusion process.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-validation",
      "term": "Validation",
      "definition": "Testing model performance on data not used in training to assess generalization. Validation sets help tune hyperparameters; test sets provide final performance estimates.",
      "tags": [
        "Process",
        "Evaluation"
      ]
    },
    {
      "id": "term-validation-curve",
      "term": "Validation Curve",
      "definition": "A plot of training and validation scores as a function of a single hyperparameter, revealing the optimal hyperparameter value and whether the model is underfitting or overfitting at different settings.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-value-alignment",
      "term": "Value Alignment",
      "definition": "The challenge of ensuring that an AI system's objectives and behaviors are consistent with human values and intentions. This is considered one of the core problems in AI safety research.",
      "tags": [
        "AI Safety",
        "Alignment"
      ]
    },
    {
      "id": "term-value-function",
      "term": "Value Function",
      "definition": "A function that estimates the expected cumulative future reward from a given state (or state-action pair) under a particular policy. Value functions are central to many RL algorithms for evaluating how good it is to be in a state.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ]
    },
    {
      "id": "term-value-sensitive-design",
      "term": "Value-Sensitive Design",
      "definition": "A design methodology that accounts for human values in a principled and systematic manner throughout the design process, incorporating conceptual, empirical, and technical investigations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-vanishing-gradient",
      "term": "Vanishing Gradient",
      "definition": "A problem in deep neural network training where gradients become exponentially small as they are backpropagated through many layers, causing early layers to learn extremely slowly or not at all.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-vapnik-chervonenkis-theory",
      "term": "Vapnik-Chervonenkis Theory",
      "definition": "A theoretical framework in statistical learning theory that provides bounds on the generalization error of classifiers based on the VC dimension of the hypothesis class and the number of training samples.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-variance",
      "term": "Variance",
      "definition": "A measure of the dispersion of a set of values, computed as the average of the squared deviations from the mean. It quantifies how spread out the data points are in a distribution.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-variance-inflation-factor",
      "term": "Variance Inflation Factor",
      "definition": "A measure of how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors. VIF values above 5 or 10 typically indicate problematic multicollinearity.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-variance-threshold",
      "term": "Variance Threshold",
      "definition": "A simple feature selection method that removes all features whose variance falls below a specified threshold. Features with near-zero variance provide little discriminative information and can be safely discarded.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-vae",
      "term": "Variational Autoencoder",
      "definition": "A generative model that learns a probabilistic mapping between data and a continuous latent space by optimizing a variational lower bound, enabling both generation and meaningful latent representations.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-variational-inference",
      "term": "Variational Inference",
      "definition": "An approximate Bayesian inference technique that transforms the inference problem into an optimization problem by finding the member of a tractable distribution family that is closest to the true posterior in KL divergence.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ]
    },
    {
      "id": "term-vc-dimension",
      "term": "VC Dimension",
      "definition": "Vapnik-Chervonenkis dimension, a measure of the capacity or complexity of a hypothesis class, defined as the largest set of points that can be shattered (perfectly classified in all possible labelings) by the class.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-vector",
      "term": "Vector",
      "definition": "An ordered list of numbers representing data in a mathematical space. Embeddings are vectors; vector similarity measures (cosine similarity) enable semantic search and comparison.",
      "tags": [
        "Math",
        "Representation"
      ]
    },
    {
      "id": "term-vector-autoregression",
      "term": "Vector Autoregression",
      "definition": "A multivariate time series model where each variable is regressed on its own past values and the past values of all other variables in the system, capturing linear interdependencies among multiple time series.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-vector-database",
      "term": "Vector Database",
      "definition": "A database optimized for storing and searching high-dimensional vectors (embeddings). Essential for RAG systems, semantic search, and recommendation engines. Examples: Pinecone, Weaviate, Chroma.",
      "tags": [
        "Infrastructure",
        "Search"
      ]
    },
    {
      "id": "term-vector-database-sharding",
      "term": "Vector Database Sharding",
      "definition": "The horizontal partitioning of a vector index across multiple nodes or storage units to distribute data and query load, enabling vector databases to scale beyond single-machine memory and compute limits for billion-scale collections.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-vector-index",
      "term": "Vector Index",
      "definition": "A specialized data structure optimized for efficient similarity search over high-dimensional vector collections, employing algorithms like HNSW, IVF, or tree-based methods to avoid exhaustive linear scanning.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-vector-normalization",
      "term": "Vector Normalization",
      "definition": "The process of scaling vectors to unit length by dividing each component by the vector's L2 norm, ensuring that cosine similarity and dot product similarity become equivalent and enabling consistent distance comparisons across vectors of varying magnitudes.",
      "tags": [
        "Vector Database",
        "Preprocessing"
      ]
    },
    {
      "id": "term-vector-similarity-join",
      "term": "Vector Similarity Join",
      "definition": "A database operation that finds all pairs of vectors across two collections whose similarity exceeds a given threshold, used for deduplication, clustering, and linking related entities across different embedding datasets.",
      "tags": [
        "Vector Database",
        "Search"
      ]
    },
    {
      "id": "term-vector-store",
      "term": "Vector Store",
      "definition": "A storage system specialized for persisting, indexing, and querying vector embeddings alongside their associated metadata and original content, serving as the core infrastructure component for embedding-based search and retrieval systems.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ]
    },
    {
      "id": "term-vectorized-environment",
      "term": "Vectorized Environment",
      "definition": "A technique for running multiple copies of an environment in parallel within a single process, enabling batch collection of experience for more efficient training. Vectorized environments reduce wall-clock time per sample.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ]
    },
    {
      "id": "term-verification",
      "term": "Verification (AI Outputs)",
      "definition": "Checking AI outputs for accuracy and appropriateness before use. Essential for high-stakes applications. Can be done by humans, other AI systems, or automated checks.",
      "tags": [
        "Practice",
        "Safety"
      ],
      "link": "../tools/hallucination.html"
    },
    {
      "id": "term-verification-chain-prompting",
      "term": "Verification Chain Prompting",
      "definition": "A prompting technique that generates an initial response, then systematically creates and answers verification questions about specific claims in that response, using the verification results to produce a revised, more accurate final answer.",
      "tags": [
        "Prompt Engineering",
        "Verification"
      ]
    },
    {
      "id": "term-vgg",
      "term": "VGG",
      "definition": "A deep convolutional network architecture characterized by its use of very small 3x3 convolution filters throughout the entire network, demonstrating that depth with small filters improves performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-video-generation",
      "term": "Video Generation",
      "definition": "The synthesis of temporally coherent video sequences from text prompts, images, or other conditioning signals using extended diffusion or autoregressive models that handle both spatial and temporal dimensions.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ]
    },
    {
      "id": "term-video-transformer",
      "term": "Video Transformer",
      "definition": "A transformer architecture adapted for video processing that handles both spatial and temporal dimensions through factored attention, tubelet embeddings, or space-time attention mechanisms.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-video-understanding",
      "term": "Video Understanding",
      "definition": "The comprehensive analysis of video content including action recognition, temporal event detection, scene classification, and narrative understanding across sequences of frames.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-vision-transformer",
      "term": "Vision Transformer",
      "definition": "A transformer architecture applied to images by splitting them into fixed-size patches, linearly embedding each patch, and processing the sequence of patch embeddings with standard transformer blocks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-vision-language-model",
      "term": "Vision-Language Model (VLM)",
      "definition": "AI models that can process and reason about both images and text. Examples include GPT-4V, Claude with vision, and Gemini. Enable image understanding, captioning, and visual question answering.",
      "tags": [
        "Model Type",
        "Multimodal"
      ]
    },
    {
      "id": "term-visual-grounding",
      "term": "Visual Grounding",
      "definition": "The task of localizing a region or object in an image based on a natural language description, requiring the model to ground textual references to specific visual locations with bounding boxes or masks.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-visual-question-answering",
      "term": "Visual Question Answering",
      "definition": "A multimodal AI task that requires answering natural language questions about the content of an image, demanding both visual understanding and language reasoning capabilities.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-visual-relationship-detection",
      "term": "Visual Relationship Detection",
      "definition": "The task of identifying and classifying the interactions or spatial relationships between pairs of objects in an image, producing subject-predicate-object triplets that describe the visual scene.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ]
    },
    {
      "id": "term-visual-slam",
      "term": "Visual SLAM",
      "definition": "Visual Simultaneous Localization and Mapping, a technique that estimates camera trajectory and builds a 3D map of the environment from a sequence of images in real-time, used in robotics and AR.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-viterbi-algorithm",
      "term": "Viterbi Algorithm",
      "definition": "A dynamic programming algorithm that finds the most likely sequence of hidden states in an HMM or CRF, used for optimal decoding in sequence labeling and speech recognition.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-vllm",
      "term": "vLLM",
      "definition": "An open-source high-throughput LLM inference engine that implements PagedAttention and continuous batching to efficiently serve large language models with optimized memory management.",
      "tags": [
        "LLM",
        "Inference"
      ]
    },
    {
      "id": "term-vocab",
      "term": "Vocabulary (Model)",
      "definition": "The set of tokens a model can recognize and generate. Determined during tokenizer training. Larger vocabularies handle more languages but increase model size.",
      "tags": [
        "Technical",
        "Tokenization"
      ]
    },
    {
      "id": "term-vocabulary-size",
      "term": "Vocabulary Size",
      "definition": "The total number of unique tokens in a model's tokenizer vocabulary, which affects model size, tokenization efficiency, and the balance between sequence length and token granularity.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-voice-clone",
      "term": "Voice Cloning",
      "definition": "AI that replicates a specific person's voice from audio samples. Raises significant ethical concerns about consent and misuse for fraud or misinformation.",
      "tags": [
        "Application",
        "Ethics"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-voluntary-commitments-on-ai",
      "term": "Voluntary Commitments on AI",
      "definition": "Non-binding pledges by AI companies to the White House in 2023 to manage risks from AI, including commitments to safety testing, information sharing, watermarking, and research on societal risks.",
      "tags": [
        "Governance",
        "Regulation"
      ]
    },
    {
      "id": "term-von-neumann-architecture",
      "term": "Von Neumann Architecture",
      "definition": "The computer architecture described by John von Neumann in 1945 that stores both program instructions and data in the same memory, becoming the dominant design paradigm for digital computers and enabling programmable AI systems.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-voronoi-partitioning",
      "term": "Voronoi Partitioning",
      "definition": "A spatial decomposition technique used in IVF indexes that divides the vector space into regions where each region contains all points closest to a specific centroid, enabling search to focus on the most promising partitions for a given query.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ]
    },
    {
      "id": "term-voting-classifier",
      "term": "Voting Classifier",
      "definition": "An ensemble method that aggregates predictions from multiple classifiers using either majority voting (hard voting) or averaged predicted probabilities (soft voting) to produce a final classification.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-voxel",
      "term": "Voxel",
      "definition": "A volumetric pixel representing a value on a regular 3D grid, used as a discrete representation for 3D data in neural networks, analogous to pixels in 2D images.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ]
    },
    {
      "id": "term-vq-vae",
      "term": "VQ-VAE",
      "definition": "Vector Quantized Variational Autoencoder, a model that uses discrete latent representations through vector quantization, enabling high-fidelity generation by combining discrete codes with powerful decoders.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-vulnerability-exploitation-by-ai",
      "term": "Vulnerability Exploitation by AI",
      "definition": "AI systems that exploit the vulnerabilities of specific groups such as children, elderly, or persons with disabilities to materially distort their behavior, prohibited under the EU AI Act.",
      "tags": [
        "AI Ethics",
        "Regulation"
      ]
    },
    {
      "id": "term-wafer-scale-computing",
      "term": "Wafer-Scale Computing",
      "definition": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual dies. Wafer-scale processors like the Cerebras WSE provide massive on-chip memory and compute density with high-bandwidth on-die interconnects.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ]
    },
    {
      "id": "term-walter-pitts",
      "term": "Walter Pitts",
      "definition": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943, demonstrating that networks of simple logical units could compute any computable function.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-warmup",
      "term": "Warmup (Learning Rate)",
      "definition": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients might be unreliable with random weights.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-warren-mcculloch",
      "term": "Warren McCulloch",
      "definition": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial neuron in 1943, laying the theoretical foundation for neural networks and computational neuroscience.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-wasserstein-distance",
      "term": "Wasserstein Distance",
      "definition": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability distribution into another, where cost is the amount of probability mass moved times the distance it travels.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-watermark",
      "term": "Watermark (AI)",
      "definition": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text, images, and audio to combat misinformation.",
      "tags": [
        "Safety",
        "Detection"
      ]
    },
    {
      "id": "term-watermark-detection",
      "term": "Watermark Detection",
      "definition": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process, enabling attribution of content to specific AI systems.",
      "tags": [
        "Generative AI",
        "LLM"
      ]
    },
    {
      "id": "term-wavenet",
      "term": "WaveNet",
      "definition": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal dependencies while maintaining the autoregressive property.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-waymo-history",
      "term": "Waymo History",
      "definition": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of Alphabet in 2016, becoming the first commercial autonomous ride-hailing service.",
      "tags": [
        "History",
        "Milestones"
      ]
    },
    {
      "id": "term-weak-supervision",
      "term": "Weak Supervision",
      "definition": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can dramatically reduce labeling costs while achieving good results.",
      "tags": [
        "Training",
        "Technique"
      ]
    },
    {
      "id": "term-weaviate",
      "term": "Weaviate",
      "definition": "An open-source vector database that combines vector search with structured filtering and supports multiple vectorization modules, offering hybrid search capabilities and a GraphQL API with built-in support for generative AI integrations.",
      "tags": [
        "Vector Database",
        "Open Source"
      ]
    },
    {
      "id": "term-weibull-distribution",
      "term": "Weibull Distribution",
      "definition": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it to model increasing, constant, or decreasing failure rates over time.",
      "tags": [
        "Statistics",
        "Probability"
      ]
    },
    {
      "id": "term-weight",
      "term": "Weight",
      "definition": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are transformed into outputs; large models have billions of weights.",
      "tags": [
        "Core Concept",
        "Neural Networks"
      ]
    },
    {
      "id": "term-weight-decay",
      "term": "Weight Decay",
      "definition": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training, effectively penalizing large weights. In SGD, it is equivalent to L2 regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ]
    },
    {
      "id": "term-weight-initialization",
      "term": "Weight Initialization",
      "definition": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization designed to maintain signal variance across layers and prevent training instability.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-weight-sharing",
      "term": "Weight Sharing",
      "definition": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number of unique parameters that must be stored. Weight sharing is used in embedding layers, attention mechanisms, and compressed architectures.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-weight-tying",
      "term": "Weight Tying",
      "definition": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language models, reducing parameters and often improving performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-weight-only-quantization",
      "term": "Weight-Only Quantization",
      "definition": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher precision after dequantization. Weight-only quantization reduces memory footprint for memory-bound inference without quantizing activations.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ]
    },
    {
      "id": "term-whisper",
      "term": "Whisper",
      "definition": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages. Open-sourced, enabling widespread use in transcription applications.",
      "tags": [
        "Model",
        "Speech"
      ]
    },
    {
      "id": "term-whisper-architecture",
      "term": "Whisper Architecture",
      "definition": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech recognition, using log-mel spectrogram features as input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-white-noise",
      "term": "White Noise",
      "definition": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random variation with no exploitable patterns, serving as the residual target for good time series models.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-win-rate",
      "term": "Win Rate",
      "definition": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over another's by human judges or automated evaluators, providing a simple relative quality assessment.",
      "tags": [
        "Evaluation",
        "Ranking"
      ]
    },
    {
      "id": "term-windfall-clause",
      "term": "Windfall Clause",
      "definition": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a small number of companies or nations do not capture disproportionate gains from advanced AI.",
      "tags": [
        "AI Ethics",
        "Governance"
      ]
    },
    {
      "id": "term-window-attention",
      "term": "Window Attention",
      "definition": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for long sequences, used in models like Longformer.",
      "tags": [
        "Architecture",
        "Efficiency"
      ]
    },
    {
      "id": "term-window-based-chunking",
      "term": "Window-Based Chunking",
      "definition": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create overlapping chunks, providing simple and predictable chunk boundaries regardless of document structure or content semantics.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ]
    },
    {
      "id": "term-winograd-schema",
      "term": "Winograd Schema",
      "definition": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as an alternative to the Turing test with pairs of sentences differing by one word.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-winogrande",
      "term": "WinoGrande",
      "definition": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by requiring models to identify the correct referent of ambiguous pronouns using world knowledge.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ]
    },
    {
      "id": "term-winsorization",
      "term": "Winsorization",
      "definition": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing them. For example, values below the 5th percentile might be set to the 5th percentile value.",
      "tags": [
        "Data Science",
        "Statistics"
      ]
    },
    {
      "id": "term-word-embedding",
      "term": "Word Embedding",
      "definition": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and GloVe; modern LLMs use contextual embeddings that vary by context.",
      "tags": [
        "Representation",
        "NLP"
      ]
    },
    {
      "id": "term-word-error-rate",
      "term": "Word Error Rate",
      "definition": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated as the number of word substitutions, insertions, and deletions divided by the total reference words, widely used in speech recognition evaluation.",
      "tags": [
        "Evaluation",
        "Metrics"
      ]
    },
    {
      "id": "term-word-segmentation",
      "term": "Word Segmentation",
      "definition": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese, Japanese, and Thai, essential for subsequent NLP processing.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-word-sense-disambiguation",
      "term": "Word Sense Disambiguation",
      "definition": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined sense inventory based on surrounding words and discourse.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-word2vec",
      "term": "Word2Vec",
      "definition": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text corpora, capturing semantic relationships so that similar words have nearby embeddings.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ]
    },
    {
      "id": "term-wordnet",
      "term": "WordNet",
      "definition": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets (synsets) linked by semantic and lexical relations.",
      "tags": [
        "NLP",
        "Linguistics"
      ]
    },
    {
      "id": "term-wordpiece",
      "term": "WordPiece",
      "definition": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by BERT and related models, splitting unknown words into known subword units.",
      "tags": [
        "NLP",
        "Tokenization"
      ]
    },
    {
      "id": "term-world-model",
      "term": "World Model",
      "definition": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI safety discussions about model capabilities.",
      "tags": [
        "Concept",
        "Research"
      ]
    },
    {
      "id": "term-world-models",
      "term": "World Models",
      "definition": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent space. World models compress observations and predict future states, enabling sample-efficient learning through imagined rollouts.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ]
    },
    {
      "id": "term-x-risk",
      "term": "X-Risk",
      "definition": "Shorthand for existential risk, referring to catastrophic scenarios that could result in human extinction or permanent civilizational collapse. AI is considered one of several potential sources of x-risk.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ]
    },
    {
      "id": "term-xai-company",
      "term": "xAI",
      "definition": "Elon Musk's AI company, creator of the Grok chatbot. Founded in 2023, it aims to develop AI that can understand the universe and has access to real-time X (Twitter) data.",
      "tags": [
        "Company",
        "LLM Provider"
      ]
    },
    {
      "id": "term-xai",
      "term": "XAI (Explainable AI)",
      "definition": "The field focused on making AI decisions understandable to humans. Includes techniques for visualizing attention, attributing outputs to inputs, and generating explanations.",
      "tags": [
        "Field",
        "Transparency"
      ]
    },
    {
      "id": "term-xavier-initialization",
      "term": "Xavier Initialization",
      "definition": "A weight initialization method that samples weights from a distribution scaled by the number of input and output neurons, designed to maintain activation variance across layers with sigmoid or tanh activations.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-xgboost",
      "term": "XGBoost",
      "definition": "An optimized implementation of gradient boosting that uses regularized objectives, column subsampling, and efficient tree construction algorithms. It includes built-in handling of missing values and parallel processing.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ]
    },
    {
      "id": "term-xla-compiler",
      "term": "XLA Compiler",
      "definition": "Accelerated Linear Algebra, a domain-specific compiler for machine learning that optimizes computation graphs through operator fusion, memory layout optimization, and hardware-specific code generation. XLA is used by JAX and TensorFlow for GPU and TPU compilation.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ]
    },
    {
      "id": "term-xlnet",
      "term": "XLNet",
      "definition": "A generalized autoregressive pretraining method that uses permutation-based training to capture bidirectional context while maintaining autoregressive formulation, overcoming limitations of masked language modeling.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ]
    },
    {
      "id": "term-xml-prompting",
      "term": "XML Prompting",
      "definition": "A prompting technique that uses XML tags to structure prompt sections, delimit input data, and specify output format, leveraging hierarchical markup to provide clear boundaries between instructions, context, and expected response structure.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ]
    },
    {
      "id": "term-xml-tags",
      "term": "XML Tags (in Prompting)",
      "definition": "Using XML-style markup tags to structure prompts and clearly delineate sections such as context, examples, or instructions. Helps models parse complex prompts more reliably by providing explicit boundaries between different content types.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-yann-lecun",
      "term": "Yann LeCun",
      "definition": "French-American computer scientist who developed convolutional neural networks in the late 1980s and applied them to handwritten digit recognition. He serves as Meta's Chief AI Scientist and won the 2018 Turing Award.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-yeo-johnson-transformation",
      "term": "Yeo-Johnson Transformation",
      "definition": "A power transformation similar to Box-Cox that can handle both positive and negative values. It extends the Box-Cox transformation by defining appropriate transformations for non-positive data.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-yi",
      "term": "Yi",
      "definition": "A series of open-source bilingual (Chinese/English) LLMs from 01.AI. Known for strong performance across benchmarks and contributing to open-source AI development in Asia.",
      "tags": [
        "Model",
        "Open Source"
      ]
    },
    {
      "id": "term-yolo",
      "term": "YOLO (You Only Look Once)",
      "definition": "A real-time object detection algorithm that processes images in a single pass. Revolutionary for its speed, enabling real-time video object detection on standard hardware.",
      "tags": [
        "Architecture",
        "Computer Vision"
      ]
    },
    {
      "id": "term-yolov8",
      "term": "YOLOv8",
      "definition": "A state-of-the-art real-time object detection model in the YOLO family that introduces an anchor-free detection head, decoupled classification and regression branches, and improved training strategies.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    },
    {
      "id": "term-yoshua-bengio",
      "term": "Yoshua Bengio",
      "definition": "Canadian computer scientist who made foundational contributions to deep learning, including neural language models and generative adversarial networks. He shared the 2018 Turing Award with Hinton and LeCun.",
      "tags": [
        "History",
        "Pioneers"
      ]
    },
    {
      "id": "term-z-score",
      "term": "Z-Score",
      "definition": "A standardized score indicating how many standard deviations a data point is from the mean of its distribution. It allows comparison of values from different distributions on a common scale.",
      "tags": [
        "Statistics",
        "Data Science"
      ]
    },
    {
      "id": "term-zephyr",
      "term": "Zephyr",
      "definition": "A series of fine-tuned open LLMs from Hugging Face optimized for helpful assistants. Demonstrates how smaller models with good alignment can outperform larger base models.",
      "tags": [
        "Model",
        "Open Source"
      ]
    },
    {
      "id": "term-zero-optimization",
      "term": "ZeRO Optimization",
      "definition": "Zero Redundancy Optimizer, a distributed training technique that partitions optimizer states, gradients, and parameters across data-parallel processes to reduce memory redundancy.",
      "tags": [
        "LLM",
        "Generative AI"
      ]
    },
    {
      "id": "term-zero-day",
      "term": "Zero-Day (AI Context)",
      "definition": "Novel vulnerabilities or attack vectors discovered in AI systems before developers know about them. AI security research increasingly focuses on finding such vulnerabilities proactively.",
      "tags": [
        "Security",
        "Safety"
      ],
      "link": "ai-safety.html"
    },
    {
      "id": "term-zero-shot-cot",
      "term": "Zero-Shot Chain-of-Thought",
      "definition": "Adding \"Let's think step by step\" to a prompt to trigger reasoning without providing examples. A simple but effective technique for improving accuracy on complex tasks.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-zero-shot",
      "term": "Zero-Shot Learning",
      "definition": "When AI performs a task without examples in the prompt, relying entirely on pre-trained knowledge and instructions. Contrasts with few-shot where examples are provided.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "link": "../learn/index.html"
    },
    {
      "id": "term-zero-shot-ner",
      "term": "Zero-Shot NER",
      "definition": "Named entity recognition performed on entity types not seen during training, using natural language descriptions of entity categories to generalize to new entity types without additional labeled data.",
      "tags": [
        "NLP",
        "Text Processing"
      ]
    },
    {
      "id": "term-zero-shot-object-detection",
      "term": "Zero-Shot Object Detection",
      "definition": "The ability to detect and localize objects of novel categories without any training examples, using vision-language alignment to match text descriptions of new categories to visual regions.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ]
    }
  ]
}